% Packages (fold)
\RequirePackage{lmodern}
\documentclass[12pt, oneside, extrafontsizes]{memoir}  % TODO 12pt, twoside

\setstocksize{11in}{8.5in}
\settrimmedsize{11in}{8.5in}{*}
\settrims{0in}{0in}
\setlrmarginsandblock{38mm}{25mm}{*}
\setulmarginsandblock{25mm}{25mm}{*}
\setheadfoot{13pt}{26pt}
\setheaderspaces{*}{13pt}{*}
\checkandfixthelayout

\DoubleSpacing

\setsecnumdepth{subsubsection}
\headstyles{default}
\chapterstyle{ell}
\setsecheadstyle{\scshape\LARGE\raggedright}

% PDF
\usepackage[colorlinks,bookmarksnumbered,bookmarksdepth=subsubsection,unicode=true]{hyperref}
\newsubfloat{figure}  % must follow hyperref
\hypersetup{
pdfauthor = {Pierre-Luc Bacon},
pdftitle = {Automatic Options Discovery from the Community Structures in the State-Transition Graph},
pdfsubject = {Subject},
pdfkeywords = {reinforcement learning, time series analysis},
pdfcreator = {LaTeX with the hyperref package},
pdfproducer = {},
linkcolor = [HTML]{000000},
citecolor = [HTML]{0F82F5},
%urlcolor = [HTML]{\colorc}
}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{standalone}
\usepackage[boxed]{algorithm2e}

% Custom commands
\newcommand{\rvs}{r.v.'s\xspace}
\newcommand{\mdp}{Markov Decision Process\xspace}
\newcommand{\mdps}{Markov Decision Processes\xspace}
\newcommand{\mrp}{Markov Reward Process\xspace}
\newcommand{\mc}{Markov Chain\xspace}

\def\given{\;\middle\vert\;}
\def\optimal{\star}
\def\transpose{\intercal}
\DeclareMathOperator{\expectation}{\mathbb{E}}

\newcommand{\todo}[1]{[TODO: #1]}
\newcommand{\termidx}[1]{\index{#1}{\textbf{#1}}}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]

\usepackage{showidx}
\makeindex
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Title (fold)
\pretitle{\begin{center}\cftchapterfont\huge}
\posttitle{\end{center}}
\preauthor{\begin{center}\huge}
\postauthor{\end{center}}
\predate{\begin{center}\large}
\postdate{\end{center}}

\title{Automatic Options Discovery from the Community Structures in the State-Transition Graph}

\author{Pierre-Luc Bacon}
\date{\today}
\renewcommand\maketitlehookb{
\vfill
}
\renewcommand\maketitlehookc{
\vfill
\begin{center}
{
\large
Computer Science\\
McGill University, Montreal
}
\end{center}
\vspace{10mm}
}
\renewcommand\maketitlehookd{
\vspace{10mm}
A thesis submitted to McGill University in partial fulfilment of the requirements of
the degree of Master of Science.
\copyright Pierre-Luc Bacon; \today.
}
% Title (end)

\begin{titlingpage}
\maketitle
\end{titlingpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{tocdepth}{2}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

The idea of \textit{bottleneck}
regions of the interaction graph has been instrumental in the
development of most of these techniques.
The canonical example of bottlenecks can be found in a navigation
task within an environment with multiple-rooms, where subgoals naturally
corresponds to the doorways connecting two rooms (i.e. regions of the state
space). Bottlenecks have been defined as those states which
appear frequently on successful trajectories to a goal but not on
unsuccessful ones~\cite{Mcgovern2001, Stolle2002} or as nodes which
allow for densely connected regions of the interaction graph to
reach other such regions~\cite{Menache2002, Wolfe2005, Kazemitabar2009}.
%To a lesser extent, \cite{Barto2002} could be belong to the latter
%class of approaches by its definition of \textit{access states}
%corresponding to those nodes which allow for reaching difficult
%regions of the interaction graph from other ones.
Bottleneck states are found using graph centrality measures in~\cite{Barto2009, Rad2010} and capture most of the properties outlined
above by defining important nodes as those which lie frequently on
shortest paths between states.

It appears that all of these approaches can also be categorized on
the basis of whether the total reward obtained on a trajectory is
taken into account in the identification of useful subgoals. Solutions that consider only the structural
properties of the interaction graph assume that the state-space
dynamics is the only determining factor in defining options. The other class of approaches treats both dynamics and rewards, accounting for the fact that bottlenecks of
the state-space dynamics do not necessarily correspond to desirable
subgoals to develop for a task.

The idea of sampling trajectories while the agent is interacting
with the environment is central to most approaches. One drawback of this
class of methods is that primitive actions might not suffice to
attain the task goal or yield to successful (defined in a
domain-dependent way) trajectories. Hence, these methods can require large
amounts of data, possibly too expensive to acquire.
%Since the complete state space
%does not have to be represented, they might however be more efficient.
Furthermore, useful solutions might
 lie within sparse subgraphs of the entire interaction
graph (a similar intuition behind manifold embedding techniques of
machine learning).

McGovern \& Barto~\cite{Mcgovern2001} formulate the problem of finding subgoals as
a multiple-instance learning problem over bag of feature vectors
collected by interacting with the environment. Two sets of bags are
obtained in this way, from observations collected
along successful and unsuccessful trajectories respectively. The notion of diverse density is then applied  either by
exhaustive search or gradient descent to find regions of the feature
space with the most positive instances and the least negative ones.
Because negative bags must only contain unsuccessful trajectories,
while positive bags have to contain at least one successful feature
vector, this method is sensitive to noise. 
%The problem of collecting
%successful trajectories would also arise here due if the agent is
%unable to reach satisfactory performances for the task.

In~\cite{Barto2002}, the authors identify subgoals by looking for \textit{access
states} which lead to the regions of the state space that have not
been visited recently and trying to capture the notion
of \textit{relative novelty}
% or what we also qualify as
%\textit{breakthrough} in the solution space. 
%The authors posit that
%the distribution of their measure of relative novelty for subgoals
%differs significantly from the other kind of states. 
The time frame
within which to search for such novelty events is a parameter of
this algorithm and influences the results obtained.

The graph-cut algorithm proposed in~\cite{Menache2002} 
tries to identify access states of those densely connected regions
linked together through a few important edges. Wolfe \& Barto~\cite{Wolfe2005} apply the same graph-cut partitioning but only over local transition
graphs obtained from the sampled trajectories.  
A similar approach of finding strongly connected components
(SCC) of the interaction graph is adopted in~\cite{Kazemitabar2009}.
Since most MDP problems  exhibit ergodicity, the applicability of this algorithm seems limited. Ergodic chains would then  lead to large components being detected by SCC.

By representing the entire interaction graph, Simsek \& Barto~\cite{Barto2009}
compute betweenness centrality to identify important subgoals. They
also propose a potential incremental formulation of their algorithm
that finds local maxima of betweenness within the subgraph obtained
by collecting short trajectories. However, it has also been argued~\cite{Rad2010} that
the centrality measure called \textit{connection graph stability} leads
to better subgoal identification than betweenness or closeness centrality.

The problem of subgoal discovery has only been recently considered under the
formulation of graph clustering. Mathew \& al. ~\cite{Mathew2012} also build an
interaction graph out of which clusters of nodes are found using the PCCA+
~\cite{Weber2004} algorithm. The concept of metastability motivates this approach
and tries to find regions of the state space in which a dynamical system spends more
time. PCCA+ is a spectral clustering technique which comes at a higher cost than the
approach proposed here.

\subsection{Outline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}

\section{Markov Chains}

A discrete-time stochastic process is a family of random variables $\{ X(t), t \in T\}$
(\rvs) indexed by a \textit{time} parameter $t \in \mathbb{N} = \{0, 1, 2,...\}$. The
value of $X_n$ is referred to as the \textit{state} of the process. When the family of
random variables is defined over a probability space for which the sample space
$\Omega$ is discrete, we say that the stochastic process is \textit{discrete-valued}.
Stochastic processes are useful for modelling probabilistic phenomenon involving in
time such as the evolution of the price of a stock over the upcoming month, or the
number of connections to a web service over the day for example. Due to the inherent
complexity many systems, it is often desirable to make simplifying assumptions to
make the exercise tractable. One might then decide for example to treat the \rvs as
\textit{independent and identically distributed} (iid) at the cost of loosing the ability to
capture the correlation structure among the \rvs. The so-called \textit{Markov}
assumption is often used when the iid property is deemed too restrictive, but is yet
simple enough to make computation tractable.

\begin{defn}
A \termidx{Markov Chain} is a discrete-time and discrete-valued stochastic process
that has the Markov Property. The Markov property implies that for all times $n \geq
0$ and all states $i_0, \dots, i, j \in \Omega$:
\begin{equation}
P(X_{n+1} = j | X_0 = i_0, X_1 = i_1, ..., X_n = i) = P( X_{n+1} = j | X_n = i)
\end{equation}
\end{defn}

The Markov (or \textit{memoryless}) could be explained in words by saying that the
\textit{future is independent of the past, given the present state}. Therefore, knowing
the value current value of $X_n = i$ is enough to characterize the future evolution of
the stochastic process $\{X_{n+1}, X_{n+2}, \dots \}$ and the history $\{X_0, X_1,
\dots, X_{n-1} \}$ can be discarded.

A Markov chain is completely specified by the \textit{one-step} transition probabilities
$p_{ij} = P(X_{n+1} = i | X_n = j)$ contained in its \textit{Markov} or
\termidx{stochastic matrix}. For a finite state-space $\Omega$, we have:
\begin{equation}
\mathbf{P} = \begin{bmatrix}
p_{11} & p_{12} & \cdots & p_{1m} \\
p_{21} & p_{22} & \cdots & p_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
p_{m1} & p_{m2} & \cdots & p_{mm}
\end{bmatrix}
\end{equation}

Furthermore, $\mathbf{P} $ must satisfy the following properties:
\begin{equation}
p_{ij} \geq 0 \mbox{ and } \sum_{j=1}^M p_{ij} =1 
\end{equation}

Those elements of $\mathbf{P}$ for which $p_{ii} = 1$ are said to be
\termidx{absorbing} and once entered, the Markov chain the never escapes.

If the elements of $\mathbf{P} $ are independent of the time index, we say that the
Markov chain is \termidx{time-homogeneous} and has \termidx{stationary transition
probabilities}. If this property is satisfied and one knows the transition probabilities of
a Markov Chain, the problem of computing probability distributions is greatly
simplified and can be succinctly expressed using matrix multiplication.  

The \termidx{Chapman-Kolmogorov} equation lets us express the probability of
going from state $i$ to $j$ in $n$ steps by $[\mathbf{P}^n]_{ij} = [
\underbrace{\mathbf{P} \times \mathbf{P} \times \dots \times \mathbf{P}}_{\text{n
times}} ]_{ij}$. The probability of transitioning from state $i$ to any other state under
$n$ steps then becomes:
\begin{equation}
P( \cdot, n | i) = e_i \mathbf{P}^n
\end{equation}
Here $e_i$ stands for the vector with zero components everywhere except of for its
$i$th component set to 1. 

Using the Chapman-Kolmogorov equation, we can classify a state $j$ as being
\termidx{accessible} from $i$ if $[\mathbf{P}^n]_{i,j} > 0$ for some $n \geq 0$. When
the relation holds in both direction, $i$ and $j$ are said to \termidx{communicate}. If
every possible pair of states can communicate, the Markov chain is classified as being
\termidx{irreducible}. Given that a state $j$ has been initially encountered once, we
say that it is \termidx{recurrent} if the probability of visiting it again is nonzero. That
is, denote $T_j$ the time at which the chain returns to $j$, the recurrence property
expresses the fact that $P(T_j < \infty | X_0 = j) > 0$. When $E(T_j | X_0) < \infty$,
state $j$ is said to be \termidx{positive recurrent}.

These last definitions were essential in order to define the \termidx{ergodicity}
property of Markov chains which holds a great importance in the analysis of Markov
Decision Processes which we will see below. 

\begin{defn}
A Markov chain is ergodic if it is irreducible and positive recurrent.
\end{defn}

It occurs often that one is interested in knowing how a given probability distribution
for $X_n$ would evolve as a function of $n$ in a Markov chain.  Denote the
distribution of $X_n$ by the row vector $\mu_n$ and the \termidx{initial distribution}
by $\mu_0$. It can seen that the following relation hold
\begin{equation}
 \mathbf{\mu}_n= \mathbf{\mu}_0 \mathbf{P}^n \label{eq:initial-dist}
\end{equation}
by noting that the Markov property implies that
\begin{align}
P(X_0 &= i_0, X_1 = i_1, X_2 = i_2, \dots, X_n = i_n) = \notag \\
&P(X_0 = i_0) P(X_1 = i_1 | X_0 = i_0) P(X_2 = i_2 | X_1 = i_1) \dots P(X_n = i_n |
X_{n-1} = i_{n-1})
\end{align}
and that our Markov chain is time-homogeneous ($\mathbf{P}$ is fixed).

If it happens that the distribution of $\mu_n$ does not change with $n$, then we say
that the Markov chain is \termidx{stationary}. 

\begin{defn}
A Markov chain said to be stationary if it satisfies $P(X_0 = i_0, X_1 = i_1, \dots, X_n
= i_n) = P(X_m = i_0, X_{m+1}, \dots, X_{m+n})$ for any $m \in \mathbb{Z_+}$.
\end{defn}

Using \ref{eq:initial-dist}, the stationarity problem for a Markov Chain amounts to
finding a vector $\pi$ such that
\begin{equation}
\mathbf{\pi}^\transpose P = \mathbf{\pi}^\transpose
\end{equation}

The stationary distribution $\mathbf{\pi}^\intercal$ can thus be seen as the left
eigenvector of $\mathbf{P}$ associated with the eigenvalue 1. Furthermore, it must be
that $\sum_k \mathbf{\pi}_k = 1$ in order for $\pi$ to be a well-defined probability
distribution. When the Markov chain reaches the stationary distribution, we say that it
is in \termidx{steady-state} mode.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Markov Decision Processes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Markov Decision Processes}
\begin{defn}
\label{defn:mdp}
A finite-action discounted \termidx{Markov Decision Process} (MDP) is a tuple
$\mathcal{M} = \langle X, \mathcal{A}, r, \mathcal{P},\gamma \rangle$ where
$\mathcal{X}$ is a non-empty countable set of states, $\mathcal{A}$ is a finite state
of actions, $\mathcal{P}$ is a transition probability kernel giving rise to a distribution
$P(\cdot |x, a)$ over $\mathcal{X}$ given any $x \in \mathcal{X}$ and $a
\mathcal{A}$,  $r$ is the reward function $r: \mathcal{X} \times \mathcal{A} \mapsto
\mathbb{N}$ and $\gamma \in [0, 1]$ is a discount factor.
\end{defn}

A \termidx{deterministic policy} $\pi$ is a mapping $\pi : \mathcal{X} \mapsto
\mathcal{A}$. In the theory of MDPs, stochastic policies of the form $\pi : \mathcal{X}
\mapsto \Omega(\mathcal{A})$ are also object of study, in which case actions are
drawn from a conditional probability distribution  over states according to $A_t \sim
\pi(\cdot | X_t)$ (here $A_t$ and $X_t$ denote \rvs at time $t$). Markov policies thus
describe a class of behaviors according to which a decision maker might be acting
upon. 

Fixing a policy for an MDP has for effect to induce a \termidx{Markov Reward Process}
(MRP) $\langle \mathcal{X}, \mathcal{R}^\pi, \mathcal{P}^\pi, \gamma \rangle$ with
the reward function $\mathcal{R}^\pi(x) = r(x, \pi(x))$ and state transition probability
kernel $P(\cdot | x) = P(\cdot | x, \pi(x))$. An MRP can be thought as a Markov chain
augmented with rewards over the set of states. The concepts of induced Markov
chains and Markov Reward Processes are particularly important in the analysis of the
dynamics of an MDP. At the level of the induced Markov chain of an MDP, the
existence of a stationary distribution $\rho^\pi$ is not necessarily guaranteed.

\subsection{Value Function}
The notion of a value function is instrumental in supporting the principle of optimality
that underlies the decision theoretic framework of MDPs. It is assumed in this context
that a decision maker, or \textit{agent}, is acting in such a way as to optimize some
intrinsic notion of \textit{goodness} of the exhibited behaviour. From to the definition
\ref{defn:mdp} of an MDP, we use the reward function $r$ to express a quantity
known as the \termidx{return}

\begin{defn}
The return of an \mdp is the total discounted sum of the rewards originating from
the induced \mrp.
\begin{equation}
\mathcal{R} = \sum_{t=0}^\infty \gamma^t R_{t+1} \label{eq:return}
\end{equation}
\end{defn}

Fixing the value of $\gamma < 1$ leads to a notion of return where the immediate
reward is worth exponentially more than the far future. In this case, the resulting MDP 
is then called a \termidx{discounted \mdp}. On the other hand, setting $\gamma =
1$ makes the immediate reward observed at each step equally important and one 
then talks of an \termidx{undiscounted \mdp}

Knowing the \textit{desirability}, or value, of a state at any moment will greatly
simplify the problem of deriving an optimal behavior. 

\begin{defn}
The \termidx{value function} $V^\pi: \mathcal{X} \mapsto \mathbb{R}$ of a
stationary policy $\pi$ is the conditional expectation of the return (discounted or
undiscounted) given a state.
\begin{equation} 
V^\pi(x) = \expectation \left[ \sum_{t=0}^\infty \gamma^t R_{t+1}  \given  X_0 =
x\right] , x \in \mathcal{X} \label{eq:value-function}
\end{equation}
\end{defn}

%It is important to observe the following recursive property which stems from
%equation \ref{eq:value-function}
%\begin{align*}
%V^\pi(x) &= \expectation \left[ R_{t+1} + \gamma \sum_{t=0}^\infty \gamma^t
%R_{t+2}  \given X_0 = x\right] \\
%V^\pi(x) &= \expectation \left[ R_{t+1} + \gamma V^\pi(X_1 = x_1) \given X_0 =
%x\right]
%\end{align*}

An optimal behavior is achieved if the value function of its policy is optimal. An
optimal value function $V^*: \mathcal{X} \mapsto \mathbb{R}$ is one which obtains
the maximum possible expected return for every state.

% Action value function 
The \termidx{action-value function} of a policy $\pi$ is tightly related to the notion of
value function presented above. Instead of being defined only over states, it specifies
the expected return of initially being in state $x$ choosing a first action $a$ and
following $\pi$. 
\begin{defn}
The action-value function $Q^\pi: \mathcal{X} \times \mathcal{A} \mapsto
\mathbb{R}$ of a stationary policy $\pi$ is the conditional expectation of the return
given an initial state and action pair. 
\begin{equation}
Q^\pi(x, a) = \expectation \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \vert X_0 = x,
A_0 = a \right], \; x \in \mathcal{X}, a \in \mathcal{A}
\end{equation}
\end{defn}

If the action-value function yields the maximum expected return for every state
action pair, we say that it is optimal and denote it by $Q^*$. One can go from the
action-value function to the value-function by noting that for finite-action MDP
(definition \ref{defn:mdp})
\begin{equation}
V^*(x) = \max_{a \in \mathcal{A}} Q^*(x, a) \label{eq:qopt-to-vopt}
\end{equation}

In the more general case of a countable non-empty sets of actions, the $\max$
operator should be replaced with the supremum ($\sup$) of $\mathcal{A}$. The
optimal action-value function can also be recovered from the value-function as
follow:
\begin{equation}
Q^*(x,a) = r(x,a) + \gamma \sum_{y \in \mathcal{X}} P(y, a, x) V^*(y), \; x \in
\mathcal{X}, a \in \mathcal{A} \label{eq:vopt-to-qopt}
\end{equation}

\subsection{Bellman Equations}
Similar to equation \ref{eq:vopt-to-qopt}, given an MDP $\mathcal{M} = \langle
\mathcal{X}, \mathcal{A}, \mathcal{P}, \gamma \rangle$ we can write its so-called
\termidx{Bellman equations}

\begin{equation}
V^\pi(x) = r(x, \pi(x)) + \gamma \sum_{y \in \mathcal{X}} P(y, \pi(x), x) V^\pi(y)
\label{eq:bellman-equations}
\end{equation}

The recursive formulation of the Bellman equations allows to relate the value of state
to that of its possible successor states, weighted by their probability of occurrence in
an average.  If we have a finite d-dimensional state space $\mathcal{X}$, $V^\pi$ and
$r^\pi$ can be thought as a vectors in $\mathbb{R}^d$ with $\mathcal{P}$ being a
transition matrix $\mathbf{P}: \mathbb{R}^{d \times d}$. We can then see that the
Bellman equations (\ref{eq:bellman-equations}) define a linear system of equations in
$d$ unknows whose unique solution is $V^\pi$

\begin{equation}
\mathbf{V}^\pi = \mathbf{r}^\pi + \gamma \mathbf{P}^\pi \mathbf{V}^\pi
\end{equation}

Solving for the left hand side we get

\begin{equation}
\mathbf{V}^\pi = (\mathbf{I} - \gamma \mathbf{P}^\pi)^{-1} \mathbf{r}^\pi
\label{eq:direct-value}
\end{equation}

Under the framework of reinforcement learning that we adopt in this work,
$\mathbf{P}$ and $\mathbf{r}^\pi$ are not available \textit{a priori}, making the
direct solution of \ref{eq:direct-value} impossible to compute. Furthermore, since
matrix inversion is generally of the order of $\mathcal{O}(n^3)$, the computational
cost will quickly become impractical for large state spaces. Iterative methods are thus
preferred under almost any practical scenario. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bellman Operator
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bellman Operator}
The notion of \termidx{Bellman operator} subtends the theoretical justifications of the
indirect (as opposed to \ref{eq:direct-value}) methods for computing the value
function of policies. 

\begin{defn}
The Bellman operator underlying the policy $\pi$ of an MDP, $T^\pi:
\mathbb{R}^\mathcal{X} \mapsto \mathbb{R}^\mathcal{X}$, is defined as

\begin{equation}
(T^\pi V) = r(x, \pi(x)) + \gamma \sum_{y \in \mathcal{X}} P(y, \pi(x), x) V(y), \; x
\mathcal{X} \label{eq:bellman-operator}
\end{equation}
\end{defn}

Similarly, one can define the Bellman operator $T^\pi : \mathbb{R}^{\mathcal{X}
\times \mathcal{A}} \mapsto \mathbb{R}^{\mathcal{X} \times \mathcal{A}}$  for the
action-value function induced by a policy $\pi$
\begin{equation}
T^\pi Q(x, a) = r(x, a) + \gamma \sum_{y \in \mathcal{X}} P(y, a, x) V(y), \; x \in
\mathcal{X} \label{eq:bellman-operator-action-value}
\end{equation}

At first blush, it can be easy to miss the difference between the overall form of the
Bellman equations (\ref{eq:bellman-equations}) and that of equation \ref{eq:bellman
operator}. One must in fact observe that, even though $T^\pi$ is defined for a given
policy, the $V$ term is general and might not need to correspond to $V^\pi$. 
The Bellman operator happens to be a special type of function called a
\termidx{contraction mapping}. This characterization allows the \termidx{Banach
fixed-point theorem} to be used to show convergence properties.  Starting with an
estimate of the value function of a policy, the iterative application of the Bellman
operator will converge in the limit to a unique fixed point corresponding to $V^\pi$.

The same principle will underlie the value and policy iteration algorithms for
computing the optimal value function. Before delving into the details of these
algorithms, the \termidx{Bellman optimality operator} must be introduced. Just as
$T^\pi$ (\ref{eq:bellman-operator}), $T^\optimal$ is a maximum-norm contraction
mapping $T^\optimal: \mathbb{R}^\mathcal{X} \times \mathbb{R}^\mathcal{X}$ but 
is defined this time as
\begin{equation}
(T^\optimal V) = \max_{a \in \mathcal{A}} \left\lbrace r(x, a) + \gamma \sum_{y
\mathcal{X}} P(y, \pi(x), x) V^\optimal (y) \right\rbrace, \; x \in \mathcal{X} 
\label{eq:bellman-optimality-operator}
\end{equation}

The optimal value function $V^\optimal$ satisfies the fixed-point equation
$T^\optimal V^\optimal = V^\optimal$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Solving MDPs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Solving MDPs}

The Banach fixed-point theorem introduced above laid out the foundations for the
two main approaches to solving MDPs: the value and policy iteration algorithms. By
\textit{solving and MDP}, one generally refers to finding the optimal value function
underlying an MDP. If the the value function found in that way is indeed optimal, an
optimal policy can be derived by greedily picking the best action. These two main
problems -- finding the value of a policy or finding an optimal policy -- are usually
distinguished under the names of \termidx{prediction} (figure \ref{fig:prediction
problem}) and \termidx{control} (figure \ref{fig:control-problem}). In the control
problem, one tries to derive an optimal policy by considering the greedy policy with
respect to the optimal value function (see \cite{Ross1983} for a proof).

\begin{equation}
\pi^\optimal(x) = \arg \max_{a \in \mathcal{A}} \left[ r(x,a) + \gamma \sum_{y
\mathcal{X}} P(y, a, x) V^*(y)\right], \; x \in
\mathcal{X}, a \in \mathcal{A} \label{eq:greedy-optimal-policy}
\end{equation}

\begin{figure}
\centering
\input{fig/spaces}
\caption{The policy evaluation problem consists in finding the value-function
corresponding to a given policy $\pi$. We restrict our attention to the space of
stationary policies $\Pi_{stat}$.}
\label{fig:prediction-problem}
\end{figure}

\begin{figure}
\centering
\input{fig/control}
\caption{The control problem aims at finding an optimal policy i.e. one for which the
corresponding value function is optimal. There might be multiple optimal policies, but
the optimal value function must be unique\cite{Ross1983}.}
\label{fig:control-problem}
\end{figure}

Finding $V^*$ as a first step would thus allow the control problem to be solved using
equation \ref{eq:greedy-optimal-policy}. The \termidx{value-iteration} algorithm is
one way in which $V^\optimal$ can be obtained. Let $V_0$ be some arbitrary initial
bounded function, this method consists in applying Bellman optimality operator
$T^\optimal$ successively in the following manner
\begin{equation}
V_{k+1} = T^\optimal V_k \label{eq:value-iteration}
\end{equation}

$V_k$ can be shown to uniformly converge to $V^*$ as $k \to \infty$ \cite{Ross1983}.

With \termidx{policy iteration}, two steps are interleaved: policy evaluation and
policy improvement. The general idea goes as follow: from an initial policy $\pi_0$
compute its corresponding value function (policy evaluation) and derive the greedy
policy $\pi_{k+1}$ from it (policy improvement), then repeat these two steps as
necessary. Fixing the number of iteration to $k$, the policy computed by policy
iteration at step $k$ can be shown to be not worse than the greedy policy computed
by value-iteration. Because of the policy evaluation step, policy-iteration is
computationally more expensive. It plays however an important role in the Actor
Critic architectures \cite{Sutton1984}.

The policy and value iterations algorithms belong to the class of algorithms known as
\termidx{dynamic programming} (DP) methods. Assuming a perfect knowledge of the
transitions dynamics and reward function, they can greatly reduce the computational
burden compared to a direct policy search approach which would be of the order of
$\left\vert \mathcal{X} \right\vert^{\left\vert \mathcal{A} \right\vert}$. We will
however be interested in problems in which these assumptions are no longer valid and
additional techniques will have to be used. DP is still of great importance for the
theoretical underpinning that it provides to the understanding of reinforcement
learning approaches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Reinforcement Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reinforcement Learning}

Reinforcement Learning (RL) considers the problem setting of a situated
\termidx{agent} trying to learn to optimize a loss (return) from the direct experience
provided by the \termidx{environment} (figure \ref{fig:rl}). Historically, the field has
been influenced greatly by the work on trial-and-error learning from the field of
psychology \todo{CITE}, consequently shaping the type of learning problems it tries to
solve. RL hinges heavily upon the theory of \mdps introduced in the previous section,
and shares the same goal as DP of solving the control problem. 

\begin{figure}
  \centering
  \input{fig/rl}
  \caption{Reinforcement Learning. An agent executes an action $a_t$ in the
  environment at time $t$ producing a state transition and instantaneous reward as
  determined by the environment dynamics.}
  \label{fig:rl}
\end{figure}

The assumptions an \termidx{environment model} being known is however usually
lifted and makes RL mainly focused on \termidx{model-free} learning. The presence
of a model can still be accommodated in this framework and will subtend a set of RL
methods for \termidx{planning}: determining the best course of action for
accomplishing a goal by simulating the consequences of actions in the model.
Additionally, RL is concerned with the problem of acquiring relevant experience
from the environment in an \termidx{online} fashion: a problem of joint
\termidx{exploration} and control. The way in which the acquired experience relates
to the target policy will differ depending on the learning algorithms which will either
be classified as \termidx{on-policy} or \termidx{off-policy}. In the following, the
Monte-Carlo (MC) method will be presented as a way to learn about the value of
states in the absence of a model. Then the Temporal Difference (TD) learning
algorithm will be introduced and shown to encompass the MC methods. Finally, the
off-policy Q-Learning algorithm will be presented. 

\subsection{Monte-Carlo}
The Monte-Carlo approach to solving the control problem tries to estimate the value
of a state by sampling online the return (equation \ref{eq:return}) directly from the
environment or through simulated experience using a model. Working under the
assumption of an episodic task, independent samples of return are averaged to
obtain an estimate of the true expectation. According to the law of large number, as
the number of samples goes to infinity, the average becomes an unbiased
estimator. 

The procedure described above is however not sufficient for solving the control
problem and only answers the prediction one. In the absence of a model, it will be
necessary to estimate the value of an action-value function. Since certain state
action pair might be difficult to sample frequently enough in large state spaces, it
is often assumed that the agent can be reset in some arbitrary state-action
configuration (an \termidx{exploring start}). In practice, this assumption turns out to
be prohibitively restrictive and \termidx{soft-policies} must be introduced to overcome
this limitation. 

The Monte-Carlo method for control is based on the general framework of policy
iteration where the current policy is improved greedily with respect to some
estimate of its value function. Since the only requirement is to update the policy
toward the greedy policy, it is possible to only consider a slightly perturbed instance
of the greedy policy. This restriction being dispensed, soft-policies where $\pi(x, a)
> 0$ can be used instead. A class of soft policies commonly used is the
\termidx{$\epsilon$-greedy} one, where a random action is chosen with probability
$\epsilon$, and the greedy action for $1 - \epsilon$. Since non-greedy actions can
now be explored, the need for exploring starts is eliminated \cite{SuttonBarto1998}.

The approach described so far belongs to the class of on-policy learning algorithms
where the value of a policy is simultaneously being evaluated and used for control.
If the policy used to obtain samples, the \termidx{behavior policy}, is different from
the one being evaluated and improved upon, the \termidx{estimation policy},
\termidx{importance sampling} must be introduced to compensate for the
discrepancy in the action selection distributions. The resulting algorithm is said to
be capable of doing off-policy learning.

\subsection{Temporal Difference Learning}
The main drawback in using Monte-Carlo methods in an on-line setting has to do with
the need to wait for the complete execution of an episode in order to update the value
function estimate. The Temporal Difference (TD) learning algorithm laid out	 the
foundations of RL by using immediate prediction as target, a technique known as
\termidx{bootstrapping}, instead of waiting for a full backup to be performed as with
MC methods. The TD($\lambda$) is a extension of the original algorithm
\cite{Sutton1984} that unifies DP and Monte-Carlo methods.

Recalling the definition of the value function given in \ref{eq:value-function}, we have
that for all $x \in \mathcal{X}$
\begin{align}
V^\pi(x) &= \expectation \left[ \sum_{t=0}^\infty \gamma^t R_{t+1}  \given  X_0 =
x\right] \notag \\
&= \expectation \left[ R_{t+1} + \gamma \sum_{k=0}^\infty \gamma^k
R_{t+k+2}\given X_t = x\right]  \notag \\
&= \expectation \left[ R_{t+1} + \gamma V(X_{t+1}) \given X_t = x\right]
\label{eq:value-function-recursive}
\end{align}

The TD(0) algorithm incrementally updates the value function estimate using samples
of the form $R_{t+1} + \gamma V(X_{t+1})$ exposed in equation 
\ref{eq:value-function-recursive}
\begin{equation}
\hat{V}_{t+1}(x) = \hat{V}_{t}(x) + \alpha_t \left[ R_{t+1} + \gamma \hat{V}_t (X_{t+1})
- \hat{V}_t(X_t)\right] \label{eq:tdupdate}
\end{equation}

This way of updating the value function estimate is known as a Stochastic
Approximation (SA) method and can be shown to converge \cite{Csaba2010} to the
true $V^\pi$ by treating the sequence $\hat{V}_t$ as a linear ordinary differential
equation (ODE). Algorithm \ref{alg:td0} shows how the TD update is performed while
interacting with the environment.

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized vector $V$, a policy $\pi$ to be evaluated}
\KwResult{The value of $\pi$}
$x \leftarrow$ initial state\;
\While{x is not terminal} {
$a \leftarrow \pi(x)$\;
$r, x^\prime \leftarrow Step(a)$\;
$V(x) \leftarrow V(x) + \alpha\left[ r + \gamma V(x^\prime) - V(x) \right]$ \;
$x \leftarrow x^\prime$
}
\caption{Tabular TD(0) algorithm for policy evaluation. The $Step$ function performs
the state transition in the environment and returns the immediate reward.}
\label{alg:td0}
\end{algorithm}

The sequence of step sizes $\alpha$ is subject to the Robbins-Monro (RM) conditions
according to which
\begin{equation}
\sum_{t=0}^\infty \alpha_t = \infty, \hspace{5mm} 
\sum_{t=0}^\infty \alpha_t^2 < + \infty
\end{equation}

\subsection{Eligibility Traces}

An important extension to the original TD algorithm is the TD($\lambda$) family
\cite{Sutton1984}, unifying TD(0) at one end and Monte-Carlo prediction at the other.
\termidx{Eligibility traces} act as kind of memory which has for effect to weight the
propagated backups at a given state. Instead of updating the value function based on 
a single n-steps estimate, TD($\lambda$) computes and an average, known as the
$\lambda$-return, over a range of multi-step predictions of the return. The multi
step discounted return is first defined as
\begin{equation}
\mathcal{R}_{t:k} = \sum_{s=t}^{t+k} \gamma^{s-t}R_{s+1} +
\gamma^{k+1}\hat{V}_t(X_{t+k+1})
\end{equation}

The $\lambda$-return is a mixture of multi-step return with weight $(1 -
\lambda)\lambda^k$ on each term
\begin{equation}
\mathcal{R}_t^\lambda = (1 - \lambda)\sum_{k \geq 0} \mathcal{R}_{t:k}
\label{eq:lambda-return}
\end{equation}

As $\lambda$ goes to 0, equation \ref{eq:lambda-return}  simplifies to $R_{t+1} +
\gamma \hat{V}_t (X_{t+1})$ and amounts to a one-step TD backup of the TD(0)
algorithm. On the other end of the spectrum, as $\lambda$ goes to 1, we get a sample
of the return from time $t$ until the end of the episode as in the MC method.

TD($\lambda$) is implemented using an additional vector of size $\left\vert
\mathcal{X} \right\vert$ where each component gets incremented by 1 each time the
corresponding state is visited. A decay of $\lambda \gamma$ is also applied upon
each component at each time step. 

\subsection{Sarsa}
The TD(0) algorithm only solves the policy evaluation problem and the question of
control must now be answered. Following the general policy iteration paradigm and
the MC algorithm for control, SARSA is an on-policy control algorithm making use 
TD(0) for policy evaluation under a soft policy exploration strategy. Similar to equation
\ref{eq:tdupdate}, the action-value function is updated as follow
\begin{equation}
\hat{Q}_{t+1}(x, a) = \hat{Q}_{t}(x, a) + \alpha \left[ R_{t+1} + \gamma \hat{Q}_t
(X_{t+1}, A_{t+1}) - \hat{Q}_t(X_t, A_t)\right] \mathbb{I}_{X_t = x, A_t = a}
\label{eq:sarsaupdate}
\end{equation}

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized matrix $Q$}
\KwResult{An optimal action-value function for control}
\ForEach{episode}{
$x \leftarrow$ initial state\;
$a \leftarrow Greedy(x)$\;
\While{x is not terminal} {
$r, x^\prime \leftarrow Step(a)$\;
$a^\prime \leftarrow Greedy(x^\prime)$ \;
$Q(x, a) \leftarrow Q(x, a) + \alpha\left[ r + \gamma Q(x^\prime, a^\prime) - Q(x, a)
\right]$ \;
$x \leftarrow x^\prime$ \;
$a \leftarrow a^\prime$
}
}
\caption{The on-policy Sarsa algorithm based on a TD(0) policy evaluation scheme.
The $Greedy$ function is the soft greedy policy derived from the current estimate of
the action-value function. An $\epsilon$ greedy exploration strategy would be
commonly used.}
\label{alg:sarsa}
\end{algorithm}

As the number of samples for each state-action pair goes to infinity, $\lim_{t \to
\infty} \epsilon = 0$, and under the RM conditions, Sarsa is guaranteed to converge to
an optimal policy \cite{Sutton1998}. 

\subsection{Q-Learning}

Since TD learning is an on-policy learning method, the choice of exploration strategy
will directly impact the convergence to the estimation policy. Q-Learning
\cite{Watkins1989} decouples the exploration and evaluation problems and allows for
any behavior policy to be followed while still converging to $Q^\optimal$. In this case,
the update to the action-value function estimate consists in
\begin{equation}
Q_{t+1} (x,a) = Q_t(x,a) + \alpha_t \left[ R_{t+1} + \gamma \max_{a^\prime \in
\mathcal{A}} Q_t(X_{t+1}, a^\prime) - Q_t(X_t, A_t) \right] \mathbb{I}_{X_t = x, A_t = a} 
\label{eq:qlearning-update}
\end{equation}

The corresponding procedural form is given in algorithm \ref{alg:qlearning}.

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized matrix $Q$}
\KwResult{An optimal action-value function for control}
\ForEach{episode}{
$x \leftarrow$ initial state\;
\While{x is not terminal} {
$a \leftarrow Greedy(x)$\;
$r, x^\prime \leftarrow Step(a)$\;
$Q(x, a) \leftarrow Q(x, a) + \alpha \left[ r + \gamma \max_{a^\prime \mathcal{A}}
Q(x^\prime, a^\prime) - Q(x, a) \right]$ \;
$x \leftarrow x^\prime$
}
}
\caption{The Q-Learning algorithm under some arbitrary exploration scheme.
$\epsilon$-greedy could once again be used for that task.}
\label{alg:qlearning}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% State Abstraction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{State-Abstraction}
The algorithm presented so far assumed \textit{tabular} updates rules where value or
action-value functions are expressed as vectors and matrices respectively. For many
problems, the set of states might have infinite cardinality or simply too large to fit in
memory. Furthermore, as the number of entries increases, the computational expense
would also increase accordingly. It matters then to seek for a representation of the
value function capable of generalizing across (possibly unseen) states or state-action
pairs.

By expressing the value function as a parametrized function, the incremental updates
would now be performed upon the entries of some parameter vector $\theta$. For
large state spaces, the number of components in $\theta$ will be much smaller than
the number of possible states -- rendering the problem more tractable. Value function
approximation could then be seen as an instance of a supervised learning problem
with a training set consisting of pairs of state and samples of the return (one-step TD,
or full Monte-Carlo).

Any existing method from supervised learning could potentially be used for that tasks:
neural networks, k-nearest neighbor for regression for example. TD-Gammon
\cite{Tesauro1995} is deemed as one of the great success story in the field and made 
use of neural networks for value function approximation. In this work, the attention
will be mainly drawn upon the so-called \textit{linear methods}. A simple, but yet
expressive, family of function approximators is of the form

\begin{equation}
V_{\theta}(x) = \theta^\transpose \varphi(x)
\label{eq:linear-methods}
\end{equation}

The so-called linear methods seek a representation of the value function as a linear
combination of \termidx{features}. In equation \ref{eq:linear-methods}, $\theta
\mathbb{R}^d$ and $\varphi : \mathcal{X} \to \mathbb{R}^d$ with the components
being defined by a set of \termidx{basis functions} $\varphi_i : \mathcal{X} \to
\mathbb{R}$. The way in which $\varphi$ is obtained specifies a feature extraction
method which needs not be linear. 

Value function approximation most commonly relies on \termidx{gradient-descent}
methods to derive the proper update that must be applied on the parameter vector at
each step. Because of the simple form of equation \ref{eq:linear-methods}, taking the
gradient of the value function with respect to $\theta$ yields

\begin{equation}
\nabla_{\theta} V_\theta(x) = \varphi(x)
\label{eq:linear-gradient}
\end{equation}

Using the gradient obtained in equation \ref{eq:linear-gradient}, gradient-descents
methods update the components of $\theta$ by small increments in the direction of
the steepest error reduction. A typical error function which is minimized by supervised
learning techniques is the mean-squared error (MSE). Casting the function
approximation problem under a setting where the true (yet unknown) underlying value
function $V^\pi$ is a target, and $V_\theta$ is the best approximation using the
parametrized form of equation \ref{eq:linear-methods}, the MSE is 

\begin{equation}
MSE(\theta) = \sum_{ x \in \mathcal{X}} P(x) \left[ V^\pi(x) - V_\theta(x) \right]
\end{equation}

The $P(x)$ term here accounts for probability of observing a given state $x
\mathcal{X}$ as an input.

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized vector $\theta$, a policy $\pi$ to be evaluated}
\KwResult{The value of $\pi$}
$x \leftarrow$ initial state\;
\While{x is not terminal} {
$a \leftarrow \pi(x)$\;
$r, x^\prime \leftarrow Step(a)$\;
$\delta \leftarrow r + \gamma \theta^\transpose \varphi[x^\prime] -
\theta^\transpose \varphi[x]$\;
$\mathbf{z} \leftarrow \varphi[x] + \gamma \lambda \mathbf{z}$ \;
$\theta \leftarrow \theta + \alpha \delta z$\;
$x \leftarrow x^\prime$
}
\caption{TD($\lambda$) with linear function approximation}
\label{alg:linear-tdlambda}
\end{algorithm}

TD($\lambda$) with linear function approximation (algorithm \ref{alg:linear
tdlambda}) can be shown to converge under the usual RM conditions on the $\alpha$
parameter \cite{Tsitsiklis1997}.

\subsection{Basis functions}

The choice of proper function space and feature extraction techniques still remains a
challenging problem which can have a great impact on the quality of the value function
being approximated. If one has prior knowledge on the shape, or \textit{regularities}
of the optimal value function under the problem at hand, relevant features could be
specified explicitly. However, when facing the problem of solving a broad class of
problems for which little is known about their structure, general set of basis functions
would be sought. Two feature extraction schemes are presented below for their wide
applicability and ease of use. 

\subsection{Radial Basis Functions}

Given a state $x \in \mathcal{X}$, each component of the $\varphi$ is set to the
distance of its corresponding basis function for which $\varphi_i(x_i) = \varphi_i(\|x -
c_i\|)$ by the radial property and where $c_i$ specifies the \textit{center}. Gaussian
functions are most commonly used as radial basis functions (RBF) and are defined as

\begin{equation}
\varphi_i(x) = \exp\left( \frac{\| x - c_i\|}{2\sigma_i^2}\right)
\label{eq:rbf}
\end{equation}

The $\sigma_i$ parameter specifies the \textit{width} of the Gaussian and must be
tuned by hand or using some through some model selection technique along with the
centers and number of basis functions. Decreasing the width and increasing the
number of basis functions would result in finer expressibility but also incur a higher
computational cost. 

The choice of norm in equation \ref{eq:rbf} is not restricted to the Euclidean distance
and other metric could be used. In \cite{Sugiyama2008} for example, the Geodesic
distance taken over the graph induced by some MDP is used in an attempt to better
capture the intrinsic geometry of the manifold.
 
\subsection{Fourier Basis Functions}

The use of fourier basis functions for value function approximation was introduced in
\cite{Konidaris2011b} but relies on the well established theory or the Fourier analysis.

The nth order Fourier expansion of some univariate periodic function $f$ with period
$T$ is given by
\begin{equation}
\hat{f}(x) = \frac{a_0}{2} \sum_{k=1}^n \left[ a_k \cos\left(k\frac{2\pi}{T} x\right) +
b_k \sin \left(k \frac{2\pi}{T}x \right) \right]
\label{eq:fourier}
\end{equation}

with Fourier coefficients $a_k$ and $b_k$ defined as
\begin{equation}
a_k = \frac{2}{T} \int_0^T f(x) \cos \frac{2\pi kx}{T}dx, \; \mbox{and} \; b_k = \frac{2}
{T} \int_0^T f(x) \sin \frac{2\pi kx}{T}dx
\end{equation}

Since $V^\optimal$ is unknown, the Fourier coefficients cannot be obtained directly
and must be thus treated as \textit{weights} in a spirit similar to the other linear
methods. The Fourier expansion of $f$ results in $2n+1$ terms but
\cite{Konidaris2011b} shows how it can be simplified to only $n+1$ terms if some
assumptions are made on the periodicity of the value function. A function $f$ is even if
$f(x) = f(-x)$, in which case the $\sin$ terms of equation \ref{eq:fourier} can be
droped. A similar observation can be made if $f$ is odd $-f(x) = f(-x)$ and the
$\cos$ terms can be omitted. 


Setting the period to $T=2$, the \textit{nth order Fourier basis} is thus defined as:
\begin{equation}
\phi_i(s) = \cos(i\pi x) \hspace{2mm} \forall i = 0, ..., n
\end{equation}

The multivariate Fourier expansion of some function $F$ with period T up to order $n$
bears a similar form
\begin{equation}
\hat{F}(\mathbf{x}) = \sum_{\mathbf{c}} \left[ a_c \cos \left( \frac{2\pi}{T} \mathbf{c}
\cdot \mathbf{x} \right) +  b_c \sin \left( \frac{2\pi}{T} \mathbf{c} \cdot \mathbf{x}
\right)\right]
\label{eq:multi-fourier}
\end{equation}

The $\mathbf{c}$ term in equation \ref{eq:multi-fourier} is the Cartesian power 
$\mathcal{X}^d$ of the d-dimensional state space such that $\mathcal{c} = [c_1,
\dots, c_i, \dots, c_d]$ where $c_i \in [0, \dots, n]$. The number of basis functions
required for the nth order expansion under this scheme would then be $2(n+1)^d$.
Fortunately using the same argument as for the univariate case, only half of the terms
must be kept and the ith basis function is defined as
\begin{equation}
\varphi(x)_i = \cos \left(  \pi \mathbf{c}^i \cdot \mathbf{x} \right)
\end{equation}

While Fourier basis severely suffer from the curse of dimensionality, the approach has
the merit of being simple and effective as suggested by empirical evidences in this
work and in \cite{Konidaris2011b} . The only choice a practitioner has to make is the
order of the expansion and reasonable results can usually obtained by using only the
few first lower frequencies.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Temporal Abstraction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Temporal Abstraction}

It has been shown in the previous section how the value function over large 
(possibly infinite)
state spaces can be represented compactly using function approximation. It would
then appear natural to harness a similar idea for exploiting \textit{temporal
regularities} of the problem at hand. In the context of planning, reasoning at different
abstract time scales can drastically reduce the time complexity for finding a solution. 
The importance of temporal abstraction can be appreciated easily under everyday
scenarios such as preparing coffee, going on a camping trip, changing a bicycle tire,
etc. When preparing an Espresso, a Barista might need to make use of her
\textsc{turn-on-the-espresso-maker} ability, followed by \textsc{grind-coffee-beans},
\textsc{tamp-the-espresso}, \textsc{lock-the-portafilter}, and \textsc{pull-a-shot}.
Capitalized expressions where used in this last example to highly the fact that these
\textit{actions} last arbitrary amount of time and should rather be seen as
\textit{closed-loop} policies. Although the tasks of preparing coffee might seem trivial
even for the inexperienced, it does rely on a complicated sequence of \textit{atomic}
decisions procedures which would be difficult to obtain with absolutely no prior
knowledge. 

Many formalisms have been proposed to cast this problem efficiently in the framework
of reinforcement learning: the MAXQ method \cite{Dietterich1998} and Hierarchical
Abstract Machines (HAM) \cite{Parr1997} or Macro-Actions \cite{Hauskrecht1998} for
example. These other approaches will not be covered here as the options framework of 
\cite{Sutton1999} will be adopted in this work. For a survey of the earlier attempts, the
interested reader is referred to \cite{StolleThesis2004, Sutton1999}.

\section{Options Framework}
\begin{defn}
An \termidx{Option} is a triple $\langle \mathcal{I} \subseteq \mathcal{X},
\mathcal{\pi} : \mathcal{X} \to \Omega(\mathcal{A}), \beta : \mathcal{X} \to [0, 1]
\rangle$ consisting of an initiation set, a policy $\pi$ and a termination condition
$\beta$.
\end{defn}

Given a state $x \in \mathcal{X}$ and option $o \in \mathcal{O}$, $o$ can be
executed in a \textit{call-and-return} fashion up to termination, dictated by $\beta$,
only if $x \in \mathcal{I}$. The initiation component is meant to facilitate decision
taking by reducing the size of possible options to consider for any given state. Its
presence also allows easier bridging with the previous work in STRIPS-style
\cite{Fikes1972} planning. In many efforts to use the options framework \todo{cite an
example}, the definition of this component is neglected and it is assumed that options
are available everywhere. The present work however proposes a method to alleviate
this difficulty by automatically curtailing the set of admissible states.

The options framework is at the crossroad of regular \mdps and semi-\mdps
\cite{Bradtke1994}. It considers a base \mdp overlaid with variable length course of
action represented as options. It is shown in \cite{Sutton1999} (theorem 1) how an
MDP and pre-defined set of options form a semi-\mdp and can thus reuse most of the
known theory on the subject. As opposed to the general theory of SMDP that treats
extended actions as indivisible and \textit{opaque} decision units, the options
framework also allows to look at the structure \textit{within}. It can also accommodate
both Markov options and semi-Markov ones. In the example coffee domain presented
above, \textsc{pull-a-shot} would be an option defined by a \textit{timeout} and not
only by the current state. It would then most likely be semi-markov option unless the
state representation is changed to support the non-markovianity.

With this formulation, primitive actions can be seen as a special kind of options which
is available everywhere, has a policy always choosing the same action, and lasting
exactly one step. 

\section{Bellman Equations for Options}

Let $\mu : \mathcal{X} \times \mathcal{O} \to [0, 1]$  be a general (Markov or semi
Markov) policy where $\mathcal{O}$ is a set of options (containing primitive actions or
not). The value of $\mu$ is defined as

\begin{equation}
V^\mu(x) = \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k
V^\mu(X_{t+k})  \given \varepsilon(x, \mu, t) \right]
\label{eq:value-option-policy}
\end{equation}

where $k$ is a random variable denoting the duration of the first option selected by
$\mu$ and $\varepsilon(x, \mu, t)$ is the event that the option policy $\mu$ is
executing at time $t$ in state $x$. Once $\mu$  terminates, it presumably selects  a
different option and the process is repeated until the episode terminates. Similarly, we
can define an \termidx{option value function} for control as 
\begin{align}
Q^\mu(x, o) &= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} +
\gamma^k V^\mu(X_{t+k})  \given \varepsilon(x, o, t) \right] \notag \\
&= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \sum_{o^\prime
\mathcal{O}_x} \mu(X_{t+1}, o^\prime)Q^\mu(x, o^\prime) \given \varepsilon(x, o,
t) \right]
\label{eq:option-value-function}
\end{align}

$\mathcal{O}_x$ in the equation \ref{eq:option-value-function} is the subset of
options which can be initiated under state $x$. The optimality principle is carried over
the following set of optimal Bellman equations

\begin{equation}
V_{\mathcal{O}}^\optimal(x) = \max_{o \in \mathcal{O}_x} \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k V_{\mathcal{O}_x}^\optimal(X_{t+k})  \given \varepsilon(x, o, t) \right]
\end{equation}

The \termidx{optimal option value function} is in turn

\begin{align}
Q_{\mathcal{O}}^\optimal(x, o) &= \expectation \left[ R_{t+1} + \dots +
\gamma^{k-1}R_{t+k} + \gamma^k V_{\mathcal{O}_x}^\optimal(X_{t+k})  \given
\varepsilon(x, o, t) \right] \notag \\
&= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k
\max_{o^\prime \in \mathcal{O}_{x_{t+k}}} Q_\mathcal{O}^\prime(o^\prime, X_{t+k}) 
\given \varepsilon(x, o, t) \right]
\label{eq:optimal-option-value-function}
\end{align}

If the optimal option value function (eq \ref{eq:optimal-option-value-function}) is
available, the greedy policy is guaranteed to be optimal. A policy over options is
optimal if given a set of options, its underlying option value function is the optimal
value function, i.e. $V^{\mu_\mathcal{O}^\optimal}(x) = V_\mathcal{O}^\optimal(x)\;
\forall x \in \mathcal{X}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Learning Behavior Policies
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Learning Behavior Policies}

Assuming that a options set $\mathcal{O}$ is specified, existing theory
\cite{Puterman1994, Bradtke1994} from semi-\mdps in continuous time can be
applied to solve the control problem. The SMDP model considers decision making at
\textit{decision epochs} spaced by random time intervals during which the system
dynamics are of no interest. The optimal control problem can be solved by an
extension of Q-learning to the SMDP case.

\begin{defn}{SMDP Q-Learning}
\begin{equation}
Q_{t+1}(x, o) \leftarrow Q_{t}(x, o) + \alpha \left[ \mathcal{R} + \gamma^k
\max_{o^\prime \in \mathcal{O}_x^\prime} Q_{t}(x^\prime, o^\prime) -
Q_t(x, o) \right]
\label{eq:smdp-qlearning}
\end{equation}
\end{defn}

The semantics of equation \ref{eq:smdp-qlearning} holds the under the event of
$\varepsilon(x, o, t)$ where option $o$ is executed for $k$ time steps with cumulative
discounted reward $\mathcal{R}$. Convergence results are provided in \cite{Parr1998}.

An identical algorithmic construct called \termidx{Macro Q-learning} is also often
encountered in the literature. It comes from the work on Macro-actions of
\cite{Mcgovern1997} as an extension of the deterministic macro-operators
\cite{Korf1985} to closed-loop policies. It might have been that the options framework
had not yet beaten the pathways rigorously enough that the authors of
\cite{Mcgovern1997} decided use a different terminology. 

As opposed to the more general SMDP Q-Learning algorithm of equation
\ref{eq:smdp-qlearning}, Macro Q-Learning treats macro-actions (options in this case)
and primitive actions separately: an SDMP update rule is applied for macro-actions
while primitive actions are updated using \ref{eq:qlearning-update}. Clearly, SMDP Q
learning encompasses Macro Q-Learning and can be expressed identically as

\begin{equation}
Q_{t+1}(x, o) \leftarrow Q_{t}(x, o) + \alpha \left[ \mathcal{R} + \gamma^k
\max_{o^\prime \in \mathcal{O}_x^\prime} Q_{t}(x^\prime, o^\prime) -
Q_t(x, o) \right]
\end{equation}

but with the only constraint that the set of options $\mathcal{O}$ must contain
primitives actions wrapped as options.

\textsc{Macro Q-Learning} is thus equivalent to \textsc{SMDP Q-Learning} where the
options set $\mathcal{O}$ is augmented with single-step options for each primitive
action.

\subsection{Intra-Option Learning}

In order for the Q-Learning algorithm in equation \ref{eq:smdp-qlearning} to provide a
good estimate of the optimal option value function, sufficient experience must be
obtained about the option set at every states. While executing an option to completion,
valuable fragments of experience are completely ignored rendering the SMDP methods
highly sample-inefficient.

Intra-option learning is an off-policy algorithm which tries to leverage the valuable
content \textit{within} the options. While an option is executing, it simultaneously
updates the value function estimate of all the consistent options which would have had
chosen the same action under a given state. It is thus required that the options be
defined by deterministic policies so that this idea of consistency can be established. 
By virtue of being an off-policy method, intra-option learning also has the potential of
learning about nonterminating options. 

\begin{defn}{Intra-option value learning}
\begin{align}
U(x, o) &= (1 - \beta(x))Q_t(x, o) + \beta(x) \max_{o^\prime \in \mathcal{O}} Q_t(x, o^\prime) \\
Q_{t+1}(x, o) &= Q_t(x, o) + \alpha \left[ R_{t+1} + \gamma U(x, o) - Q_t(x, o) \right]
\label{eq:intra-option-learning}
\end{align}
The update rule takes place under the event $\varphi(x, o, t)$ after each primitive
transition and is applied over all other consistent options for which $\pi(X_t) = A_t$.
\end{defn}

Intra-Option learning being an off-policy method, it is susceptible to suffer from i
nstabilities and divergence issues in a similar way to the classical Q-Learning algorithm
with function approximation. GQ($\lambda)$ was introduced in \cite{Maei2010} as an
extension of Q-Learning but with special treatment against the aforementioned
problems. While the theory for GQ($\lambda$) is theoretically correct, empirical
evidences remain to be gathered to better understand its behavior. Importance sampling
corrections similar as those used in \cite{Precup2001} for off-policy TD($\lambda$)
could then be used as a substitute. 

Finally, in order to learn an option's policy $\pi$, it is customary to adopt the notion of 
\termidx{subgoal}. Options can be seen as optimizing some intrinsic notion of reward
consistent with the overall task's objective. A \termidx{terminal subgoal value} assigns a
value for reaching the terminal states of an option as determined by $\beta$. A subgoal
value function thus arises from the combination of the original underlying MDP plus the
overlaid discounted subgoal value function. This concept is often referred to as a 
\termidx{pseudo-reward} function: a term originally coined in \cite{Dietterich1999}
about the MAXQ framework.	


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% From Structure to Dynamics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{From Structure to Dynamics}
\section{Random Walk}
Given a graph $G$, we can generate a random sequence by picking a vertex uniformly at random in the neighborhood of an initial vertex and carrying on the procedure over the newly obtained vertex. This procedure corresponds to the realization of a type of random process called \textit{random walk}. More precisely, we define a \textbf{simple random walk} on a graph $G$ as a Markov Chain having $V$ as state-space and for which the transition matrix is given by:
\begin{equation}
P(x, y) = \begin{cases} 
\frac{1}{d(x)} \hspace{2mm} \mbox{if $x \sim y$} \\
0 \hspace{2mm} \mbox{otherwise}
\end{cases}
\end{equation}

\section{Mixing Time}

\section{Spectral Properties}
We will see below how the spectral properties of the Laplace operator relate to random walks. By doing so, we will define the \textit{diffusion map} and see how it yields a distance function that has a probabilistic interpretation.

From the similarity matrix $W$, we can obtain a stochastic matrix $M$ by normalizing it as follow:
\begin{equation}
M = D^{-1}W
\end{equation}

Note that $M$ is similar to the symmetric matrix $M_s$:
\begin{equation}
M_s = D^{1/2} M D^{-1/2}
\end{equation}

Therefore $M$ and $M_s$ share the same eigenvalues. Since $M_s$ is symmetric and diagonalizable, its corresponding set of eigenvectors $\{\mathbf{v}\}$ forms an othonormal basis for $\mathbb{R}^n$. We can write the left and right eigenvectors of $M$ in terms of  $\{\mathbf{v}\}$ as follow:
\begin{equation}
\phi_j = v_j D^{1/2} \mbox{ and } \psi_j = v_j D^{-1/2}
\end{equation}

Since $M$ is a stochastic matrix, we can return to our initial random walk view where each $M_{ij}$ specify the one-step transition probabilities.During the graph construction phase, assume that $\epsilon$, or equivalently the number of nearest neighbours, is large enough so that $G$ is connected. $M$ is then irreducible and aperiodic. The Perron-Frobenius theorem tells us that $\lambda_0 = 1$ with a unique left eigenvector $\phi_0$. Furthermore, $\phi_0$ is the stationary distribution for $M$ and can be written explicitly as:
\begin{equation}
\phi_0(x_i) = \frac{D_{ii}}{\sum_{j}D_{jj}}
\end{equation} 

Taking the spectral decomposition of $M$, we express the probability distribution $p(t, y|x)$ as:
\begin{equation}
\label{eq:prob-eig}
p(t, y|x) = \phi_0(y) + \sum_{i\geq 1} \psi_i(x) \lambda_i^t \phi_i(y) 
\end{equation} 

We could measure the distance between two points from the original state-space by considering the dissimilarity between their respective probability distributions given in equation \ref{eq:prob-eig}.

\begin{align}
\label{eq:prob-dist}
D^2(x_0, x_1) &= || p(t, y|x_0) - p(t, y|x_1) ||^2_w \\
&= \sum_{y} (p(t, y|x_0) - p(t, y|x_1))^2 w(y) \notag
\end{align}

$w(y)$ is a weighting term meant to take into account the local density at $y$. 
If we set $w(y) = 1/\phi_0(y)$ low density points will receive more weight. Furthermore, one could establish with \eqref{eq:prob-eig} and this choice of weighting the equivalence of  \eqref{eq:prob-dist} to the Euclidean distance in diffusion map space. Replacing \eqref{eq:prob-eig} into \eqref{eq:prob-dist}, we get:

\begin{align}
D^2(x_0, x_1) &= \sum_y \left(\sum_{i \geq 1} \psi_i(x_0) \lambda_i^t \phi_i(y) - \psi_i(x_1) \lambda_i^t \phi_i(y) \right)^2 \frac{1}{\phi_0(y)} \notag \\
&= \sum_y \left( \sum_{i \geq 1} (\lambda_i^t(\psi_i(x_0) - \psi_i(x_1))\phi_i(y) \right) ^2 \frac{1}{\phi_i(y)}
\end{align}

\section{Spectral Graph Clustering}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Building Options
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Building Options}
\section{Graph Construction}
%\subsection{Nearest Neighbors Graph}
%\subsection{Relative Neighborhood Graph}
%\subsection{Grabriel Graphs}
%\subsection{Beta-Skeleton}
\section{Graph Clustering}
% Walktrap algorithm
\section{Initiation and Termination}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiments}
% Distributed, parameters of sarsa

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{library}
\bibliographystyle{plain}

\printindex

\end{document}