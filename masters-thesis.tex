% Packages (fold)
\RequirePackage{lmodern}
\documentclass[12pt, oneside, extrafontsizes]{memoir}  % TODO 12pt, twoside

\setstocksize{11in}{8.5in}
\settrimmedsize{11in}{8.5in}{*}
\settrims{0in}{0in}
\setlrmarginsandblock{38mm}{25mm}{*}
\setulmarginsandblock{25mm}{25mm}{*}
\setheadfoot{13pt}{26pt}
\setheaderspaces{*}{13pt}{*}
\checkandfixthelayout

\DoubleSpacing

\setsecnumdepth{subsubsection}
\headstyles{default}
\chapterstyle{ell}
\setsecheadstyle{\scshape\LARGE\raggedright}

% PDF
\usepackage[colorlinks,bookmarksnumbered,bookmarksdepth=subsubsection,unicode=true]{hyperref}
\newsubfloat{figure}  % must follow hyperref
\hypersetup{
pdfauthor = {Pierre-Luc Bacon},
pdftitle = {Automatic Options Discovery from the Community Structures in the State-Transition Graph},
pdfsubject = {Subject},
pdfkeywords = {reinforcement learning, time series analysis},
pdfcreator = {LaTeX with the hyperref package},
pdfproducer = {},
linkcolor = [HTML]{000000},
citecolor = [HTML]{0F82F5},
%urlcolor = [HTML]{\colorc}
}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{xspace}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{standalone}
\usepackage[boxed]{algorithm2e}

% Custom commands
\newcommand{\rvs}{r.v.'s\xspace}
\newcommand{\mdp}{Markov Decision Process\xspace}
\newcommand{\mdps}{Markov Decision Processes\xspace}
\newcommand{\mrp}{Markov Reward Process\xspace}
\newcommand{\mc}{Markov Chain\xspace}

\def\given{\;\middle\vert\;}
\def\optimal{\star}
\def\transpose{\intercal}
\def\laplacian{\mathbf{\mathcal{L}}}
\def\eqdef{\overset{\underset{\mathrm{def}}{}}{=}}
\def\indicator{\mathds{1}}
\DeclareMathOperator{\expectation}{\mathbb{E}}
\DeclareMathOperator{\vol}{\text{vol}}

\newcommand{\todo}[1]{[TODO: #1]}
\newcommand{\termidx}[1]{\index{#1}{\textbf{#1}}}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]

\usepackage{showidx}
\makeindex
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title page
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Title (fold)
\pretitle{\begin{center}\cftchapterfont\huge}
\posttitle{\end{center}}
\preauthor{\begin{center}\huge}
\postauthor{\end{center}}
\predate{\begin{center}\large}
\postdate{\end{center}}

%\title{Automatic Options Discovery from the Community Structures in the State-Transition Graph}
\title{Towards Better Theoretical Underpinnings for the Bottleneck Concept}
\author{Pierre-Luc Bacon}
\date{\today}
\renewcommand\maketitlehookb{
\vfill
}
\renewcommand\maketitlehookc{
\vfill
\begin{center}
{
\large
Computer Science\\
McGill University, Montreal
}
\end{center}
\vspace{10mm}
}
\renewcommand\maketitlehookd{
\vspace{10mm}
A thesis submitted to McGill University in partial fulfilment of the requirements of
the degree of Master of Science.
\copyright Pierre-Luc Bacon; \today.
}
% Title (end)

\begin{titlingpage}
\maketitle
\end{titlingpage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{tocdepth}{2}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}

The idea of \textit{bottleneck}
regions of the interaction graph has been instrumental in the
development of most of these techniques.
The canonical example of bottlenecks can be found in a navigation
task within an environment with multiple-rooms, where subgoals naturally
corresponds to the doorways connecting two rooms (i.e. regions of the state
space). Bottlenecks have been defined as those states which
appear frequently on successful trajectories to a goal but not on
unsuccessful ones~\cite{Mcgovern2001, Stolle2002} or as nodes which
allow for densely connected regions of the interaction graph to
reach other such regions~\cite{Menache2002, Wolfe2005, Kazemitabar2009}.
%To a lesser extent, \cite{Barto2002} could be belong to the latter
%class of approaches by its definition of \textit{access states}
%corresponding to those nodes which allow for reaching difficult
%regions of the interaction graph from other ones.
Bottleneck states are found using graph centrality measures in~\cite{Barto2009, Rad2010} and capture most of the properties outlined
above by defining important nodes as those which lie frequently on
shortest paths between states.

It appears that all of these approaches can also be categorized on
the basis of whether the total reward obtained on a trajectory is
taken into account in the identification of useful subgoals. Solutions that consider only the structural
properties of the interaction graph assume that the state-space
dynamics is the only determining factor in defining options. The other class of approaches treats both dynamics and rewards, accounting for the fact that bottlenecks of
the state-space dynamics do not necessarily correspond to desirable
subgoals to develop for a task.

The idea of sampling trajectories while the agent is interacting
with the environment is central to most approaches. One drawback of this
class of methods is that primitive actions might not suffice to
attain the task goal or yield to successful (defined in a
domain-dependent way) trajectories. Hence, these methods can require large
amounts of data, possibly too expensive to acquire.
%Since the complete state space
%does not have to be represented, they might however be more efficient.
Furthermore, useful solutions might
 lie within sparse subgraphs of the entire interaction
graph (a similar intuition behind manifold embedding techniques of
machine learning).

McGovern \& Barto~\cite{Mcgovern2001} formulate the problem of finding subgoals as
a multiple-instance learning problem over bag of feature vectors
collected by interacting with the environment. Two sets of bags are
obtained in this way, from observations collected
along successful and unsuccessful trajectories respectively. The notion of diverse density is then applied  either by
exhaustive search or gradient descent to find regions of the feature
space with the most positive instances and the least negative ones.
Because negative bags must only contain unsuccessful trajectories,
while positive bags have to contain at least one successful feature
vector, this method is sensitive to noise. 
%The problem of collecting
%successful trajectories would also arise here due if the agent is
%unable to reach satisfactory performances for the task.

In~\cite{Barto2002}, the authors identify subgoals by looking for \textit{access
states} which lead to the regions of the state space that have not
been visited recently and trying to capture the notion
of \textit{relative novelty}
% or what we also qualify as
%\textit{breakthrough} in the solution space. 
%The authors posit that
%the distribution of their measure of relative novelty for subgoals
%differs significantly from the other kind of states. 
The time frame
within which to search for such novelty events is a parameter of
this algorithm and influences the results obtained.

The graph-cut algorithm proposed in~\cite{Menache2002} 
tries to identify access states of those densely connected regions
linked together through a few important edges. Wolfe \& Barto~\cite{Wolfe2005} apply the same graph-cut partitioning but only over local transition
graphs obtained from the sampled trajectories.  
A similar approach of finding strongly connected components
(SCC) of the interaction graph is adopted in~\cite{Kazemitabar2009}.
Since most MDP problems  exhibit ergodicity, the applicability of this algorithm seems limited. Ergodic chains would then  lead to large components being detected by SCC.

By representing the entire interaction graph, Simsek \& Barto~\cite{Barto2009}
compute betweenness centrality to identify important subgoals. They
also propose a potential incremental formulation of their algorithm
that finds local maxima of betweenness within the subgraph obtained
by collecting short trajectories. However, it has also been argued~\cite{Rad2010} that
the centrality measure called \textit{connection graph stability} leads
to better subgoal identification than betweenness or closeness centrality.

The problem of subgoal discovery has only been recently considered under the
formulation of graph clustering. Mathew \& al. ~\cite{Mathew2012} also build an
interaction graph out of which clusters of nodes are found using the PCCA+
~\cite{Weber2004} algorithm. The concept of metastability motivates this approach
and tries to find regions of the state space in which a dynamical system spends more
time. PCCA+ is a spectral clustering technique which comes at a higher cost than the
approach proposed here.

\subsection{Outline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Sequential Decision Making under Uncertainty}


\section{Markov Chains}

A discrete-time stochastic process is a family of random variables $\{ X(t), t \in T\}$
(\rvs) indexed by a time parameter $t \in \mathbb{N} = \{0, 1, 2,...\}$. The
value of $X_n$ is referred to as the \termidx{state} of the process. When the family of
random variables is defined over a discrete sample space
$\Omega$, the stochastic process is said to be \textit{discrete-valued}.
Stochastic processes are useful for modelling probabilistic phenomenon involving in
time such as the evolution of the price of a stock, or the
number of connections to a web service over the day for example. Due to the inherent
complexity many systems, it is often desirable to make simplifying assumptions to
make the exercise tractable. One might then decide to treat the \rvs as
\textit{independent and identically distributed} (iid) but only at the cost of loosing the ability to
capture the correlation structure among the \rvs. The so-called \textit{Markov}
assumption is often used when the iid property is deemed too restrictive, but is yet
simple enough to make computation tractable.

\begin{defn}
A \termidx{Markov Chain} is a discrete-time and discrete-valued stochastic process
which possesses the Markov property. The Markov property implies that for all times $n \geq
0$ and all states $i_0, \dots, i, j \in \Omega$:
\begin{equation}
P(X_{n+1} = j | X_0 = i_0, X_1 = i_1, ..., X_n = i) = P( X_{n+1} = j | X_n = i)
\end{equation}
\end{defn}

The Markov property (or \termidx{memoryless property}) could be stated simply by
saying that the \textit{future is independent of the past, given the present state}.
Therefore, knowing the current value of $X_n = i$ is enough to characterize the
future evolution of the stochastic process $\{X_{n+1}, X_{n+2}, \dots \}$ and the history
$\{X_0, X_1,\dots, X_{n-1} \}$ can be discarded.

A Markov chain is completely specified by the one-step transition probabilities
$p_{ij} = P(X_{n+1} = i | X_n = j)$ contained in its Markov or
\termidx{stochastic matrix}. For a finite state-space $\mathcal{X}$, we have:
\begin{equation}
\mathbf{P} = \begin{bmatrix}
p_{11} & p_{12} & \cdots & p_{1m} \\
p_{21} & p_{22} & \cdots & p_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
p_{m1} & p_{m2} & \cdots & p_{mm}
\end{bmatrix}
\end{equation}

Furthermore, $\mathbf{P} $ must satisfy the following properties:
\begin{equation}
p_{ij} \geq 0 \mbox{ and } \sum_{j=1}^M p_{ij} =1 
\end{equation}

Let $\mathbf{P}^\optimal$ denote the adjoint of $\mathbf{P}$. If $\mathbf{P}^\optimal = \mathbf{P}$ holds, then the stochastic matrix is said to be \termidx{time-reversible}. 

Those elements of $\mathbf{P}$ for which $p_{ii} = 1$ are said to be
\termidx{absorbing} and once entered, the Markov chain the never escapes.

If the elements of $\mathbf{P} $ are independent of the time index, the
Markov chain is said to be \termidx{time-homogeneous} and has \termidx{stationary transition
probabilities}. If this property is satisfied and one knows the transition probabilities of
a Markov Chain, the problem of computing probability distributions is greatly
simplified and can be succinctly expressed using matrix multiplication.  

The \termidx{Chapman-Kolmogorov} equation lets us express the probability of
going from state $i$ to $j$ in $n$ steps by $[\mathbf{P}^n]_{ij} = [
\underbrace{\mathbf{P} \times \mathbf{P} \times \dots \times \mathbf{P}}_{\text{n
times}} ]_{ij}$. The probability of transitioning from state $i$ to any other state under
$n$ steps then becomes:
\begin{equation}
P( \cdot, n | i) = e_i \mathbf{P}^n
\end{equation}
Here $e_i$ stands for the vector with zero components everywhere except of for its
$i$th component set to 1. 

Using the Chapman-Kolmogorov equation, a state $j$ is classified as being
\termidx{accessible} from $i$ if $[\mathbf{P}^n]_{i,j} > 0$ for some $n \geq 0$. When
the relation holds in both direction, $i$ and $j$ are said to \termidx{communicate}. If
every possible pair of states can communicate, the Markov chain is classified as being
\termidx{irreducible}. Given that a state $j$ has been initially encountered once, we
say that it is \termidx{recurrent} if the probability of visiting it again is nonzero. That
is, denote $T_j$ the time at which the chain returns to $j$, the recurrence property
expresses the fact that $P(T_j < \infty | X_0 = j) > 0$. When $E(T_j | X_0) < \infty$,
state $j$ is said to be \termidx{positive recurrent}.

These last definitions are essential for defining the \termidx{ergodicity}
property which is a condition often assumed in the analysis of Markov Decision Processes which will be covered below. 

\begin{defn}
A Markov chain is ergodic if it is irreducible and positive recurrent.
\end{defn}

It occurs often that one is interested in knowing how a given probability distribution
for $X_n$ would evolve in time.  Denote the
distribution of $X_n$ by the row vector $\mu_n$ and the \termidx{initial distribution}
by $\mu_0$. It can seen that the following relation hold
\begin{equation}
 \mathbf{\mu}_n= \mathbf{\mu}_0 \mathbf{P}^n \label{eq:initial-dist}
\end{equation}
by noting that the Markov property implies that
\begin{align}
P(X_0 &= i_0, X_1 = i_1, X_2 = i_2, \dots, X_n = i_n) = \notag \\
&P(X_0 = i_0) P(X_1 = i_1 | X_0 = i_0) P(X_2 = i_2 | X_1 = i_1) \dots P(X_n = i_n |
X_{n-1} = i_{n-1})
\end{align}
and that our Markov chain is time-homogeneous ($\mathbf{P}$ is fixed).

If it happens that the distribution of $\mu_n$ does not change with $n$, then 
 the Markov chain is said to be \termidx{stationary}. 

\begin{defn}
A Markov chain said to be stationary if it satisfies $P(X_0 = i_0, X_1 = i_1, \dots, X_n
= i_n) = P(X_m = i_0, X_{m+1}, \dots, X_{m+n})$ for any $m \in \mathbb{Z_+}$.
\end{defn}

Using \ref{eq:initial-dist}, the stationarity problem for a Markov Chain amounts to
finding a vector $\pi$ such that
\begin{equation}
\mathbf{\pi}^\transpose P = \mathbf{\pi}^\transpose
\end{equation}

The stationary distribution $\mathbf{\pi}^\intercal$ can thus be seen as the left
eigenvector of $\mathbf{P}$ associated with the eigenvalue 1. Furthermore, it must be
that $\sum_k \mathbf{\pi}_k = 1$ in order for $\pi$ to be a well-defined probability
distribution. When the Markov chain reaches the stationary distribution, it is said to be
in its \termidx{steady-state} mode.

An important characterization of the properties of stochastic matrices comes from the
theory of nonnegative matrices in the form of the Perron-Frobenius theorem. First, the spectral radius of a square matrix is defined as 

\begin{equation}
\rho(A) \overset{\underset{\mathrm{def}}{}}{=} \max_i(|\lambda_i|)
\end{equation}

The theorem in its original form about irreducible matrices \cite{Horn1986} then amounts to the following.

\begin{thm}[Perron-Frobenius Theorem]

Let $A$ be an irreducible and nonnegative matrix, then the following claims hold
\begin{enumerate}
\item The spectral radius $\rho(\mathbf{A})  > 0$
\item $\rho(\mathbf{A})$ is an eigenvalue of $\mathbf{A}$
\item There exists a positive vector $\mathbf{x}$ such that $\mathbf{A}\mathbf{x} = \rho(\mathbf{A})\mathbf{x}$ and $\rho(\mathbf{A})$ is a simple eigenvalue of $\mathbf{A}$.
\item The unique eigenvector whose components sum to 1 is called \termidx{Perron
 vector}, that is
\begin{equation}
\mathbf{A} \mathbf{p} = \rho(\mathbf{A}) \mathbf{p}, \hspace{2.5mm} \mathbf{p} > 0 \hspace{2.5mm} \mbox{and} \hspace{2.5mm} \|\mathbf{p}\|_1 = 1
\end{equation}
\end{enumerate}
\end{thm}

The theorem can also be conveniently adapted to Markov chains under the following
lemma proven in \cite{Montenegro2006}.
\begin{lem}
Let $\mathbf{P}$ be the stochastic matrix associated with a irreducible and reversible 
Markov chain over a state space $\mathcal{X}$ of size $n$. $\mathbf{P}$ must then
have a complete spectrum of real eigenvalues of magnitude at most 1 and of the form
\begin{equation}
1 = \lambda_0 \geq \lambda_1 \lambda \geq \dots \geq \lambda_{n-1} \geq -1
\end{equation}
\end{lem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Markov Decision Processes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Markov Decision Processes}
Markov chains of the kind presented in the last section do not consider the influence of an external \textit{input} on its dynamics. The case of controlled Markov chains under the framework of \mdps is studied in this section. The theory of \mdps will lay the theoretical foundations for the problem of optimal sequential decision making for situated agents considered in Reinforcement Learning. The following presentation adopts most of the notion from \cite{Csaba2010}. 

\begin{defn}
\label{defn:mdp}
A finite-action discounted \termidx{Markov Decision Process} (MDP) is a tuple
$\mathcal{M} = \langle \mathcal{X}, \mathcal{A}, r, \mathcal{P},\gamma \rangle$ where
$\mathcal{X}$ is a non-empty countable set of states, $\mathcal{A}$ is a finite state
of actions, $\mathcal{P}$ is a transition probability kernel giving rise to a distribution
$P(\cdot |x, a)$ over $\mathcal{X}$ given any $x \in \mathcal{X}$ and $a \in
\mathcal{A}$,  $r$ is the reward function $r: \mathcal{X} \times \mathcal{A} \mapsto
\mathbb{N}$ and $\gamma \in [0, 1]$ is a discount factor.
\end{defn}

A \termidx{deterministic policy} $\pi$ is a mapping $\pi : \mathcal{X} \mapsto
\mathcal{A}$. In the theory of MDPs, stochastic policies of the form $\pi : \mathcal{X}
\mapsto \Omega(\mathcal{A})$ are also object of study, in which case actions are
drawn from a conditional probability distribution  over states according to $A_t \sim
\pi(\cdot | X_t)$ (here $A_t$ and $X_t$ denote \rvs at time $t$). 

Fixing a policy for an MDP has for effect to induce a \termidx{Markov Reward Process}
(MRP) $\langle \mathcal{X}, \mathcal{R}^\pi, \mathcal{P}^\pi, \gamma \rangle$ with
the reward function $\mathcal{R}^\pi(x) = r(x, \pi(x))$ and state transition probability
kernel $P(\cdot | x) = P(\cdot | x, \pi(x))$. An MRP can be thought as a Markov chain
augmented with rewards over the set of states. The concepts of induced Markov
chains and Markov Reward Processes are particularly important in the analysis of the
dynamics of an MDP. At the level of the induced Markov chain of an MDP, the
existence of a stationary distribution is not necessarily guaranteed.

\subsection{Value Function}
The notion of a value function is instrumental in supporting the principle of optimality
that underlies the decision theoretic framework of MDPs. It is assumed in this context
that a decision maker, or \textit{agent}, is acting in such a way as to optimize some
intrinsic notion of what \textit{good} behaviour is. From to the definition
\ref{defn:mdp} of an MDP, the reward function $r$ is used to express a quantity
known as the \termidx{return}

\begin{defn}
The return of an \mdp is the total discounted sum of the rewards originating from
the induced \mrp.
\begin{equation}
\mathcal{R} = \sum_{t=0}^\infty \gamma^t R_{t+1} \label{eq:return}
\end{equation}
\end{defn}

Fixing the value of $\gamma < 1$ leads to a notion of return where the immediate
reward is worth exponentially more than the far future. In this case, the resulting MDP 
is then called a \termidx{discounted \mdp}. On the other hand, setting $\gamma =
1$ makes the immediate reward observed at each step equally important and one 
then talks of an \termidx{undiscounted \mdp}

Knowing the \textit{desirability}, or value, of a state at any moment will greatly
simplify the problem of deriving an optimal behavior. 

\begin{defn}
The \termidx{value function} $V^\pi: \mathcal{X} \mapsto \mathbb{R}$ of a
stationary policy $\pi$ is the conditional expectation of the return (discounted or
undiscounted) given a state.
\begin{equation} 
V^\pi(x) = \expectation \left[ \sum_{t=0}^\infty \gamma^t R_{t+1}  \given  X_0 =
x\right] , x \in \mathcal{X} \label{eq:value-function}
\end{equation}
\end{defn}

An optimal behavior is achieved if the value function of its policy is optimal. An
optimal value function $V^*: \mathcal{X} \mapsto \mathbb{R}$ is one which obtains
the maximum possible expected return for every state.

% Action value function 
The \termidx{action-value function} of a policy $\pi$ is tightly related to the notion of
value function presented above. Instead of being defined only over states, it specifies
the expected return of initially being in state $x$ choosing a first action $a$ and
following $\pi$. 
\begin{defn}
The action-value function $Q^\pi: \mathcal{X} \times \mathcal{A} \mapsto
\mathbb{R}$ of a stationary policy $\pi$ is the conditional expectation of the return
given an initial state and action pair. 
\begin{equation}
Q^\pi(x, a) = \expectation \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \vert X_0 = x,
A_0 = a \right], \; x \in \mathcal{X}, a \in \mathcal{A}
\end{equation}
\end{defn}

If the action-value function yields the maximum expected return for every state
action pair, it is said to be optimal and denote it by $Q^*$. One can go from the
action-value function to the value-function by noting that for finite-action MDP
(definition \ref{defn:mdp})
\begin{equation}
V^*(x) = \max_{a \in \mathcal{A}} Q^*(x, a) \label{eq:qopt-to-vopt}
\end{equation}

In the more general case of a countable non-empty sets of actions, the $\max$
operator should be replaced with the supremum ($\sup$) of $\mathcal{A}$. The
optimal action-value function can also be recovered from the value-function as
follow:
\begin{equation}
Q^*(x,a) = r(x,a) + \gamma \sum_{y \in \mathcal{X}} P(y, a, x) V^*(y), \; x \in
\mathcal{X}, a \in \mathcal{A} \label{eq:vopt-to-qopt}
\end{equation}

\subsection{Bellman Equations}
Similar to equation \ref{eq:vopt-to-qopt}, given an MDP, the  so-called 
\termidx{Bellman equations} are written as

\begin{equation}
V^\pi(x) = r(x, \pi(x)) + \gamma \sum_{y \in \mathcal{X}} P(y, \pi(x), x) V^\pi(y)
\label{eq:bellman-equations}
\end{equation}

The recursive formulation of the Bellman equations allows to relate the value of state
to that of its possible successor states, weighted by their probability of occurrence in
an average.  For a finite d-dimensional state space $\mathcal{X}$, $V^\pi$ and
$r^\pi$ can be thought as a vectors in $\mathbb{R}^d$ with $\mathcal{P}$ being a
transition matrix $\mathbf{P}: \mathbb{R}^{d \times d}$. It can be seen that the
Bellman equations (\ref{eq:bellman-equations}) define a linear system of equations in
$d$ unknows whose unique solution is $V^\pi$

\begin{equation}
\mathbf{V}^\pi = \mathbf{r}^\pi + \gamma \mathbf{P}^\pi \mathbf{V}^\pi
\end{equation}

Solving for the left hand side, 

\begin{equation}
\mathbf{V}^\pi = (\mathbf{I} - \gamma \mathbf{P}^\pi)^{-1} \mathbf{r}^\pi
\label{eq:direct-value}
\end{equation}

Under the framework of reinforcement learning adopted in this work,
$\mathbf{P}$ and $\mathbf{r}^\pi$ are not available \textit{a priori}, making the
direct solution of \ref{eq:direct-value} impossible to compute. Furthermore, since
matrix inversion is generally of the order of $\mathcal{O}(n^3)$, the computational
cost will quickly become impractical for large state spaces. Iterative methods are thus
preferred under almost any practical scenario. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bellman Operator
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bellman Operator}
The notion of \termidx{Bellman operator} subtends the theoretical justifications of the
indirect (as opposed to \ref{eq:direct-value}) methods for computing the value
function of policies. 

\begin{defn}
The Bellman operator underlying the policy $\pi$ of an MDP, $T^\pi:
\mathbb{R}^\mathcal{X} \mapsto \mathbb{R}^\mathcal{X}$, is defined as

\begin{equation}
(T^\pi V) = r(x, \pi(x)) + \gamma \sum_{y \in \mathcal{X}} P(y, \pi(x), x) V(y), \; x
\mathcal{X} \label{eq:bellman-operator}
\end{equation}
\end{defn}

Similarly, one can define the Bellman operator $T^\pi : \mathbb{R}^{\mathcal{X}
\times \mathcal{A}} \mapsto \mathbb{R}^{\mathcal{X} \times \mathcal{A}}$  for the
action-value function induced by a policy $\pi$
\begin{equation}
T^\pi Q(x, a) = r(x, a) + \gamma \sum_{y \in \mathcal{X}} P(y, a, x) V(y), \; x \in
\mathcal{X} \label{eq:bellman-operator-action-value}
\end{equation}

At first blush, it can be easy to miss the difference between the overall form of the
Bellman equations (\ref{eq:bellman-equations}) and that of equation \ref{eq:bellman
operator}. One must in fact observe that, even though $T^\pi$ is defined for a given
policy, the $V$ term is general and might not need to correspond to $V^\pi$. 
The Bellman operator happens to be a special type of function called a
\termidx{contraction mapping}. This characterization allows the \termidx{Banach
fixed-point theorem} to be used to show convergence properties.  Starting with an
estimate of the value function of a policy, the iterative application of the Bellman
operator will converge in the limit to a unique fixed point corresponding to $V^\pi$.

The same principle will underlie the value and policy iteration algorithms for
computing the optimal value function. Before delving into the details of these
algorithms, the \termidx{Bellman optimality operator} must be introduced. Just as
$T^\pi$ (\ref{eq:bellman-operator}), $T^\optimal$ is a maximum-norm contraction
mapping $T^\optimal: \mathbb{R}^\mathcal{X} \times \mathbb{R}^\mathcal{X}$ but 
is defined this time as
\begin{equation}
(T^\optimal V) = \max_{a \in \mathcal{A}} \left\lbrace r(x, a) + \gamma \sum_{y
\mathcal{X}} P(y, \pi(x), x) V^\optimal (y) \right\rbrace, \; x \in \mathcal{X} 
\label{eq:bellman-optimality-operator}
\end{equation}

The optimal value function $V^\optimal$ satisfies the fixed-point equation
$T^\optimal V^\optimal = V^\optimal$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Solving MDPs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Solving MDPs}

The Banach fixed-point theorem introduced above laid out the foundations for the
two main approaches for solving MDPs: the value and policy iteration algorithms. By
\textit{solving and MDP}, one generally refers to finding the optimal value function
underlying an MDP. If the the value function found in that way is indeed optimal, an
optimal policy can be derived by greedily picking the best action. These two main
problems -- finding the value of a policy or finding an optimal policy -- are usually
distinguished under the names of \termidx{prediction} (figure \ref{fig:prediction-problem}) and \termidx{control} (figure \ref{fig:control-problem}). In the control
problem, one tries to derive an optimal policy by considering the greedy policy with
respect to the optimal value function (see \cite{Ross1983} for a proof).

\begin{equation}
\pi^\optimal(x) = \arg \max_{a \in \mathcal{A}} \left[ r(x,a) + \gamma \sum_{y
\mathcal{X}} P(y, a, x) V^*(y)\right], \; x \in
\mathcal{X}, a \in \mathcal{A} \label{eq:greedy-optimal-policy}
\end{equation}

\begin{figure}
\centering
\input{fig/spaces}
\caption{The policy evaluation problem consists in finding the value-function
corresponding to a given policy $\pi$. The space of 
stationary policies $\Pi_{stat}$ is only considered in this work.}
\label{fig:prediction-problem}
\end{figure}

\begin{figure}
\centering
\input{fig/control}
\caption{The control problem aims at finding an optimal policy i.e. one for which the
corresponding value function is optimal. There might be multiple optimal policies, but
the optimal value function must be unique\cite{Ross1983}.}
\label{fig:control-problem}
\end{figure}

Finding $V^*$ as a first step would thus allow the control problem to be solved using
equation \ref{eq:greedy-optimal-policy}. The \termidx{value-iteration} algorithm is
one way in which $V^\optimal$ can be obtained. Let $V_0$ be some arbitrary initial
bounded function, this method consists in applying Bellman optimality operator
$T^\optimal$ successively in the following manner
\begin{equation}
V_{k+1} = T^\optimal V_k \label{eq:value-iteration}
\end{equation}

$V_k$ can be shown to uniformly converge to $V^*$ as $k \to \infty$ \cite{Ross1983}.

With \termidx{policy iteration}, two steps are interleaved: policy evaluation and
policy improvement. The general idea goes as follow: from an initial policy $\pi_0$
compute its corresponding value function (policy evaluation) and derive the greedy
policy $\pi_{k+1}$ from it (policy improvement), then repeat these two steps as
necessary. Fixing the number of iteration to $k$, the policy computed by policy
iteration at step $k$ can be shown not to be worse than the greedy policy computed
by value-iteration. Because of the policy evaluation step, policy-iteration is
computationally more expensive. It plays however an important role in the Actor
Critic architectures \cite{Sutton1984}.

The policy and value iterations algorithms belong to the class of algorithms known as
\termidx{dynamic programming} (DP) methods. Assuming a perfect knowledge of the
transitions dynamics and reward function, they can greatly reduce the computational
burden compared to a direct policy search approach which would be of the order of
$\left\vert \mathcal{X} \right\vert^{\left\vert \mathcal{A} \right\vert}$. For the class of problems considered in this work, these assumptions would no longer be valid and
additional techniques will have to be used. DP is still of great importance for the
theoretical underpinning that it provides to the understanding of reinforcement
learning approaches.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Reinforcement Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reinforcement Learning}

Reinforcement Learning (RL) considers the problem setting of a situated
\termidx{agent} trying to learn to optimize a loss (return) from the direct experience
provided by the \termidx{environment} (figure \ref{fig:rl}). Historically, the field has
been influenced greatly by the work on trial-and-error learning from the field of
psychology \cite{Sutton1998}, consequently shaping the type of learning problems it tries to solve. RL hinges heavily upon the theory of \mdps introduced in the previous section while sharing the same goal as DP of solving the control problem. 

\begin{figure}
  \centering
  \input{fig/rl}
  \caption{Reinforcement Learning. An agent executes an action $a_t$ in the
  environment at time $t$ producing a state transition and instantaneous reward as
  determined by the environment dynamics.}
  \label{fig:rl}
\end{figure}

The assumptions an \termidx{environment model} being known is however usually
lifted and makes RL mainly focused on \termidx{model-free} learning. The presence
of a model can still be accommodated in this framework and will subtend a set of RL
methods for \termidx{planning}: determining the best course of action for
accomplishing a goal by simulating the consequences of actions in the model.
Additionally, RL is concerned with the problem of acquiring relevant experience
from the environment in an \termidx{online} fashion: a problem of joint
\termidx{exploration} and control. The way in which the acquired experience relates
to the target policy will differ depending on the learning algorithms which will either
be classified as \termidx{on-policy} or \termidx{off-policy}. In the following, the
Monte-Carlo (MC) method will be presented as a way to learn about the value of
states in the absence of a model. Then the Temporal Difference (TD) learning
algorithm will be introduced and shown to encompass the MC methods. Finally, the
off-policy Q-Learning algorithm will be presented. 

\subsection{Monte-Carlo}
The Monte-Carlo approach to solving the control problem tries to estimate the value
of a state by online sampling of the return (equation \ref{eq:return}) directly from the
environment or through simulated experience using a model. Working under the
assumption of an episodic task, independent samples of return are averaged to
obtain an estimate of the true expectation. According to the law of large number, as
the number of samples goes to infinity, the average becomes an unbiased
estimator. 

The procedure described above is however not sufficient for solving the control
problem and only answers the prediction one. In the absence of a model, it will be
necessary to estimate the value of an action-value function. Since certain state-
action pair might be difficult to sample frequently enough in large state spaces, it
is often assumed that the agent can be reset in some arbitrary state-action
configuration (an \termidx{exploring start}). In practice, this assumption turns out to
be prohibitively restrictive and \termidx{soft-policies} must be introduced to overcome
this limitation. 

The Monte-Carlo method for control is based on the general framework of policy
iteration where the current policy is improved greedily with respect to some
estimate of its value function. Since the only requirement is to update the policy
toward the greedy policy, it is possible to only consider a slightly perturbed instance
of the greedy policy. This restriction being dispensed, soft-policies where $\pi(x, a)
> 0$ can be used instead. A class of soft policies commonly used is the
\termidx{$\epsilon$-greedy} one, where a random action is chosen with probability
$\epsilon$, and the greedy action for $1 - \epsilon$. Since non-greedy actions can
now be explored, the need for exploring starts is eliminated \cite{SuttonBarto1998}.

The approach described so far belongs to the class of on-policy learning algorithms
where the value of a policy is simultaneously being evaluated and used for control.
If the policy used to obtain samples, the \termidx{behavior policy}, is different from
the one being evaluated and improved upon, the \termidx{estimation policy},
\termidx{importance sampling} must be introduced to compensate for the
discrepancy in the action selection distributions. The resulting algorithm is said to
be capable of doing off-policy learning.

\subsection{Temporal Difference Learning}
The main drawback in using Monte-Carlo methods in an on-line setting has to do with
the need to wait for the complete execution of an episode in order to update the value
function estimate. The Temporal Difference (TD) learning algorithm was instrumental in making the RL approach practical. TD learning first showed how to use immediate prediction as target, a technique known as
\termidx{bootstrapping}, instead of waiting for a full backup to be performed as with
MC methods. The TD($\lambda$) is a extension of the original algorithm
\cite{Sutton1984} that unifies DP and Monte-Carlo methods.

Recalling the definition of the value function given in \ref{eq:value-function}, for all $x \in \mathcal{X}$
\begin{align}
V^\pi(x) &= \expectation \left[ \sum_{t=0}^\infty \gamma^t R_{t+1}  \given  X_0 =
x\right] \notag \\
&= \expectation \left[ R_{t+1} + \gamma \sum_{k=0}^\infty \gamma^k
R_{t+k+2}\given X_t = x\right]  \notag \\
&= \expectation \left[ R_{t+1} + \gamma V(X_{t+1}) \given X_t = x\right]
\label{eq:value-function-recursive}
\end{align}

The TD(0) algorithm incrementally updates the value function estimate using samples
of the form $R_{t+1} + \gamma V(X_{t+1})$ exposed in equation 
\ref{eq:value-function-recursive}
\begin{equation}
\hat{V}_{t+1}(x) = \hat{V}_{t}(x) + \alpha_t \left[ R_{t+1} + \gamma \hat{V}_t (X_{t+1})
- \hat{V}_t(X_t)\right] \label{eq:tdupdate}
\end{equation}

This way of updating the value function estimate is known as a Stochastic
Approximation (SA) method and can be shown to converge \cite{Csaba2010} to the
true $V^\pi$ by treating the sequence $\hat{V}_t$ as a linear ordinary differential
equation (ODE). Algorithm \ref{alg:td0} shows how the TD update is performed while
interacting with the environment.

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized vector $V$, a policy $\pi$ to be evaluated}
\KwResult{The value of $\pi$}
$x \leftarrow$ initial state\;
\While{x is not terminal} {
$a \leftarrow \pi(x)$\;
$r, x^\prime \leftarrow Step(a)$\;
$V(x) \leftarrow V(x) + \alpha\left[ r + \gamma V(x^\prime) - V(x) \right]$ \;
$x \leftarrow x^\prime$
}
\caption{Tabular TD(0) algorithm for policy evaluation. The $Step$ function performs
the state transition in the environment and returns the immediate reward.}
\label{alg:td0}
\end{algorithm}

The sequence of step sizes $\alpha$ is subject to the Robbins-Monro (RM) conditions
according to which
\begin{equation}
\sum_{t=0}^\infty \alpha_t = \infty, \hspace{5mm} 
\sum_{t=0}^\infty \alpha_t^2 < + \infty
\end{equation}

\subsection{Eligibility Traces}

An important extension to the original TD algorithm is the TD($\lambda$) family
\cite{Sutton1984}, unifying TD(0) at one end and Monte-Carlo prediction at the other.
\termidx{Eligibility traces} act as kind of memory which has for effect to weight the
propagated backups at a given state. Instead of updating the value function based on 
a single n-steps estimate, TD($\lambda$) computes and an average, known as the
$\lambda$-return, over a range of multi-step predictions of the return. The multi-step discounted return is first defined as
\begin{equation}
\mathcal{R}_{t:k} = \sum_{s=t}^{t+k} \gamma^{s-t}R_{s+1} +
\gamma^{k+1}\hat{V}_t(X_{t+k+1})
\end{equation}

The $\lambda$-return is a mixture of multi-step return with weight $(1 -
\lambda)\lambda^k$ on each term
\begin{equation}
\mathcal{R}_t^\lambda = (1 - \lambda)\sum_{k \geq 0} \mathcal{R}_{t:k}
\label{eq:lambda-return}
\end{equation}

As $\lambda$ goes to 0, equation \ref{eq:lambda-return}  simplifies to $R_{t+1} +
\gamma \hat{V}_t (X_{t+1})$ and amounts to a one-step TD backup of the TD(0)
algorithm. On the other end of the spectrum, as $\lambda$ goes to 1, we get a sample
of the return from time $t$ until the end of the episode as in the MC method.

TD($\lambda$) is implemented using an additional vector of size $\left\vert
\mathcal{X} \right\vert$ where each component gets incremented by 1 each time the
corresponding state is visited. A decay of $\lambda \gamma$ is also applied upon
each component at each time step. 

\subsection{Sarsa}
The TD(0) algorithm only solves the policy evaluation problem and the question of
control must now be answered. Following the general policy iteration paradigm and
the MC algorithm for control, SARSA is an on-policy control algorithm making use 
TD(0) for policy evaluation under a soft policy exploration strategy. Similar to equation
\ref{eq:tdupdate}, the action-value function is updated as follow
\begin{equation}
\hat{Q}_{t+1}(x, a) = \hat{Q}_{t}(x, a) + \alpha \left[ R_{t+1} + \gamma \hat{Q}_t
(X_{t+1}, A_{t+1}) - \hat{Q}_t(X_t, A_t)\right] \mathbb{I}_{X_t = x, A_t = a}
\label{eq:sarsaupdate}
\end{equation}

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized matrix $Q$}
\KwResult{An optimal action-value function for control}
\ForEach{episode}{
$x \leftarrow$ initial state\;
$a \leftarrow Greedy(x)$\;
\While{x is not terminal} {
$r, x^\prime \leftarrow Step(a)$\;
$a^\prime \leftarrow Greedy(x^\prime)$ \;
$Q(x, a) \leftarrow Q(x, a) + \alpha\left[ r + \gamma Q(x^\prime, a^\prime) - Q(x, a)
\right]$ \;
$x \leftarrow x^\prime$ \;
$a \leftarrow a^\prime$
}
}
\caption{The on-policy Sarsa algorithm based on a TD(0) policy evaluation scheme.
The $Greedy$ function is the soft greedy policy derived from the current estimate of
the action-value function. An $\epsilon$ greedy exploration strategy would be
commonly used.}
\label{alg:sarsa}
\end{algorithm}

As the number of samples for each state-action pair goes to infinity, $\lim_{t \to
\infty} \epsilon = 0$, and under the RM conditions, Sarsa is guaranteed to converge to
an optimal policy \cite{Sutton1998}. 

\subsection{Q-Learning}

Since TD learning is an on-policy learning method, the choice of exploration strategy
will directly impact the convergence to the estimation policy. Q-Learning
\cite{Watkins1989} decouples the exploration and evaluation problems and allows for
any behavior policy to be followed while still converging to $Q^\optimal$. In this case,
the update to the action-value function estimate consists in
\begin{equation}
Q_{t+1} (x,a) = Q_t(x,a) + \alpha_t \left[ R_{t+1} + \gamma \max_{a^\prime \in
\mathcal{A}} Q_t(X_{t+1}, a^\prime) - Q_t(X_t, A_t) \right] \mathbb{I}_{X_t = x, A_t = a} 
\label{eq:qlearning-update}
\end{equation}

The corresponding procedural form is given in algorithm \ref{alg:qlearning}.

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized matrix $Q$}
\KwResult{An optimal action-value function for control}
\ForEach{episode}{
$x \leftarrow$ initial state\;
\While{x is not terminal} {
$a \leftarrow Greedy(x)$\;
$r, x^\prime \leftarrow Step(a)$\;
$Q(x, a) \leftarrow Q(x, a) + \alpha \left[ r + \gamma \max_{a^\prime \mathcal{A}}
Q(x^\prime, a^\prime) - Q(x, a) \right]$ \;
$x \leftarrow x^\prime$
}
}
\caption{The Q-Learning algorithm under some arbitrary exploration scheme.
$\epsilon$-greedy could once again be used for that task.}
\label{alg:qlearning}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% State Abstraction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{State-Abstraction}
The algorithm presented so far assumed \textit{tabular} updates rules where value or
action-value functions are expressed as vectors and matrices respectively. For many
problems, the set of states might have infinite cardinality or be simply too large to fit in
memory. Furthermore, as the number of entries increases, the computational expense
 also increases accordingly. It matters then to seek for a representation of the
value function capable of generalizing across (possibly unseen) states or state-action
pairs.

By expressing the value function as a parametrized function, the incremental updates
would now be performed upon the entries of some parameter vector $\theta$. For
large state spaces, the number of components in $\theta$ would be much smaller than
the number of possible states -- rendering the problem more tractable. Value function
approximation could then be seen as an instance of a supervised learning problem
with a training set consisting of pairs of state and samples of the return (one-step TD,
or full Monte-Carlo).

Any existing method from supervised learning could potentially be used for that tasks:
neural networks, k-nearest neighbors for regression for example. TD-Gammon
\cite{Tesauro1995} is deemed as one of the great success story in the field and made 
use of neural networks for value function approximation. In this work, the attention
will be mainly drawn upon the so-called \textit{linear methods}. A simple, but yet
expressive, family of function approximators is of the form

\begin{equation}
V_{\theta}(x) = \theta^\transpose \varphi(x)
\label{eq:linear-methods}
\end{equation}

The so-called linear methods seek a representation of the value function as a linear
combination of \termidx{features}. In equation \ref{eq:linear-methods}, $\theta
\mathbb{R}^d$ and $\varphi : \mathcal{X} \to \mathbb{R}^d$ with the components
being defined by a set of \termidx{basis functions} $\varphi_i : \mathcal{X} \to
\mathbb{R}$. The way in which $\varphi$ is obtained specifies a feature extraction
method which needs not be linear. 

Value function approximation most commonly relies on \termidx{gradient-descent}
methods to derive the proper update that must be applied on the parameter vector at
each step. Because of the simple form of equation \ref{eq:linear-methods}, taking the
gradient of the value function with respect to $\theta$ yields

\begin{equation}
\nabla_{\theta} V_\theta(x) = \varphi(x)
\label{eq:linear-gradient}
\end{equation}

Using the gradient obtained in equation \ref{eq:linear-gradient}, gradient-descents
methods update the components of $\theta$ by small increments in the direction of
the steepest error reduction. A typical error function which is minimized by supervised
learning techniques is the mean-squared error (MSE). Casting the function
approximation problem under a setting where the true (yet unknown) underlying value
function $V^\pi$ is a target, and $V_\theta$ is the best approximation using the
parametrized form of equation \ref{eq:linear-methods}, the MSE is 

\begin{equation}
MSE(\theta) = \sum_{ x \in \mathcal{X}} P(x) \left[ V^\pi(x) - V_\theta(x) \right]
\end{equation}

The $P(x)$ term here accounts for probability of observing a given state $x
\mathcal{X}$ as an input.

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized vector $\theta$, a policy $\pi$ to be evaluated}
\KwResult{The value of $\pi$}
$x \leftarrow$ initial state\;
\While{x is not terminal} {
$a \leftarrow \pi(x)$\;
$r, x^\prime \leftarrow Step(a)$\;
$\delta \leftarrow r + \gamma \theta^\transpose \varphi[x^\prime] -
\theta^\transpose \varphi[x]$\;
$\mathbf{z} \leftarrow \varphi[x] + \gamma \lambda \mathbf{z}$ \;
$\theta \leftarrow \theta + \alpha \delta z$\;
$x \leftarrow x^\prime$
}
\caption{TD($\lambda$) with linear function approximation}
\label{alg:linear-tdlambda}
\end{algorithm}

TD($\lambda$) with linear function approximation (algorithm \ref{alg:linear-tdlambda}) can be shown to converge under the usual RM conditions on the $\alpha$
parameter \cite{Tsitsiklis1997}.

\subsection{Basis functions}

The choice of proper function space and feature extraction techniques still remains a
challenging problem which can have a great impact on the quality of the value function
being approximated. If one has prior knowledge on the shape, or \textit{regularities}
of the optimal value function under the problem at hand, relevant features could be
specified explicitly. However, when facing the problem of solving a broad class of
problems for which little is known about their structure, general set of basis functions
would be sought. Two feature extraction schemes are presented below for their wide
applicability and ease of use. 

\subsection{Radial Basis Functions}

Given a state $x \in \mathcal{X}$, each component of the $\varphi$ is set to the
distance of its corresponding basis function for which $\varphi_i(x_i) = \varphi_i(\|x -
c_i\|)$ by the radial property and where $c_i$ specifies the \textit{center}. Gaussian
functions are most commonly used as radial basis functions (RBF) and are defined as

\begin{equation}
\varphi_i(x) = \exp\left( \frac{\| x - c_i\|}{2\sigma_i^2}\right)
\label{eq:rbf}
\end{equation}

The $\sigma_i$ parameter specifies the \textit{width} of the Gaussian and must be
tuned by hand or using some through some model selection technique along with the
centers and number of basis functions. Decreasing the width and increasing the
number of basis functions would result in finer expressibility but also incur a higher
computational cost. 

The choice of norm in equation \ref{eq:rbf} is not restricted to the Euclidean distance
and other metric could be used. In \cite{Sugiyama2008} for example, the Geodesic
distance taken over the graph induced by some MDP is used in an attempt to better
capture the intrinsic geometry of the manifold.
 
\subsection{Fourier Basis Functions}

The use of fourier basis functions for value function approximation was introduced in
\cite{Konidaris2011b} but relies on the well established theory or the Fourier analysis.

The nth order Fourier expansion of some univariate periodic function $f$ with period
$T$ is given by
\begin{equation}
\hat{f}(x) = \frac{a_0}{2} \sum_{k=1}^n \left[ a_k \cos\left(k\frac{2\pi}{T} x\right) +
b_k \sin \left(k \frac{2\pi}{T}x \right) \right]
\label{eq:fourier}
\end{equation}

with Fourier coefficients $a_k$ and $b_k$ defined as
\begin{equation}
a_k = \frac{2}{T} \int_0^T f(x) \cos \frac{2\pi kx}{T}dx, \; \mbox{and} \; b_k = \frac{2}
{T} \int_0^T f(x) \sin \frac{2\pi kx}{T}dx
\end{equation}

Since $V^\optimal$ is unknown, the Fourier coefficients cannot be obtained directly
and must be thus treated as \textit{weights} in a spirit similar to the other linear
methods. The Fourier expansion of $f$ results in $2n+1$ terms but
\cite{Konidaris2011b} shows how it can be simplified to only $n+1$ terms if some
assumptions are made on the periodicity of the value function. A function $f$ is even if
$f(x) = f(-x)$, in which case the $\sin$ terms of equation \ref{eq:fourier} can be
droped. A similar observation can be made if $f$ is odd $-f(x) = f(-x)$ and the
$\cos$ terms can be omitted. 


Setting the period to $T=2$, the \textit{nth order Fourier basis} is thus defined as:
\begin{equation}
\phi_i(s) = \cos(i\pi x) \hspace{2mm} \forall i = 0, ..., n
\end{equation}

The multivariate Fourier expansion of some function $F$ with period T up to order $n$
bears a similar form
\begin{equation}
\hat{F}(\mathbf{x}) = \sum_{\mathbf{c}} \left[ a_c \cos \left( \frac{2\pi}{T} \mathbf{c}
\cdot \mathbf{x} \right) +  b_c \sin \left( \frac{2\pi}{T} \mathbf{c} \cdot \mathbf{x}
\right)\right]
\label{eq:multi-fourier}
\end{equation}

The $\mathbf{c}$ term in equation \ref{eq:multi-fourier} is the Cartesian power 
$\mathcal{X}^d$ of the d-dimensional state space such that $\mathcal{c} = [c_1,
\dots, c_i, \dots, c_d]$ where $c_i \in [0, \dots, n]$. The number of basis functions
required for the nth order expansion under this scheme would then be $2(n+1)^d$.
Fortunately using the same argument as for the univariate case, only half of the terms
must be kept and the ith basis function is defined as
\begin{equation}
\varphi(x)_i = \cos \left(  \pi \mathbf{c}^i \cdot \mathbf{x} \right)
\end{equation}

While Fourier basis severely suffer from the curse of dimensionality, the approach has
the merit of being simple and effective as suggested by empirical evidences in this
work and in \cite{Konidaris2011b} . The only choice a practitioner has to make is the
order of the expansion and reasonable results can usually obtained by using only the
few first lower frequencies.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Temporal Abstraction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Temporal Abstraction}

It has been shown in the previous section how the value function over large 
(possibly infinite)
state spaces can be represented compactly using function approximation. It would
then appear natural to harness a similar idea for exploiting \textit{temporal
regularities} of the problem at hand. In the context of planning, reasoning at different
abstract time scales can drastically reduce the time complexity for finding a solution. 
The importance of temporal abstraction can be appreciated easily under everyday
scenarios such as preparing coffee, going on a camping trip, changing a bicycle tire,
etc. When preparing an Espresso, a Barista might need to make use of her
\textsc{turn-on-the-espresso-maker} ability, followed by \textsc{grind-coffee-beans},
\textsc{tamp-the-espresso}, \textsc{lock-the-portafilter}, and \textsc{pull-a-shot}.
Capitalized expressions where used in this last example to highlight the fact that these
\textit{actions} last arbitrary amount of time and should rather be seen as
\textit{closed-loop} policies. Although the tasks of preparing coffee might seem trivial
even for the inexperienced, it does rely on a complicated sequence of \textit{atomic}
decisions procedures which would be difficult to obtain with absolutely no prior
knowledge. 

Many formalisms have been proposed to cast this problem efficiently in the framework
of reinforcement learning: the MAXQ method \cite{Dietterich1998} and Hierarchical
Abstract Machines (HAM) \cite{Parr1997} or Macro-Actions \cite{Hauskrecht1998} for
example. These other approaches will not be covered as the options framework of 
\cite{Sutton1999} will be adopted in this work. For a survey of the earlier attempts, the
interested reader is referred to \cite{StolleThesis2004, Sutton1999}.

\section{Options Framework}
\begin{defn}
An \termidx{Option} is a triple $\langle \mathcal{I} \subseteq \mathcal{X},\;
\mathcal{\pi} : \mathcal{X} \to \Omega(\mathcal{A}),\; \beta : \mathcal{X} \to [0, 1]
\rangle$ consisting of an initiation set, a policy $\pi$ and a termination condition
$\beta$.
\end{defn}

Given a state $x \in \mathcal{X}$ and option $o \in \mathcal{O}$, $o$ can be
executed in a \textit{call-and-return} fashion up to termination, dictated by $\beta$,
only if $x \in \mathcal{I}$. The initiation component is meant to facilitate decision
taking by reducing the size of possible options to consider for any given state. Its
presence also allows easier bridging with the previous work in STRIPS-style
\cite{Fikes1972} planning. In many efforts to use the options framework \todo{cite an
example}, the definition of this component is neglected and it is assumed that options
are available everywhere. The present work however proposes a method to alleviate
this difficulty by automatically curtailing the set of admissible states.

The options framework is at the crossroad of regular \mdps and semi-\mdps
\cite{Bradtke1994}. It considers a base \mdp overlaid with variable length course of
action represented as options. It is shown in \cite{Sutton1999} (theorem 1) how an
MDP and pre-defined set of options form a semi-\mdp and can thus reuse most of the
known theory on the subject. As opposed to the general theory of SMDP that treats
extended actions as indivisible and \textit{opaque} decision units, the options
framework also allows to look at the structure \textit{within}. It can also accommodate
both Markov options and semi-Markov ones. In the example coffee domain presented
above, \textsc{pull-a-shot} would be an option defined by a \textit{timeout} and not
only by the current state. It would then most likely be semi-markov option unless the
state representation is changed to support the non-markovianity.

With this formulation, primitive actions can be seen as a special kind of options which
is available everywhere, has a policy always choosing the same action, and lasting
exactly one step. 

\section{Bellman Equations for Options}

Let $\mu : \mathcal{X} \times \mathcal{O} \to [0, 1]$  be a general (Markov or semi
Markov) policy where $\mathcal{O}$ is a set of options (containing primitive actions or
not). The value of $\mu$ is defined as

\begin{equation}
V^\mu(x) = \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k
V^\mu(X_{t+k})  \given \varepsilon(x, \mu, t) \right]
\label{eq:value-option-policy}
\end{equation}

where $k$ is a random variable denoting the duration of the first option selected by
$\mu$ and $\varepsilon(x, \mu, t)$ is the event that the option policy $\mu$ is
executing at time $t$ in state $x$. Once $\mu$  terminates, it presumably selects  a
different option and the process is repeated until the episode terminates. Similarly, we
can define an \termidx{option value function} for control as 
\begin{align}
Q^\mu(x, o) &= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} +
\gamma^k V^\mu(X_{t+k})  \given \varepsilon(x, o, t) \right] \notag \\
&= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \sum_{o^\prime
\mathcal{O}_x} \mu(X_{t+1}, o^\prime)Q^\mu(x, o^\prime) \given \varepsilon(x, o,
t) \right]
\label{eq:option-value-function}
\end{align}

$\mathcal{O}_x$ in the equation \ref{eq:option-value-function} is the subset of
options which can be initiated under state $x$. The optimality principle is carried over
the following set of optimal Bellman equations

\begin{equation}
V_{\mathcal{O}}^\optimal(x) = \max_{o \in \mathcal{O}_x} \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k V_{\mathcal{O}_x}^\optimal(X_{t+k})  \given \varepsilon(x, o, t) \right]
\end{equation}

The \termidx{optimal option value function} is in turn

\begin{align}
Q_{\mathcal{O}}^\optimal(x, o) &= \expectation \left[ R_{t+1} + \dots +
\gamma^{k-1}R_{t+k} + \gamma^k V_{\mathcal{O}_x}^\optimal(X_{t+k})  \given
\varepsilon(x, o, t) \right] \notag \\
&= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k
\max_{o^\prime \in \mathcal{O}_{x_{t+k}}} Q_\mathcal{O}^\prime(o^\prime, X_{t+k}) 
\given \varepsilon(x, o, t) \right]
\label{eq:optimal-option-value-function}
\end{align}

If the optimal option value function (eq \ref{eq:optimal-option-value-function}) is
available, the greedy policy is guaranteed to be optimal. A policy over options is
optimal if given a set of options, its underlying option value function is the optimal
value function, i.e. $V^{\mu_\mathcal{O}^\optimal}(x) = V_\mathcal{O}^\optimal(x)\;
\forall x \in \mathcal{X}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Learning Behavior Policies
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Learning Behavior Policies}

Assuming that a options set $\mathcal{O}$ is specified, existing theory
\cite{Puterman1994, Bradtke1994} from semi-\mdps in continuous time can be
applied to solve the control problem. The SMDP model considers decision making at
\textit{decision epochs} spaced by random time intervals during which the system
dynamics are of no interest. The optimal control problem can be solved by an
extension of Q-learning to the SMDP case.

\begin{defn}{SMDP Q-Learning}
\begin{equation}
Q_{t+1}(x, o) \leftarrow Q_{t}(x, o) + \alpha \left[ \mathcal{R} + \gamma^k
\max_{o^\prime \in \mathcal{O}_x^\prime} Q_{t}(x^\prime, o^\prime) -
Q_t(x, o) \right]
\label{eq:smdp-qlearning}
\end{equation}
\end{defn}

The semantics of equation \ref{eq:smdp-qlearning} holds the under the event of
$\varepsilon(x, o, t)$ where option $o$ is executed for $k$ time steps with cumulative
discounted reward $\mathcal{R}$. Convergence results are provided in \cite{Parr1998}.

An identical algorithmic construct called \termidx{Macro Q-learning} is also often
encountered in the literature. It comes from the work on Macro-actions of
\cite{Mcgovern1997} as an extension of the deterministic macro-operators
\cite{Korf1985} to closed-loop policies. It might have been that the options framework
had not yet beaten the pathways rigorously enough that the authors of
\cite{Mcgovern1997} decided use a different terminology. 

As opposed to the more general SMDP Q-Learning algorithm of equation
\ref{eq:smdp-qlearning}, Macro Q-Learning treats macro-actions (options in this case)
and primitive actions separately: an SMDP update rule is applied for macro-actions
while primitive actions are updated using \ref{eq:qlearning-update}. Clearly, SMDP Q-learning encompasses Macro Q-Learning and can be expressed identically as

\begin{equation}
Q_{t+1}(x, o) \leftarrow Q_{t}(x, o) + \alpha \left[ \mathcal{R} + \gamma^k
\max_{o^\prime \in \mathcal{O}_x^\prime} Q_{t}(x^\prime, o^\prime) -
Q_t(x, o) \right]
\end{equation}

but with the only constraint that the set of options $\mathcal{O}$ must contain
primitives actions wrapped as options.

\textsc{Macro Q-Learning} is thus equivalent to \textsc{SMDP Q-Learning} where the
options set $\mathcal{O}$ is augmented with single-step options for each primitive
action.

\subsection{Intra-Option Learning}

In order for the Q-Learning algorithm in equation \ref{eq:smdp-qlearning} to provide a
good estimate of the optimal option value function, sufficient experience must be
obtained about the option set at every states. While executing an option to completion,
valuable fragments of experience are completely ignored rendering the SMDP methods
highly sample-inefficient.

Intra-option learning is an off-policy algorithm which tries to leverage the valuable
content \textit{within} the options. While an option is executing, it simultaneously
updates the value function estimate of all the consistent options which would have had
chosen the same action under a given state. It is thus required that the options be
defined by deterministic policies so that this idea of consistency can be established. 
By virtue of being an off-policy method, intra-option learning also has the potential of
learning about nonterminating options. 

\begin{defn}{Intra-option value learning}
\begin{align}
U(x, o) &= (1 - \beta(x))Q_t(x, o) + \beta(x) \max_{o^\prime \in \mathcal{O}} Q_t(x, o^\prime) \\
Q_{t+1}(x, o) &= Q_t(x, o) + \alpha \left[ R_{t+1} + \gamma U(x, o) - Q_t(x, o) \right]
\label{eq:intra-option-learning}
\end{align}
The update rule takes place under the event $\varphi(x, o, t)$ after each primitive
transition and is applied over all other consistent options for which $\pi(X_t) = A_t$.
\end{defn}

Intra-Option learning being an off-policy method, it is susceptible to suffer from i
nstabilities and divergence issues in a similar way to the classical Q-Learning algorithm
with function approximation. GQ($\lambda)$ was introduced in \cite{Maei2010} as an
extension of Q-Learning but with special treatment against the aforementioned
problems. While the theory for GQ($\lambda$) is theoretically correct, empirical
evidences remain to be gathered to better understand its behavior. Importance sampling
corrections similar as those used in \cite{Precup2001} for off-policy TD($\lambda$)
could then be used as a substitute. 

Finally, in order to learn an option's policy $\pi$, it is customary to adopt the notion of 
\termidx{subgoal}. Options can be seen as optimizing some intrinsic notion of reward
consistent with the overall task's objective. A \termidx{terminal subgoal value} assigns a
value for reaching the terminal states of an option as determined by $\beta$. A subgoal
value function thus arises from the combination of the original underlying MDP plus the
overlaid discounted subgoal value function. This concept is often referred to as a 
\termidx{pseudo-reward} function: a term originally coined in \cite{Dietterich1999}
about the MAXQ framework.	


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% From Structure to Dynamics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{From Structure to Dynamics}

Graph theory has deep ramifications in almost every mathematical fields. It does not come as a surprise then to consider taking a graph perspective of \mdps. As it will be shown below, spectral graph theory provides powerful
tools for studying the dynamical properties of Markov chains. The graph
representation of Markov chains will make matrix theory and linear
algebra sufficient tools for proving rich results elegantly. Some of the early
applications making use of this connection can be traced back to some early work by \cite{SimonAndo1961, DonathHoffman1973, Fiedler1973}.

Spectral graph theory will provide the theoretical justification for the heuristic approach
to options discovery algorithm proposed in this work. This presentation should be seen
as an initial foray towards a better theoretical grounding of the bottleneck concept. The notion of random walk on an arbitrary graph
$\mathcal{G}$ will first be introduced, followed by a characterization of the properties of the graph Laplacian and its representation for MDPs. It will be then shown how
graph partitioning can be achieved on the basis of the dynamics induced by the random
walk process. Finally, it will be argued that the mixing time property of MDP-induced
graphs is relevant to the problem of options discovery and spectral graph partitioning
can provide the necessary construction. 

\section{Random Walk}
\label{sec:random-walk}
The notion of random walk is necessary in the presentation of some important spectral
properties related to the graph Laplacian in section \ref{sec:laplacian}. Given a graph
$G$, we can generate a random sequence by picking a vertex uniformly at
random in the neighborhood of an initial vertex and carrying on the procedure over the
newly obtained vertex. This procedure corresponds to the realization of a type of
random process called \textit{random walk}. More precisely, we define a \textbf{simple
random walk} on a graph $G$ as a Markov Chain having $V$ as state-space and for
which the transition matrix is given by:

\begin{equation}
P_{ij} = \begin{cases} 
\frac{w_{ij}}{d(i)} \hspace{2mm} \mbox{if $i \sim j$} \\
0 \hspace{2mm} \mbox{otherwise}
\end{cases}
\end{equation}

Using matrix notation, the stochastic matrix of the random walk can be written as
\begin{equation}
P = \mathbf{D}^{-1}\mathbf{W}
\label{eq:random-walk}
\end{equation}

If each vertex of $G$ has the same degree -- in which case $G$ is said to be \textit{d
regular} -- the stationary distribution is the uniform distribution. 

% Say that P is reversible

\section{Graph Laplacian}
\label{sec:laplacian}

The combinatorial (unnormalized) graph Laplacian of a weighted graph $G$ is defined
as
\begin{equation}
\mathbf{L} = \mathbf{D} - \mathbf{W}
\label{eq:unnormalized-laplacian}
\end{equation}

and corresponds to the symmetric matrix
\begin{equation}
L(u,v) = 
\begin{cases}
d_v & \mbox{if $u = v$,} \\
-1 & \text{if $u$ and $v$ are adjacent,} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

Where $\mathbf{D} = \text{diag}(d_1, d_2, \dots, d_n)$ and $\mathbf{W}$ are the
diagonal degree and weight (adjacency) matrix for $G$ respectively.

When stochastic matrices are being manipulated, it is customary to work with the
\textit{normalized}
version of the Laplacian such that
\begin{equation}
\laplacian = \mathbf{D}^{-1/2} \mathbf{L} \mathbf{D}^{-1/2} = \mathbf{I} -
\mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2}
\label{eq:normalized-laplacian}
\end{equation}	

where $\mathbf{L}$ is the unnormalized Laplacian defined in equation
\ref{eq:unnormalized-laplacian}.  The elements of the matrix $\laplacian$ then become

\begin{align}
\laplacian(u,v) = 
\begin{cases}
1 & \text{if $u = v$ and $d_v \neq 0$,} \\
-\frac{1}{\sqrt{d_u d_v}} & \text{if $u$ and $v$ are adjacent,}\\
0 & \text{otherwise}
\end{cases}
\end{align}

Clearly, $\laplacian$ is also symmetric since
\begin{align}
\laplacian^\transpose = \left(\mathbf{D}^{-1/2} \right)^\transpose \mathbf{L}^\transpose \left( \mathbf{D}^{-1/2} \right)^\transpose = \laplacian
\end{align}

The spectral theorem for symmetric matrices \cite{Watkins2010} also ensures that
$\laplacian$ only has real eigenvalues. Furthermore, eigenvectors corresponding to
distinct eigenvalues must be orthogonal.

The normalized Laplacian is closely related to the transition matrix $P$ of the random
walk process presented in section \ref{sec:random-walk}. It can be seen through the
identity 

\begin{equation}
\mathbf{I} - \laplacian = \mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2}
\end{equation}

that

\begin{align}
P &= \mathbf{D}^{-1/2} \left( \mathbf{I} - \laplacian \right) \mathbf{D}^{1/2} \label{eq:rw-similarity} \\
 &= \mathbf{D}^{-1/2} \left( \mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2} \right) \mathbf{D}^{1/2} \notag \\
 &= \mathbf{D}^{-1} \mathbf{W} \notag
\end{align}

It can be seen from equation \ref{eq:rw-similarity} that $P$ is similar (in the
linear algebra sense) to $\mathbf{I} - \laplacian$. It follows that they also share
the same eigenvalues and the right eigenvectors of $\mathbf{I} -\laplacian$ can be
obtained from the eigenvectors of $P$ by multiplying them by $\mathbf{D}^{1/2}$

\begin{align}
P\mathbf{v} &= \lambda \mathbf{v} \notag \\
\mathbf{D}^{-1/2} \left( \mathbf{I} - \laplacian \right) \mathbf{D}^{1/2} \mathbf{v}  &= \lambda \mathbf{v} \notag \\
\left( \mathbf{I} - \laplacian \right) \mathbf{D}^{1/2} \mathbf{v}  &= \lambda \mathbf{D}^{1/2} \mathbf{v} \notag \\
\left( \mathbf{I} - \laplacian \right) \mathbf{u} &= \lambda \mathbf{u} \\
\mathbf{u} &= \mathbf{D}^{1/2} \mathbf{v}
\label{eq:similarity-rw}
\end{align}

Because of this connection to random walks, some authors have defined the \termidx{normalized random walk Laplacian} as

\begin{equation}
\laplacian_{rw} = \mathbf{D}^{-1} L = \mathbf{I} - \mathbf{D}^{-1} \mathbf{W}
\end{equation}

% Difference operator
The Laplacian can then be thought as an \termidx{operator} acting upon the space of
functions $\mathcal{F} : V \to \mathbb{R}$, with $V$ denoting the vertex set.
Expressing such functions as vector $f \in \mathbb{R}^{|V|}$, the normalized Laplacian
can be shown to satisfy
 
\begin{align}
\mathcal{L}f(u) &= f(u) - \sum_{\substack{ v \\ u \sim v}} \frac{f(v)}{\sqrt{d_ud_v}} \notag \\
 &= \frac{1}{\sqrt{d_u}} \left( \sum_{\substack{ v \\ u \sim v}} \frac{f(u)}{\sqrt{d_u}} - \frac{f(v)}{\sqrt{d_v}} \right)
\label{eq:difference-operator}
\end{align}

The same could also be said about the normalized Laplacian generalized over weighted
graphs, in which case
\begin{equation}
\laplacian f (u) = \frac{1}{\sqrt{d_u}} \left( \sum_{\substack{ v \\ u \sim v}} \frac{f(u)}{\sqrt{d_u}} - \frac{f(v)}{\sqrt{d_v}} \right) w_{uv}
\end{equation}

where $w_{uv}$ denotes the edge weight between adjacent vertices $u$ and $v$. 

An $n \times  n$ symmetric matrix $A$ is said to be positive semidefinite (PSD) if for any vector $x \in \mathbb{R}^n, x^\transpose A x \geq 0$.  The normalized Laplacian admits a quadratic form from which the PSD property becomes clear
\begin{align}
f^\transpose \laplacian f &= f^\transpose \left( \mathbf{I} - \mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2} \right) f \notag \\
&= f^\transpose f - f^\transpose \mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2} f \notag \notag \\
&= \sum_{i} f_i^2 - \sum_{i} \frac{f_i}{\sqrt{d_i}} \sum_j w_{ij} \frac{f_j}{\sqrt{d_j}} \notag \\
&= \frac{1}{2} \left( \sum_i f_i^2 - 2 \sum_{i,j} \frac{f_i}{\sqrt{d_i}} \frac{f_j}{\sqrt{d_j}} w_{ij} + \sum_j f_j^2 \right) \notag \\
&= \frac{1}{2} \sum_{ij} \left(  f_i^2 -2\frac{f_i}{\sqrt{d_i}}\frac{f_j}{\sqrt{d_j}} w_{ij}  + f_j^2 \right) \notag \\
&=  \frac{1}{2} \sum_{ij} w_{ij} \left(  \frac{f_i^2}{d_i} -2\frac{f_i}{\sqrt{d_i}}\frac{f_j}{\sqrt{d_j}} + \frac{f_j^2}{d_j} \right) \hspace{5mm} \text{by defn. $d_i = \sum_j w_{ij}$} \notag \\
&= \frac{1}{2} \sum_{ij} w_{ij} \left( \frac{f_i}{\sqrt{d_i}} - \frac{f_j}{\sqrt{d_j}} \right)^2
\label{eq:quadratic-form}
\end{align}

Clearly, $f^\transpose \laplacian f \geq 0$, and $\laplacian$ must be positive
semidefinite. Under the definition of the combinatorial Laplacian, a similar quadratic
form is induced \cite{Mohar91}

\begin{equation}
f^\transpose L f = \sum_{ij} w_{ij} (f_i - f_j)^2
\end{equation}

The quadratic form can be interpreted as a measure of smoothness of a  function over
the graph. Intuitively, if $f$ is smooth over each pair of vertices, $f^\transpose
\laplacian f$ will be small. On the other hand, if $f$ locally varies is a more drastic
manner, this quantity will grow larger.  The weighting factor in equation
\ref{eq:quadratic-form} also helps to understand how local density might influence this
measure of smoothness. In regions of higher density, the proximity or similarity
encoded through $w_{ij}$ is higher; hence the greater amplification of the $(f_i - f_j)^2$
term. This property can be exploited in semi-supervised learning to encode the so-
called \termidx{cluster assumption} and forcing the decision boundary to lie in low
density regions \cite{Chapelle2002}.

% induce regular norm , projection on eigenspace of laplacian produce smoothest
%approximation, if piecewise smoot should be compressible in some basis
% Relate quadratic form to smoothness and relation to continuous Laplace-Beltrami
% Dilatation of operators and invariant subspaces. motivate approach: constant
%dynamics. compressible in some basis

\section{Laplacian of MDPs}
Consider the \mrp induced by fixing a policy $\pi$ for an MDP and let $\mathbf{P}^\pi$
denote the corresponding transition probability matrix. As shown earlier, a kind of
Laplacian closely related to the normalized Laplacian can be obtained from the transition
matrix of a random walk process. Clearly, $\mathbf{P}^\pi$ also defines a stochastic
matrix encoding the dynamics proper to policy $\pi$ and the notion of Laplacian can
naturally be extended for MDPs. 

\begin{defn}
Given a transition probability matrix $\mathbf{P}^\pi$ underlying an MDP for a fixed
policy, the \termidx{Laplacian of an MDP} is
\begin{equation}
\laplacian^\pi = \mathbf{I} - \mathbf{P}^\pi
\label{eq:mdp-graph-laplacian}
\end{equation}
\end{defn} 

The Laplacian view of \mdps was adopted by a number of authors \cite{Lamond1989,
Puterman1994, Filar2007} for the study of their induced Markov chains by the
decomposition of the transition matrix through generalized inverse. While the term 
Laplacian is not explicitly mentioned, their analysis fundamentally relies on the
properties of the so-called \textit{Drazin} inverse \cite{Drazin1958} of $\mathbf{I} -
\mathbf{P}^\pi$. The work on Proto-Value Functions (PVF) \cite{Mahadevan2009}
exploits the concept of Laplacian (and terminology) for the purpose of representation
and control.

An obstacle to studying the spectrum of $\laplacian^\pi$ as defined above lies in the
difficulty to obtain a diagonalization when the induced Markov chain is not
reversible. Furthermore, the assumption that $\mathbf{P}^\pi$ is available from an
existing MDP is problematic. Under the reinforcement learning framework, the transition
matrix is generally unknown, thus calling for sampling-based methods to estimate it. In
certain scenarios, the control problem is itself very hard to solve. Therefore, requiring
the knowledge of $\laplacian^\pi$ to facilitate the problem would be nothing but a
chicken-and-egg scenario. In the approach proposed in this work, the random walk
Laplacian would be considered as replacement for practical purposes. Such substitution
is also adopted in \cite{Mahadevan2009} for the same reasons.

\section{Graph Partitioning}
Through the study of the set of eigenvalues associated with the Laplacian, some remarkable results on 	the subgraph structures can be obtained. The set of problems concerning the optimality of graph cuts with regard to the size of their components is commonly referred to as \termidx{isoperimetric problems}. Edge cuts are of particular interest as it will be shown below that they relate to spectrum of $\laplacian$ through the so-called Cheeger constant. 

% Define boundary
Let $A, B \subseteq V$ be subsets of the vertices of a graph $G$ and $E(A, B)$ denote the set of edges having one end in $A$ and the other in $B$. It is also convenient to describe the set of edges having only one end point in $S \subseteq V$ as the \termidx{boundary} \cite{Chung1994}, \termidx{edge boundary}\cite{Chung1997} or \termidx{coboundary} \cite{Mohar91} of $S$, that is
\begin{equation}
\partial S = \{ \{u, v\} \in E : u \in S \text{ and } v \not \in S \}
\end{equation}

% Define conductance
The \termidx{volume} for $S$ also corresponds to the sum of the degrees of its vertices
\begin{equation}
\vol(S) = \sum_{x \in S} d_x
\end{equation}

The \termidx{conductance} of $G$, also known as the \termidx{Cheeger constant} \cite{Chung1997}, is defined as

\begin{equation}
h_G = \min_S h_G(S) \hspace{2.5mm} \text{where} \hspace{2.5mm} h_G(S) = \frac{ |E(S, \bar{S}) |}{\min \left( \vol(S), \vol(\bar{S}) \right)}
\label{eq:cheeger-constant}
\end{equation}

The \termidx{Cheeger inequality} relates it to the spectrum of $\laplacian$ as follow \cite{Chung1997}
\begin{lem}
\begin{equation}
2h_g \geq \lambda_1 > \frac{h_g^2}{2}
\end{equation}
\end{lem}

The more general graph partitioning problem can be better understood first in terms of
the graph bipartitioning one. A natural attempt at solving this problem would be to
express it under a \textsc{min-cut} formulation which could be solved in time
$\mathcal{O}(n^3)$ using the Ford-Fulkerson algorithm \cite{FordFulkerson1956}. The
resulting partitions however tend to be highly imbalanced and in certain cases can
degenerate to singleton partitions \cite{Hagen1992}. A related problem to consider is
then the \textsc{minimum-width bisection} which attempts at finding a minimum cut
with partitions of equal size. Unfortunately, the problem was shown to be
$\mathcal{NP}$-complete by \cite{Garey1976}. A natural response is then to relax the
optimization criterion in such a way to maintain balanced partitions without requiring
them to be of same size. The \termidx{ratio-cut} criterion is defined as \cite{Chuen1989, Chuen1991, Hagen1992} 

\begin{equation}
RCut(S) = \min_S \frac{|E(S, \bar{S})|}{|S| \cdot |S'|}
\end{equation}

and is very much related to the modified Cheeger constant \ref{eq:cheeger-constant}
also called the \termidx{isoperimetric number} \footnote{Although some authors seems
to make no distinction between the terms \cite{ShiMalik2000, Levin2008},
\cite{Chung1997} shows in chapter 2 how they relate differently to the eigenvalues of
$\laplacian$.}. Once again,  finding the minimum ratio cut is also $\mathcal{NP}$-
complete but a spectral relaxation based on the combinatorial Laplacian (see equation
\ref{eq:unnormalized-laplacian}) is proposed in \cite{Hagen1992}.

A similar measure called the \termidx{normalized cut} was made popular in \cite{ShiMalik2000} and considers

\begin{equation}
NCut(S) = \frac{|E(S, \bar{S})|}{\vol(S)} + \frac{|E(S, \bar{S})|}{\vol(\bar{S})}
\label{eq:ncut}
\end{equation}

Just as the ratio cut, the normalized cut also falls short of an exact solution which is not
$\mathcal{NP}$-complete \cite{ShiMalik2000}. The \textit{normalized} qualification in
the name \textit{normalized cut} can however seem misleading as it might imply that
the normalized Laplacian as defined in equation \ref{eq:normalized-laplacian} is directly
exploited. Indeed, the optimization problem is first set \cite{Luxburg2007} as
\begin{align}
\min_A \mathbf{f}^\transpose \left( \mathbf{D} - \mathbf{W} \right) \mathbf{f} \\
\text{subject to} \hspace{2.5mm} \mathbf{D} \mathbf{f} \perp \indicator, \hspace{2.5mm}  \mathbf{f}^\transpose \mathbf{D} \mathbf{f} = \vol(V) \notag \\
f_i = \begin{cases}
\sqrt{\frac{\vol(A)}{\vol(\bar{A})}} & \text{if $v_i \in A$} \\
-\sqrt{\frac{\vol(\bar{A})}{\vol(A)}} & \text{if $v_i \in \bar{A}$} 
\end{cases} \notag
\end{align}

where $\mathbf{D} - \mathbf{W}$ can be recognized to be the combinatorial Laplacian
as defined in equation \ref{eq:unnormalized-laplacian}.  By relaxing $\mathbf{f}$ to 
values in $\mathbb{R}$ instead of discrete ones and setting $g = \mathbf{D}^{1/2}
\mathbf{f}$, the problem becomes

\begin{align}
\min_{g \in \mathbb{R}^{|V|}} \mathbf{g}^\transpose\mathbf{D}^{-1/2}\left( \mathbf{D} - \mathbf{W} \right) \mathbf{D}^{-1/2} g \\
\text{subject to} \hspace{2.5mm} \mathbf{g} \perp D^{1/2} \indicator, \hspace{2.5mm}  \| \mathbf{g} \|^2 = \vol(V) \notag \\
\end{align}

Under this form, the normalized Laplacian $\laplacian$ (equation 
\ref{eq:normalized-laplacian}) finally appears. By the Rayleigh-Ritz theorem, this
quantity is minimized when $\mathbf{g}$ equals the second eigenvector of $\laplacian$
\cite{Chung1997}. From the real vector $g$, the partition membership can be recovered
by thresholding its components. The problem of multi-partitioning or \textit{k-way} partitioning can be solved by applying the spectral bipartitioning approach described above in a top-down recursive manner in each partition.

The normalized cut criterion not only seems to provide better empirical results, but also
provides an intuitive interpretation in terms of random walks. Recall the connection
drawn from $\mathbf{I} - \laplacian$ to the normalized random walk Laplacian $P$ in
equation \ref{eq:rw-similarity} where they were shown to be similar. Therefore, if
$\lambda$ is an eigenvalue of $\laplacian$, then $1 - \lambda$ must be an eigenvector
of $P$. 

It is shown in \cite{Shi2001} how this can be further exploited to express the Ncut
criterion in terms of probability distributions from a random walk process over the
graph. The stationary distribution of a random walk on a connected, non-bipartite graph
can be shown \cite{Levin2008} to be equal to
\begin{equation}
\pi_i = \frac{d_i}{\vol(V)}
\end{equation}

The probability of transitioning from a partition $A$ to $B$ if at the stationary distribution the initial state is in $A$, is written as
\begin{equation}
P(A \to B | A) = \frac{\sum_{x \in A, y \in B} \pi(x) P(x, y)}{\pi(S)}
\label{eq:rw-pr}
\end{equation}

which, for a random walks, boils down to 
\begin{equation}
P(A \to B | A) = \frac{\sum_{x \in A} d_x}{\vol(A)}
\end{equation}

From equation \ref{eq:rw-pr} and \ref{eq:ncut}, it is then easy to see that
\begin{equation}
NCut(S) = P(S \to \bar{S} | S) + P(\bar{S} \to S | \bar{S})
\end{equation}

Interestingly, \cite{Levin2008} calls this measure in \ref{eq:rw-pr} the
\termidx{bottleneck ratio} and defines the \termidx{bottleneck ratio for a Markov chain}
as 
\begin{equation}
\Phi_\star = \min_{S\; : \;\pi(S) \leq \frac{1}{2}} P(S \to \bar{S} | S)
\end{equation}
which corresponds to the Cheeger constant presented in equation 
\ref{eq:cheeger-constant}. Such a quantity happened to have been studied intensively as
a way to bound the \termidx{mixing time} of Markov chains through the notion of
\termidx{conductance} (a term referring to the same Cheeger constant or the bottleneck
ratio presented here). The notion of mixing time can described intuitively as the number
of steps necessary for Markov chain to reach its stationary distribution. Since it is the asymptotic behavior of some Markov chain that is involved, the number of steps involved would not be finite. It is therefore customary to define the notion of mixing time as being \textit{close to} the steady-state distribution \cite{Jerrum1988}.

The smaller the conductance is, the longer the chain is expected to \textit{mix}.
Furthermore, the Markov chain would spend more time within a given partition of the
graph and with lower probability, transition to different adjacent partition. It can then be
understood that low conductance would also result in lower probabilities of inter
partition jumps.

It has been presented above how the second smallest eigenvector of $\laplacian$
minimizes the relaxed $NCut$ criterion and can also bound the Cheeger inequality
\ref{eq:cheeger-constant}. Therefore, it comes as no suprise to see it appearing in many
bounds on the mixing time under the form of the \termidx{spectral gap} $1 - \lambda_1$ The interested reader is referred to \cite{Jerrum1988, Lovasz1996,
Levin2008} for some of these results. 
 
\section{Motivation}

The definition of what makes for \textit{interesting} options has remained vaguely
defined throughout the vast body of work on bottleneck-like approaches in
reinforcement learning. The intuition developed in this chapter might provide further
development on this question. After having distilled the essence of the bottleneck
concept through the graph Laplacian, it is also easier to obtain an historical perspective
along the work on \textit{nearly completely decomposable Markov chains}

It has been shown in this chapter how the optimal bipartitioning problem relates to the
cheeger constant or bottleneck ratio which in turn, provide bounds on the mixing time
of Markov chains. It might seem then that bottlenecks allow for the discovery of fast
mixing regions of the state space. In the planning problem, fast mixing might translate
into computational speedup, making policies \textit{simpler} to compute in those
regions. Efficient random sampling techniques of large state spaces are also often
concerned fast mixing time \cite{Boyd2004}. In reinforcement learning, the same
problem would be translated under the question of designing smart \textit{exploration}
strategies. Optimal partitioning of the MDP-induced graph Laplacian could thus help
identifying regions of fast mixing time and create options tailored for exploration.

It also seems that good options are those which are intrinsically \textit{simple}, acting as
elementary building building blocks of increasingly complex hierarchies. Traces of this
ideal of simplicity can be observed under different forms. For example, its is
known from\cite{Shi2001}, that the stochastic matrix $P$ has piecewise constant
eigenvectors with respect to the $NCut$ partitioning. This suggests that optimal
partitions can be described more succinctly (simply). The authors also note the
potential connection with the notion of \termidx{lumpability} introduced in
\cite{Kemeny1976}. Lumpability essentially consists in aggregating states of similar
dynamics so as to simplifying Markov chains when confronted with the curse of
dimensionality. 

The idea of matrix decomposition was initiated in \cite{AndoFisher1963} from
economics for the study of linear dynamical systems with \textit{nearly decomposable}
structures. The class of Markov chains with strong intra-cluster interactions and weak
inter-cluster ones has then been studied under the name of \textit{nearly completely
decomposable Markov chains} (NDMC) \cite{Gaitsgori1975} and covered by
\cite{Courtois1977} in a book on queuing theory. The control problem for Markov chains
with such structure was considered in \cite{Teneketzis1980, Delebecque1981,
Phillips1981, Coderch1983} building upon the Courtois' pertubational aggregation
decomposition. This approach treats the \textit{slower} inter-cluster transitions as
matrix perturbations. Similar ideas are also brought closer to the framework of \mdps
with \cite{Haviv1985, Aldhaheri1991, Zhang1997}. 

From an information theoretic point of view, lumpability can be seen as a lossy
compression scheme \cite{Lindqvist1978}, \cite{Watanabe1960}. On the more general
topic of graph clustering, the  \textsc{infomap} algorithm of \cite{Rosvall2008} also
exploits this compression intuition. By taking random walks in the graph, their
algorithms seeks a partitioning of \textit{minimum description length}. More recently,
\cite{Deng2011} gave a characterization of the optimal aggregation of NDMC systems in
terms of the Kullback-Leibler divergence. They show that optimal partitioning yields
aggregates of maximum \textit{predictability} as measured by the mutual information
between $X(t)$ and $X(t+1)$. It should be stressed that the applicability of these results
were constrained to the uncontrolled case. Similar derivations could be attempted in the
controlled case only by considering the rate distortion for a fixed value function such as
in \cite{Still2012}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Building Options
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Building Options}

It was shown in the last chapter how spectral graph theory can offer a dual perspective
in studying the dynamics of Markov chains. For the problem of options discovery, the
properties of interest in the graph Laplacian are those which could capture
\textit{temporal regularities}. An algorithm seeking this principles is presented in this
chapter. As opposed to the other similar algorithms \cite{Menache2002, Mannor2004,
Mathew2012, Bouvrie2012}, a continuous state space is assumed for the MDPs. This
brings significant challenges in terms of graph representation and computational
complexity which had not been tackled before.

The main distinguishing factor between the control techniques developed in
reinforcement learning and those found in traditional stochastic dynamic programming
is that the latter assumes the knowledge of the transition and reward probabilities. The
graph Laplacian underlying an MDP (equation \ref{eq:mdp-graph-laplacian}) must then
be assumed to be inaccessible in the RL setting. By sampling a large number of state
action transitions, the dynamics could be recovered in the limit but the curse of
dimensionality would render this approach hopeless.

The replacement strategy adopted here consists in building a \textit{proximity graph}
over sampled states and studying the dynamical properties of random walks over this
graph rather than the MC induced by fixing an optimal policy for an MDP. This reduction is not without consequence as the precious optimality-preserving transitions of $\mathbf{P}_\pi$ are completely ignored. The random walk Laplacian will however still capture relevant geometrical features of the state space, which in certain environments,  
also reflect optimal structures of the solution space. \cite{Mahadevan2007} also faced a similar trade-off but argues for its merits in transfer learning.

\section{Graph Construction}
%\subsection{Nearest Neighbors Graph}
%\subsection{Relative Neighborhood Graph}
%\subsection{Grabriel Graphs}
%\subsection{Beta-Skeleton}
\begin{figure}
  \centering
      \subbottom[Mutual]{%
    \includegraphics[width=0.45\linewidth]{fig/Mutual-graph.eps}}
  \subbottom[Symmetric Graph]{%
    \includegraphics[width=0.45\linewidth]{fig/Symmetric-graph.eps}}
  \subbottom[Gabriel]{%
    \includegraphics[width=0.45\linewidth]{fig/Gabriel-graph.eps}}
  \subbottom[RNG]{%
    \includegraphics[width=0.45\linewidth]{fig/RNG-graph.eps}}

  \caption{Proximity Graphs}
\end{figure}
\section{Graph Clustering}
% Walktrap algorithm
\section{Initiation and Termination}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiments}
% Distributed, parameters of sarsa

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{library}
\bibliographystyle{plain}

\printindex

\end{document}