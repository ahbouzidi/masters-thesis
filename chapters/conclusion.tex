The bottleneck approach to options discovery can appear rebuttal because of its lack of proper theoretical foundations. This thesis attempted to motivate the soundness of the family of bottleneck heuristics by establishing a connection to spectral graph theory and relating it to vast body of work on Nearly Decomposable Markov Chains (NDMC) spanning across more than three decades. Furthermore, an algorithm based on these principles was proposed for discovering options in continuous state spaces. While many bottleneck-like approaches have succeeded in applying them to discrete domains, none had yet shown how to extend these ideas in a continuous domain such as Pinball. The empirical evaluation of the proposed algorithm showed its feasibility but also highlighted difficulties in practice to use it as a \textit{black-box} approach.

A frequent critique brought against this class of algorithms has often to do with the fact that the reward structured is apparently ignored. After having studied the properties of the graph Laplacian in section \ref{chap:dynamics}, this claim however seems to dismissible. Indeed, the algorithm proposed here constructs a graph representation which does not seek approximate transition probabilities as edge weights. The resulting clustering can then be understood in terms of the dynamics of a random walk process over it; a different stochastic process than the one induced by the optimal policy. More precisely, bottlenecks captured in this way are related to those of the random walk Laplacian rather than the original Laplacian of the MDP. If such a substitution preserves no information on the reward structure, how can it then succeed in practice ?

A plausible reason might have to do with the kind of reinforcement learning domains commonly used in the field of HRL. In the four-rooms domains for example, doors appear as natural bottleneck states which happen to be in accordance with the optimal value function. The four-rooms domain is so ubiquitous that bottlenecks are often explained in terms of doors. Such references have the unfortunate effect of misguiding practitioners into conceiving bottlenecks as merely \textit{state features}, or \textit{saliencies}, rather than some effective \textit{coarsening} of the dynamics induced by an MDP.

The empirical results presented in this work departs from those obtained by previous authors in the fact that a bottleneck-like option construction algorithm was applied over the four-dimensional continuous state space of the Pinball domain. While subject to the same oversimplification concerning the reward structure, a useful decomposition was still obtained. By exploring the bottleneck concept under a new domain, practical obstacles were uncovered. 
It seems at this point that a useful research methodology for this problem should include not only results in a variety of reinforcement learning domains but also consider the generation of random MDPs with varying degrees of decomposability. The approach of \cite{Archibald1995} for the generating MDPs seems to be an appropriate fit as the mixing rate can be controlled as well as other structural properties of transition matrix. 

The recurrence of the bottleneck concept in the literature of the last decade should suffice to motivate the establishment of a proper theoretical research agenda.  The theory of Nearly Decomposable Markov Chains, and more generally the theory of MDP from operational research, seems to have been overlooked in reinforcement learning. The problem of options discovery should be first tackled using the theory of MDPs and extended to more of model-free and online setting compatible with the reinforcement learning framework. A promising research avenue might lie in some information theoretic approach similar to \cite{Deng2011} but subject to a rate distortion perspective of the value function as in \cite{Still2012}. Such results would not only benefit our understanding of temporal abstraction in RL but also provide insights to the line of work initiated by \cite{Botvinick2012} on the cognitive process involved in human subgoal discovery.
