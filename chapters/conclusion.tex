The bottleneck approach to options discovery can appear rebuttal because of the lack of proper theoretical foundations. This thesis attempted to motivate this family of heuristics by establishing a connection to spectral graph theory and relating it to the vast body of work on Nearly Decomposable Markov Chains (NCD) spanning across more than three decades. Furthermore, an algorithm based on these principles was proposed for discovering options in continuous state spaces. While many bottleneck-like approaches have succeeded in applying them to discrete domains, none had yet shown how to extend these ideas in a continuous domain such as Pinball. The empirical evaluation of the proposed algorithm showed its feasibility but also highlighted practical difficulties to use it as a \textit{black-box} approach.

A frequent critique brought against this class of algorithms has to do with the reward structured being apparently ignored. After having studied the properties of the graph Laplacian in section \ref{chap:dynamics}, this claim can be dismissed. It is true that algorithm proposed here constructs a graph representation which does not capture the underlying transitions probabilities of some MDP. The resulting clustering can then be understood in terms of the dynamics of a random walk process over it; a different stochastic process than the MC induced by some policy. Bottlenecks captured in this way are thus related to those of the random walk Laplacian rather than the original Laplacian of the MDP. If such a substitution does not preserve the reward structure, how can it then succeed in practice ?

A plausible reason might have to do with the kind of domains commonly chosen in HRL. In the four-rooms domains for example, doors appear as natural bottleneck states but also happen to be in structural elements in solution space. The four-rooms domain is so ubiquitous that bottlenecks are often explained in terms of doors. Such references have the unfortunate effect of misguiding practitioners into conceiving bottlenecks as merely \textit{state features}, or \textit{saliencies}, rather than as an effective \textit{coarsening} of the dynamics induced by an MDP.

The empirical results presented in this work depart from those obtained by previous authors in the fact that a bottleneck-like option construction algorithm for continuous state space was proposed and evaluated under a new HRL domain. While subject to the same oversimplification concerning the reward structure, a useful options decomposition was still obtained in the Pinball domain, but also uncovered practical obstacles. 

It seems at this point that a proper research methodology should be concerned with not only a more diverse set of domains but also consider the generation of random MDPs with varying degrees of decomposability. The approach of \cite{Archibald1995} for the generating MDPs seems to be an appropriate fit with the mixing rate being controllable as well as other structural properties of the transition matrix. 

The recurrence of the bottleneck concept in the literature should motivate the establishment of a research agenda to equip the notion with proper theoretical foundations. The theory of NCD Markov chains, and more generally that of MDPs from operations research, seems to have been overlooked in in HRL. The problem of options discovery should be first tackled using the theory of MDPs and extended to more of model-free and online setting compatible with the reinforcement learning framework. A promising research avenue might lie in some information theoretic approach similar to \cite{Deng2011} under a rate distortion perspective of the value function as initiated by \cite{Still2012}. Such results would not only benefit our understanding of temporal abstraction in RL but also provide insights to the line of work initiated by \cite{Botvinick2012} on the cognitive process involved in human subgoal discovery.
