It was shown in the previous chapter how the value function over large 
(possibly infinite)
state spaces can be represented compactly using function approximation. It would
then appear natural to harness a similar idea for exploiting \textit{temporal
regularities}. In the context of planning, reasoning at different
abstract time scales can drastically reduce the problem complexity.
The importance of temporal abstraction can be overlooked when considering \textit{simple} everyday
human tasks such as preparing coffee, going on a camping trip, changing a bicycle tire,
etc. A decomposition of these activities at the level of muscle twitches would however quickly expose 
the impossibility of carrying any form of learning at this level. A form of hierarchical organization must then exist from the micro to the macro-scales. Under the view adopted in this work, the task of preparing coffee would rather consist in a sequence of \textit{coarse} closed-loop policies executed sequentially towards the overall goal of having coffee in a cup. 

Many authors have tried to formalize this idea under the framework
of reinforcement learning: the \textsc{MAXQ} method of \cite{Dietterich1998}, the Hierarchical
Abstract Machines (HAM) of \cite{Parr1997} or Macro-Actions of \cite{Hauskrecht1998} for
example. These other approaches will not be covered in this thesis and the options framework of 
\cite{Sutton1999} will be adopted. For a survey of the early work on HRL, the
interested reader is referred to \cite{StolleThesis2004, Sutton1999}.

\section{Options Framework}
\begin{defn}
An \termidx{Option} is a triple $\langle \mathcal{I} \subseteq \mathcal{X},\;
\mathcal{\pi} : \mathcal{X} \times \mathcal{A} \to [0, 1],\; \beta : \mathcal{X} \to [0, 1]
\rangle$ consisting of an initiation set, a policy $\pi$ and a termination condition
$\beta$.
\end{defn}

An option $o \in \mathcal{O}$ can be executed in a \textit{call-and-return} fashion only if under the current state $x \in \mathcal{X}$, $x \in \mathcal{I}$. When $o$ is chosen, the agent acts according to the option policy $\pi$ until  $\beta$ dictates termination, at which point the agent must choose a new option again.

The initiation component is meant to facilitate the decision procedure by reducing the number of possible options to consider at a given state. Its presence could also had been motivated by  historical considerations to accommodate STRIPS-style planning \parencite{Fikes1972} more easily. The definition of the initiation set is sometimes neglected  by some authors, assuming that all options are available everywhere. The algorithm proposed in this thesis automatically curtails admissible states to form well-defined initiation set for every option. It should be noted finally that primitive actions can be seen as a special kind of options which are available everywhere, have a policy which always chooses the same action, and last exactly one step. 

The options framework is at the crossroad of  \mdps and semi-\mdps of
\cite{Bradtke1994}. It considers a base MDP overlaid with variable length courses of
action represented as options. It is shown in \cite{Sutton1999} (theorem 1) how an
MDP and a pre-defined set of options form a semi-\mdp. Most of the theory on SMDP can
thus be reused for options. As opposed to the usual theory of SMDP that treats
extended actions as indivisible and \textit{opaque} decision units, the options
framework allows to look at the structure \textit{within}. Furthermore, options can be either Markov or semi-Markov. In the fictitious coffee domain, waiting a predefined amount of time for the coffee to percolate would most likely be a semi-markov option unless the
state representation is changed to compensate for the non-markovianity.


\section{Bellman Equations for Options}

Let $\mu : \mathcal{X} \times \mathcal{O} \to [0, 1]$  be a general (Markov or semi-Markov) policy where $\mathcal{O}$ is any set of options. The value of $\mu$ is defined as

\begin{equation}
V^\mu(x) = \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k
V^\mu(X_{t+k})  \given \varepsilon(x, \mu, t) \right]
\label{eq:value-option-policy}
\end{equation}

where $k$ is a random variable denoting the duration of the first option selected by
$\mu$ and $\varepsilon(x, \mu, t)$ is the event that the policy over options $\mu$ is
executing at time $t$ in state $x$. Similarly, an \termidx{option-value function} for control can be defined as 
\begin{align}
Q^\mu(x, o) &= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} +
\gamma^k V^\mu(X_{t+k})  \given \varepsilon(x, o, t) \right] \notag \\
&= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \sum_{o^\prime
\mathcal{O}_x} \mu(X_{t+1}, o^\prime)Q^\mu(x, o^\prime) \given \varepsilon(x, o,
t) \right]
\label{eq:option-value-function}
\end{align}

$\mathcal{O}_x$ in the equation \ref{eq:option-value-function} is the subset of
options which can be initiated under state $x$. The optimality principle is carried over
to the following set of optimal Bellman equations

\begin{equation}
V_{\mathcal{O}}^\optimal(x) = \max_{o \in \mathcal{O}_x} \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k V_{\mathcal{O}_x}^\optimal(X_{t+k})  \given \varepsilon(x, o, t) \right]
\end{equation}

The \termidx{optimal option value function} is in turn

\begin{align}
Q_{\mathcal{O}}^\optimal(x, o) &= \expectation \left[ R_{t+1} + \dots +
\gamma^{k-1}R_{t+k} + \gamma^k V_{\mathcal{O}_x}^\optimal(X_{t+k})  \given
\varepsilon(x, o, t) \right] \notag \\
&= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k
\max_{o^\prime \in \mathcal{O}_{x_{t+k}}} Q_\mathcal{O}^\prime(o^\prime, X_{t+k}) 
\given \varepsilon(x, o, t) \right]
\label{eq:optimal-option-value-function}
\end{align}

If the optimal option value function is
available, the greedy policy is guaranteed to be optimal. A policy over options is
optimal if given a set of options, its underlying option value function is the optimal
value function, i.e. $V^{\mu_\mathcal{O}^\optimal}(x) = V_\mathcal{O}^\optimal(x)\; \forall \; x \in \mathcal{X}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Learning Behavior Policies
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Learning Behavior Policies}

Assuming that an options set $\mathcal{O}$ is specified, existing theory
of  semi-\mdps in continuous time can be
applied to solve the control problem \parencite{Puterman1994, Bradtke1994}. Under the SMDP model, decision are taken at certain \textit{epochs} spaced by random time intervals during which the system dynamics can be arbitrary. The optimal control problem can be solved by an
extension of \textsc{Q-Learning} to the SMDP case.

\begin{defn}{\textsc{SMDP Q-Learning}}
\begin{equation}
Q_{t+1}(x, o) \leftarrow Q_{t}(x, o) + \alpha \left[ \mathcal{R} + \gamma^k
\max_{o^\prime \in \mathcal{O}_x^\prime} Q_{t}(x^\prime, o^\prime) -
Q_t(x, o) \right]
\label{eq:smdp-qlearning}
\end{equation}
\end{defn}

The semantics of equation \ref{eq:smdp-qlearning} holds the under the event of
$\varepsilon(x, o, t)$ where option $o$ is executed for $k$ time steps with cumulative
discounted reward $\mathcal{R}$. Convergence results are provided in \cite{Parr1998}.

An identical algorithmic construct called \textsc{Macro Q-Learning} is also often
encountered in the literature. It stems from the work on Macro-actions by
\cite{Mcgovern1997} in an attempt to extend the deterministic macro-operators of
\cite{Korf1985} to closed-loop policies. As opposed to the more general \textsc{SMDP Q-Learning} algorithm of equation \ref{eq:smdp-qlearning}, \textsc{Macro Q-Learning} treats macro-actions and
and primitive actions separately. The SMDP update rule is applied for macro-actions
while primitive actions are updated using \ref{eq:qlearning-update}. Clearly, \textsc{SMDP Q-learning} encompasses \textsc{Macro Q-Learning} and can be expressed identically as

\begin{equation}
Q_{t+1}(x, o) \leftarrow Q_{t}(x, o) + \alpha \left[ \mathcal{R} + \gamma^k
\max_{o^\prime \in \mathcal{O}_x^\prime} Q_{t}(x^\prime, o^\prime) -
Q_t(x, o) \right]
\end{equation}

but where $\mathcal{O}$ necessarily contains all of the primitive actions wrapped as options.\textsc{Macro Q-Learning} is thus equivalent to \textsc{SMDP Q-Learning} where the
options set $\mathcal{O}$ is augmented with single-step options for each primitive
action.

\subsection{Intra-Option Learning}

In order for the \textsc{SMDP Q-Learning} algorithm in equation \ref{eq:smdp-qlearning} to provide a
good estimate of the optimal option value function, sufficient experience must be
obtained about every option. While executing an option to completion, fragments of experience of experience relevant for the current option could potentially be used to learn about other options. Since  it is possible to look \textit{within} an option, such an idea can be realized and leading to a sample efficient algorithm.

\textsc{Intra-Option Learning} is an off-policy algorithm which leverages the valuable
experience taking place \textit{within} options. While an option is executing, it can simultaneously
updates the value function estimate of all the consistent options which would have had
taken the same action under a given state. It is thus required that the options be
defined using deterministic policies so that this idea of consistency can be established. 

\begin{defn}{\textsc{Intra-Option} Value Learning}
\begin{align}
U(x, o) &= (1 - \beta(x))Q_t(x, o) + \beta(x) \max_{o^\prime \in \mathcal{O}} Q_t(x, o^\prime) \\
Q_{t+1}(x, o) &= Q_t(x, o) + \alpha \left[ R_{t+1} + \gamma U(x, o) - Q_t(x, o) \right]
\label{eq:intra-option-learning}
\end{align}
The update rule takes place under the event $\varepsilon(x, o, t)$ after each primitive
transition and is applied over every other consistent option for which $\pi(X_t) = A_t$.
\end{defn}

\textsc{Intra-Option Learning} being an off-policy method, it is susceptible to suffer from instabilities and divergence issues just as \textsc{Q-Learning}
with function approximation. GQ($\lambda)$ was introduced in \cite{Maei2010} as an
extension of \textsc{Q-Learning} but with special treatment against the aforementioned
problems. Although the theory for GQ($\lambda$) is provably correct, sufficient empirical
understanding is still lacking. Importance sampling
corrections similar as those used in \cite{Precup2001} for off-policy TD($\lambda$)
could be used as a substitute to GQ($\lambda$).

Finally, in order to learn an options policy $\pi$, it is customary to adopt the notion of 
\termidx{subgoals}. Subgoal options can be seen as optimizing some intrinsic notion of reward
consistent with the overall task objective. A \termidx{terminal subgoal value} assigns a
value for reaching the terminal states of an option determined by $\beta$. A subgoal
value function thus arises from the combination of the original underlying MDP plus the
overlaid discounted subgoal value function. This concept is often referred to as a 
\termidx{pseudo-reward} function: a term coined in \cite{Dietterich1999}
about the \textsc{MAXQ} framework.	

