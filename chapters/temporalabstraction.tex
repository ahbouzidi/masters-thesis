
It has been shown in the previous section how the value function over large 
(possibly infinite)
state spaces can be represented compactly using function approximation. It would
then appear natural to harness a similar idea for exploiting \textit{temporal
regularities} of the problem at hand. In the context of planning, reasoning at different
abstract time scales can drastically reduce the time complexity for finding a solution. 
The importance of temporal abstraction can be appreciated easily under everyday
scenarios such as preparing coffee, going on a camping trip, changing a bicycle tire,
etc. When preparing an Espresso, a Barista might need to make use of her
\textsc{turn-on-the-espresso-maker} ability, followed by \textsc{grind-coffee-beans},
\textsc{tamp-the-espresso}, \textsc{lock-the-portafilter}, and \textsc{pull-a-shot}.
Capitalized expressions where used in this last example to highlight the fact that these
\textit{actions} last arbitrary amount of time and should rather be seen as
\textit{closed-loop} policies. Although the tasks of preparing coffee might seem trivial
even for the inexperienced, it does rely on a complicated sequence of \textit{atomic}
decisions procedures which would be difficult to obtain with absolutely no prior
knowledge. 

Many formalisms have been proposed to cast this problem efficiently in the framework
of reinforcement learning: the MAXQ method \cite{Dietterich1998} and Hierarchical
Abstract Machines (HAM) \cite{Parr1997} or Macro-Actions \cite{Hauskrecht1998} for
example. These other approaches will not be covered as the options framework of 
\cite{Sutton1999} will be adopted in this work. For a survey of the earlier attempts, the
interested reader is referred to \cite{StolleThesis2004, Sutton1999}.

\section{Options Framework}
\begin{defn}
An \termidx{Option} is a triple $\langle \mathcal{I} \subseteq \mathcal{X},\;
\mathcal{\pi} : \mathcal{X} \to \Omega(\mathcal{A}),\; \beta : \mathcal{X} \to [0, 1]
\rangle$ consisting of an initiation set, a policy $\pi$ and a termination condition
$\beta$.
\end{defn}

Given a state $x \in \mathcal{X}$ and option $o \in \mathcal{O}$, $o$ can be
executed in a \textit{call-and-return} fashion up to termination, dictated by $\beta$,
only if $x \in \mathcal{I}$. The initiation component is meant to facilitate decision
taking by reducing the size of possible options to consider for any given state. Its
presence also allows easier bridging with the previous work in STRIPS-style
\cite{Fikes1972} planning. In many efforts to use the options framework \todo{cite an
example}, the definition of this component is neglected and it is assumed that options
are available everywhere. The present work however proposes a method to alleviate
this difficulty by automatically curtailing the set of admissible states.

The options framework is at the crossroad of regular \mdps and semi-\mdps
\cite{Bradtke1994}. It considers a base \mdp overlaid with variable length course of
action represented as options. It is shown in \cite{Sutton1999} (theorem 1) how an
MDP and pre-defined set of options form a semi-\mdp and can thus reuse most of the
known theory on the subject. As opposed to the general theory of SMDP that treats
extended actions as indivisible and \textit{opaque} decision units, the options
framework also allows to look at the structure \textit{within}. It can also accommodate
both Markov options and semi-Markov ones. In the example coffee domain presented
above, \textsc{pull-a-shot} would be an option defined by a \textit{timeout} and not
only by the current state. It would then most likely be semi-markov option unless the
state representation is changed to support the non-markovianity.

With this formulation, primitive actions can be seen as a special kind of options which
is available everywhere, has a policy always choosing the same action, and lasting
exactly one step. 

\section{Bellman Equations for Options}

Let $\mu : \mathcal{X} \times \mathcal{O} \to [0, 1]$  be a general (Markov or semi
Markov) policy where $\mathcal{O}$ is a set of options (containing primitive actions or
not). The value of $\mu$ is defined as

\begin{equation}
V^\mu(x) = \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k
V^\mu(X_{t+k})  \given \varepsilon(x, \mu, t) \right]
\label{eq:value-option-policy}
\end{equation}

where $k$ is a random variable denoting the duration of the first option selected by
$\mu$ and $\varepsilon(x, \mu, t)$ is the event that the option policy $\mu$ is
executing at time $t$ in state $x$. Once $\mu$  terminates, it presumably selects  a
different option and the process is repeated until the episode terminates. Similarly, we
can define an \termidx{option value function} for control as 
\begin{align}
Q^\mu(x, o) &= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} +
\gamma^k V^\mu(X_{t+k})  \given \varepsilon(x, o, t) \right] \notag \\
&= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \sum_{o^\prime
\mathcal{O}_x} \mu(X_{t+1}, o^\prime)Q^\mu(x, o^\prime) \given \varepsilon(x, o,
t) \right]
\label{eq:option-value-function}
\end{align}

$\mathcal{O}_x$ in the equation \ref{eq:option-value-function} is the subset of
options which can be initiated under state $x$. The optimality principle is carried over
the following set of optimal Bellman equations

\begin{equation}
V_{\mathcal{O}}^\optimal(x) = \max_{o \in \mathcal{O}_x} \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k V_{\mathcal{O}_x}^\optimal(X_{t+k})  \given \varepsilon(x, o, t) \right]
\end{equation}

The \termidx{optimal option value function} is in turn

\begin{align}
Q_{\mathcal{O}}^\optimal(x, o) &= \expectation \left[ R_{t+1} + \dots +
\gamma^{k-1}R_{t+k} + \gamma^k V_{\mathcal{O}_x}^\optimal(X_{t+k})  \given
\varepsilon(x, o, t) \right] \notag \\
&= \expectation \left[ R_{t+1} + \dots + \gamma^{k-1}R_{t+k} + \gamma^k
\max_{o^\prime \in \mathcal{O}_{x_{t+k}}} Q_\mathcal{O}^\prime(o^\prime, X_{t+k}) 
\given \varepsilon(x, o, t) \right]
\label{eq:optimal-option-value-function}
\end{align}

If the optimal option value function (eq \ref{eq:optimal-option-value-function}) is
available, the greedy policy is guaranteed to be optimal. A policy over options is
optimal if given a set of options, its underlying option value function is the optimal
value function, i.e. $V^{\mu_\mathcal{O}^\optimal}(x) = V_\mathcal{O}^\optimal(x)\;
\forall x \in \mathcal{X}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Learning Behavior Policies
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Learning Behavior Policies}

Assuming that a options set $\mathcal{O}$ is specified, existing theory
\cite{Puterman1994, Bradtke1994} from semi-\mdps in continuous time can be
applied to solve the control problem. The SMDP model considers decision making at
\textit{decision epochs} spaced by random time intervals during which the system
dynamics are of no interest. The optimal control problem can be solved by an
extension of Q-learning to the SMDP case.

\begin{defn}{SMDP Q-Learning}
\begin{equation}
Q_{t+1}(x, o) \leftarrow Q_{t}(x, o) + \alpha \left[ \mathcal{R} + \gamma^k
\max_{o^\prime \in \mathcal{O}_x^\prime} Q_{t}(x^\prime, o^\prime) -
Q_t(x, o) \right]
\label{eq:smdp-qlearning}
\end{equation}
\end{defn}

The semantics of equation \ref{eq:smdp-qlearning} holds the under the event of
$\varepsilon(x, o, t)$ where option $o$ is executed for $k$ time steps with cumulative
discounted reward $\mathcal{R}$. Convergence results are provided in \cite{Parr1998}.

An identical algorithmic construct called \termidx{Macro Q-learning} is also often
encountered in the literature. It comes from the work on Macro-actions of
\cite{Mcgovern1997} as an extension of the deterministic macro-operators
\cite{Korf1985} to closed-loop policies. It might have been that the options framework
had not yet beaten the pathways rigorously enough that the authors of
\cite{Mcgovern1997} decided use a different terminology. 

As opposed to the more general SMDP Q-Learning algorithm of equation
\ref{eq:smdp-qlearning}, Macro Q-Learning treats macro-actions (options in this case)
and primitive actions separately: an SMDP update rule is applied for macro-actions
while primitive actions are updated using \ref{eq:qlearning-update}. Clearly, SMDP Q-learning encompasses Macro Q-Learning and can be expressed identically as

\begin{equation}
Q_{t+1}(x, o) \leftarrow Q_{t}(x, o) + \alpha \left[ \mathcal{R} + \gamma^k
\max_{o^\prime \in \mathcal{O}_x^\prime} Q_{t}(x^\prime, o^\prime) -
Q_t(x, o) \right]
\end{equation}

but with the only constraint that the set of options $\mathcal{O}$ must contain
primitives actions wrapped as options.

\textsc{Macro Q-Learning} is thus equivalent to \textsc{SMDP Q-Learning} where the
options set $\mathcal{O}$ is augmented with single-step options for each primitive
action.

\subsection{Intra-Option Learning}

In order for the Q-Learning algorithm in equation \ref{eq:smdp-qlearning} to provide a
good estimate of the optimal option value function, sufficient experience must be
obtained about the option set at every states. While executing an option to completion,
valuable fragments of experience are completely ignored rendering the SMDP methods
highly sample-inefficient.

Intra-option learning is an off-policy algorithm which tries to leverage the valuable
content \textit{within} the options. While an option is executing, it simultaneously
updates the value function estimate of all the consistent options which would have had
chosen the same action under a given state. It is thus required that the options be
defined by deterministic policies so that this idea of consistency can be established. 
By virtue of being an off-policy method, intra-option learning also has the potential of
learning about nonterminating options. 

\begin{defn}{Intra-option value learning}
\begin{align}
U(x, o) &= (1 - \beta(x))Q_t(x, o) + \beta(x) \max_{o^\prime \in \mathcal{O}} Q_t(x, o^\prime) \\
Q_{t+1}(x, o) &= Q_t(x, o) + \alpha \left[ R_{t+1} + \gamma U(x, o) - Q_t(x, o) \right]
\label{eq:intra-option-learning}
\end{align}
The update rule takes place under the event $\varphi(x, o, t)$ after each primitive
transition and is applied over all other consistent options for which $\pi(X_t) = A_t$.
\end{defn}

Intra-Option learning being an off-policy method, it is susceptible to suffer from i
nstabilities and divergence issues in a similar way to the classical Q-Learning algorithm
with function approximation. GQ($\lambda)$ was introduced in \cite{Maei2010} as an
extension of Q-Learning but with special treatment against the aforementioned
problems. While the theory for GQ($\lambda$) is theoretically correct, empirical
evidences remain to be gathered to better understand its behavior. Importance sampling
corrections similar as those used in \cite{Precup2001} for off-policy TD($\lambda$)
could then be used as a substitute. 

Finally, in order to learn an option's policy $\pi$, it is customary to adopt the notion of 
\termidx{subgoal}. Options can be seen as optimizing some intrinsic notion of reward
consistent with the overall task's objective. A \termidx{terminal subgoal value} assigns a
value for reaching the terminal states of an option as determined by $\beta$. A subgoal
value function thus arises from the combination of the original underlying MDP plus the
overlaid discounted subgoal value function. This concept is often referred to as a 
\termidx{pseudo-reward} function: a term originally coined in \cite{Dietterich1999}
about the MAXQ framework.	

