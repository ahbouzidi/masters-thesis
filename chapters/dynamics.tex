Graph theory has deep ramifications in many fields. The theory of \mdps in one such example where taking a graph perspective provides a natural language for describing and understanding fundamental properties. As shown in this section, spectral graph theory provides powerful
tools for studying the dynamical properties of Markov chains.  Some of the early
applications of this connection can be traced back to the work of \cite{SimonAndo1961, DonathHoffman1973, Fiedler1973}.

Spectral graph theory will provide the theoretical justification for the heuristic approach
to options discovery algorithm proposed in this work. This presentation should be seen
as an initial foray towards a better theoretical understanding of the bottleneck concept. Random walk  processes on arbitrary graphs
$G$ will first be introduced, followed by a characterization of the properties of the graph Laplacian and its definition for MDPs. It will then be shown how
graph partitioning can be achieved on the basis of the dynamics induced by the random
walk process. Finally, it will be argued that the mixing time property of the MDP-induced
graph is relevant to the problem of options discovery and can be related to spectral graph partitioning.

\section{Random Walk}
\label{sec:random-walk}
The notion of random walk is necessary in the presentation of some important spectral
properties related to the graph Laplacian in section \ref{sec:laplacian}. Given a graph
$G$, we can generate a random sequence by picking a vertex uniformly at
random in the neighborhood of an initial vertex and carrying on the procedure over to the
newly obtained vertex. This procedure corresponds to the realization of a type of
random process called \textit{random walk}. More precisely, we define a \textbf{simple
random walk} on a graph $G = (V, E)$ as a Markov Chain with state space $V$ and for
which the transition matrix is given by:

\begin{equation}
P_{ij} = \begin{cases} 
\frac{w_{ij}}{d(i)} \hspace{2mm} \mbox{if $i \sim j$} \\
0 \hspace{2mm} \mbox{otherwise}
\end{cases}
\end{equation}

$w_{ij}$ above is an edge weight and $d(i)$ is the degree of $i$.

Using matrix notation, the stochastic matrix of the random walk can be written as
\begin{equation}
\mathbf{P} = \mathbf{D}^{-1}\mathbf{W}
\label{eq:random-walk}
\end{equation}

If each vertex of $G$ has the same degree, in which case $G$ is said to be \textit{d
regular}, the stationary distribution of the random walk is the uniform distribution. 

\section{Graph Laplacian}
\label{sec:laplacian}

The combinatorial (unnormalized) graph Laplacian of a weighted graph $G$ is defined
as
\begin{equation}
\mathbf{L} = \mathbf{D} - \mathbf{W}
\label{eq:unnormalized-laplacian}
\end{equation}

and corresponds to the symmetric matrix
\begin{equation}
L(u,v) = 
\begin{cases}
d_v & \mbox{if $u = v$,} \\
-1 & \text{if $u$ and $v$ are adjacent,} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

Where $\mathbf{D} = \text{diag}(d_1, d_2, \dots, d_n)$ and $\mathbf{W}$ are the
diagonal degree and weight (adjacency) matrices for $G$ respectively.

Because of its easier stochastic interpretation, the \textit{normalized} version of the graph Laplacian is often preferred. The interested reader is referred to the highly influential monograph by \cite{Chung1997} which offers an in-depth treatment on the topic.

The normalized graph Laplacian is defined as
\begin{equation}
\laplacian = \mathbf{D}^{-1/2} \mathbf{L} \mathbf{D}^{-1/2} = \mathbf{I} -
\mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2}
\label{eq:normalized-laplacian}
\end{equation}	

where $\mathbf{L}$ is the unnormalized Laplacian defined in equation
\ref{eq:unnormalized-laplacian}.  The elements of the matrix $\laplacian$ then become

\begin{align}
\laplacian(u,v) = 
\begin{cases}
1 & \text{if $u = v$ and $d_v \neq 0$,} \\
-\frac{1}{\sqrt{d_u d_v}} & \text{if $u$ and $v$ are adjacent,}\\
0 & \text{otherwise}
\end{cases}
\end{align}

Clearly, $\laplacian$ is also symmetric since
\begin{align}
\laplacian^\transpose = \left(\mathbf{D}^{-1/2} \right)^\transpose \mathbf{L}^\transpose \left( \mathbf{D}^{-1/2} \right)^\transpose = \laplacian
\end{align}

The spectral theorem for symmetric matrices \parencite{Watkins2010} also ensures that
$\laplacian$ only has real eigenvalues. Furthermore, eigenvectors corresponding to
distinct eigenvalues must be orthogonal.

The normalized Laplacian is closely related to the transition matrix $\mathbf{P}$ of the random
walk process presented in section \ref{sec:random-walk}. It can be seen through the
identity 

\begin{equation}
\mathbf{I} - \laplacian = \mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2}
\end{equation}

that

\begin{align}
P &= \mathbf{D}^{-1/2} \left( \mathbf{I} - \laplacian \right) \mathbf{D}^{1/2} \label{eq:rw-similarity} \\
 &= \mathbf{D}^{-1/2} \left( \mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2} \right) \mathbf{D}^{1/2} \notag \\
 &= \mathbf{D}^{-1} \mathbf{W} \notag
\end{align}

and $\mathbf{P}$ must be similar (in the
linear algebra sense) to $\mathbf{I} - \laplacian$. It follows that they also share
the same eigenvalues and the right eigenvectors of $\mathbf{I} -\laplacian$ can be
obtained from the eigenvectors of $\mathbf{P}$ by multiplying them by $\mathbf{D}^{1/2}$

\begin{align}
P\mathbf{v} &= \lambda \mathbf{v} \notag \\
\mathbf{D}^{-1/2} \left( \mathbf{I} - \laplacian \right) \mathbf{D}^{1/2} \mathbf{v}  &= \lambda \mathbf{v} \notag \\
\left( \mathbf{I} - \laplacian \right) \mathbf{D}^{1/2} \mathbf{v}  &= \lambda \mathbf{D}^{1/2} \mathbf{v} \notag \\
\left( \mathbf{I} - \laplacian \right) \mathbf{u} &= \lambda \mathbf{u} \\
\mathbf{u} &= \mathbf{D}^{1/2} \mathbf{v}
\label{eq:similarity-rw}
\end{align}

Because of this connection to random walks, some authors have also defined the \termidx{normalized random walk Laplacian} as

\begin{equation}
\laplacian_{rw} = \mathbf{D}^{-1} L = \mathbf{I} - \mathbf{D}^{-1} \mathbf{W}
\end{equation}

% Difference operator
The Laplacian can then be thought as an \termidx{operator} acting upon the space of
functions $\mathcal{F} : V \to \mathbb{R}$, with $V$ denoting the vertex set.
Expressing such functions as vector $f \in \mathbb{R}^{|V|}$, the normalized Laplacian
can be shown to satisfy
 
\begin{align}
\mathcal{L}f(u) &= f(u) - \sum_{\substack{ v \\ u \sim v}} \frac{f(v)}{\sqrt{d_ud_v}} \notag \\
 &= \frac{1}{\sqrt{d_u}} \left( \sum_{\substack{ v \\ u \sim v}} \frac{f(u)}{\sqrt{d_u}} - \frac{f(v)}{\sqrt{d_v}} \right)
\label{eq:difference-operator}
\end{align}

The same could also be said about the normalized Laplacian generalized over weighted
graphs, in which case
\begin{equation}
\laplacian f (u) = \frac{1}{\sqrt{d_u}} \left( \sum_{\substack{ v \\ u \sim v}} \frac{f(u)}{\sqrt{d_u}} - \frac{f(v)}{\sqrt{d_v}} \right) w_{uv}
\end{equation}

where $w_{uv}$ denotes the edge weight between adjacent vertices $u$ and $v$. 

An $n \times  n$ symmetric matrix $A$ is said to be positive semidefinite (PSD) if for any vector $x \in \mathbb{R}^n, x^\transpose A x \geq 0$.  The normalized Laplacian admits a quadratic form from which the PSD property becomes clear
\begin{align}
f^\transpose \laplacian f &= f^\transpose \left( \mathbf{I} - \mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2} \right) f \notag \\
&= f^\transpose f - f^\transpose \mathbf{D}^{-1/2} \mathbf{W} \mathbf{D}^{-1/2} f \notag \notag \\
&= \sum_{i} f_i^2 - \sum_{i} \frac{f_i}{\sqrt{d_i}} \sum_j w_{ij} \frac{f_j}{\sqrt{d_j}} \notag \\
&= \frac{1}{2} \left( \sum_i f_i^2 - 2 \sum_{i,j} \frac{f_i}{\sqrt{d_i}} \frac{f_j}{\sqrt{d_j}} w_{ij} + \sum_j f_j^2 \right) \notag \\
&= \frac{1}{2} \sum_{ij} \left(  f_i^2 -2\frac{f_i}{\sqrt{d_i}}\frac{f_j}{\sqrt{d_j}} w_{ij}  + f_j^2 \right) \notag \\
&=  \frac{1}{2} \sum_{ij} w_{ij} \left(  \frac{f_i^2}{d_i} -2\frac{f_i}{\sqrt{d_i}}\frac{f_j}{\sqrt{d_j}} + \frac{f_j^2}{d_j} \right) \hspace{5mm} \text{by defn. $d_i = \sum_j w_{ij}$} \notag \\
&= \frac{1}{2} \sum_{ij} w_{ij} \left( \frac{f_i}{\sqrt{d_i}} - \frac{f_j}{\sqrt{d_j}} \right)^2
\label{eq:quadratic-form}
\end{align}

Clearly, $f^\transpose \laplacian f \geq 0$, and $\laplacian$ must be positive
semidefinite. Under the definition of the combinatorial Laplacian, a similar quadratic
form is induced \parencite{Mohar91}

\begin{equation}
f^\transpose L f = \sum_{ij} w_{ij} (f_i - f_j)^2
\end{equation}

The quadratic form can be interpreted as a measure of smoothness of a  function over
the graph. Intuitively, if $f$ is smooth over each pair of vertices, $f^\transpose
\laplacian f$ must be small. On the other hand, if $f$ locally varies is a more drastic
manner, this quantity should grow larger. The weighting factor in equation
\ref{eq:quadratic-form} also helps to understand how local density might influence this
measure of smoothness. In regions of higher density, the proximity or similarity
encoded through $w_{ij}$ is higher; hence the greater amplification of the $(f_i - f_j)^2$
term. This property is exploited in semi-supervised learning to encode the so-called \termidx{cluster assumption} and force the decision boundary to lie in low density regions \cite{Chapelle2002}.

% induce regular norm , projection on eigenspace of laplacian produce smoothest
%approximation, if piecewise smoot should be compressible in some basis
% Relate quadratic form to smoothness and relation to continuous Laplace-Beltrami
% Dilatation of operators and invariant subspaces. motivate approach: constant
%dynamics. compressible in some basis

\section{Laplacian of MDPs}
Consider the \mrp induced by fixing a policy $\pi$ for an MDP and let $\mathbf{P}^\pi$
denote the corresponding transition probability matrix. As shown earlier, a kind of
Laplacian closely related to the normalized Laplacian can be obtained from the transition
matrix of a random walk process. Clearly, $\mathbf{P}^\pi$ also defines a stochastic
matrix encoding the dynamics induced by $\pi$ in the environment.The notion of graph Laplacian can
naturally be extended for MDPs. 

\begin{defn}
Given a transition probability matrix $\mathbf{P}^\pi$ underlying an MDP for a fixed
policy, the \termidx{Laplacian of an MDP} is
\begin{equation}
\laplacian^\pi = \mathbf{I} - \mathbf{P}^\pi
\label{eq:mdp-graph-laplacian}
\end{equation}
\end{defn} 

The Laplacian view of \mdps was adopted by a number of authors \parencite{Lamond1989,
Puterman1994, Filar2007} for the study of their chain structure. While the term 
Laplacian was not explicitly used, their analysis relied on the
properties of the so-called \textit{Drazin} inverse \parencite{Drazin1958} of $\mathbf{I} -
\mathbf{P}^\pi$. More recently, the work on Proto-Value Functions (PVF) \parencite{Mahadevan2009}
also exploited the concept and terminology of the graph Laplacian for the purpose of representation
and control.

An obstacle to studying the spectrum of $\laplacian^\pi$ occurs when the induced Markov chain is not
reversible. Furthermore, the assumption that $\mathbf{P}^\pi$ is available from an
existing MDP is problematic. Under the reinforcement learning framework, the transition
matrix is generally unknown and it is necessary to resort to sampling-based methods. In
certain scenarios, the control problem is itself very hard to solve. Therefore, requiring
the knowledge of $\laplacian^\pi$ to facilitate the problem would be nothing other than a
chicken-and-egg scenario. In the approach proposed in this thesis, the random walk
Laplacian is considered as a practical replacement. Such a substitution
is also adopted in \cite{Mahadevan2009} for the same reasons.

\section{Graph Partitioning}
\label{sec:graph-partitioning}
Through the study of the set of eigenvalues associated with the graph Laplacian, some remarkable results on the subgraph structures can be obtained. The set of problems concerning the optimality of graph cuts with regard to the size of their components is commonly referred to as \termidx{isoperimetric problems}. Edge cuts are of particular interest because they relate to spectrum of $\laplacian$ through the so-called Cheeger constant. 

% Define boundary
Let $A, B \subseteq V$ be subsets of the vertices of a graph $G$ and $E(A, B)$ denote the set of edges having one end in $A$ and the other in $B$. It is also convenient to describe the set of edges having only one end point in $S \subseteq V$ as the \termidx{boundary} \parencite{Chung1994}, \termidx{edge boundary} \parencite{Chung1997} or \termidx{coboundary} \parencite{Mohar91} of $S$, that is
\begin{equation}
\partial S = \{ \{u, v\} \in E : u \in S \text{ and } v \not \in S \}
\end{equation}

% Define conductance
The \termidx{volume} for $S$ also corresponds to the sum of the degrees of its vertices
\begin{equation}
\vol(S) = \sum_{x \in S} d_x
\end{equation}

The \termidx{conductance} of $G$, also known as the \termidx{Cheeger constant} \parencite{Chung1997}, is defined as

\begin{equation}
h_G = \min_S h_G(S) \hspace{2.5mm} \text{where} \hspace{2.5mm} h_G(S) = \frac{ |E(S, \bar{S}) |}{\min \left( \vol(S), \vol(\bar{S}) \right)}
\label{eq:cheeger-constant}
\end{equation}

The \termidx{Cheeger inequality} relates it to the spectrum of $\laplacian$ as follow \parencite{Chung1997}
\begin{lem}
\begin{equation}
2h_g \geq \lambda_1 > \frac{h_g^2}{2}
\end{equation}
\end{lem}

The more general graph partitioning problem can be better understood first in terms of
the graph bipartitioning one. A natural attempt at solving this problem would be to
express it under a \textsc{min-cut} formulation which could be solved in time
$\mathcal{O}(n^3)$ using the Ford-Fulkerson algorithm \cite{FordFulkerson1956}. The
resulting partitions however tend to be highly imbalanced and in certain cases can
degenerate to singleton partitions \cite{Hagen1992}. A related problem to consider is
then the \textsc{minimum-width bisection} which attempts at finding a minimum cut
with partitions of equal size. Unfortunately, the problem was shown to be
$\mathcal{NP}$-complete by \cite{Garey1976}. A natural response is then to relax the
optimization criterion in such a way to maintain balanced partitions without requiring
them to be of same size. The \termidx{ratio-cut} criterion is defined as \cite{Chuen1989, Chuen1991, Hagen1992} 

\begin{equation}
RCut(S) = \min_S \frac{|E(S, \bar{S})|}{|S| \cdot |S'|}
\end{equation}

and is very much related to the modified Cheeger constant \ref{eq:cheeger-constant}
also called the \termidx{isoperimetric number} \footnote{Although some authors seems
to make no distinction between the terms \cite{ShiMalik2000, Levin2008},
\cite{Chung1997} shows in chapter 2 how they relate differently to the eigenvalues of
$\laplacian$.}. Once again,  finding the minimum ratio cut is also $\mathcal{NP}$-
complete but a spectral relaxation based on the combinatorial Laplacian (see equation
\ref{eq:unnormalized-laplacian}) is proposed in \cite{Hagen1992}.

A similar measure called the \termidx{normalized cut} was made popular in \cite{ShiMalik2000} and considers

\begin{equation}
NCut(S) = \frac{|E(S, \bar{S})|}{\vol(S)} + \frac{|E(S, \bar{S})|}{\vol(\bar{S})}
\label{eq:ncut}
\end{equation}

Just as the ratio cut, the normalized cut also falls short of an exact solution which is not
$\mathcal{NP}$-complete \cite{ShiMalik2000}. The \textit{normalized} qualification in
the name \textit{normalized cut} can however seem misleading as it might imply that
the normalized Laplacian as defined in equation \ref{eq:normalized-laplacian} is directly
exploited. Indeed, the optimization problem is first set \cite{Luxburg2007} as
\begin{align}
\min_A \mathbf{f}^\transpose \left( \mathbf{D} - \mathbf{W} \right) \mathbf{f} \\
\text{subject to} \hspace{2.5mm} \mathbf{D} \mathbf{f} \perp \indicator, \hspace{2.5mm}  \mathbf{f}^\transpose \mathbf{D} \mathbf{f} = \vol(V) \notag \\
f_i = \begin{cases}
\sqrt{\frac{\vol(A)}{\vol(\bar{A})}} & \text{if $v_i \in A$} \\
-\sqrt{\frac{\vol(\bar{A})}{\vol(A)}} & \text{if $v_i \in \bar{A}$} 
\end{cases} \notag
\end{align}

where $\mathbf{D} - \mathbf{W}$ can be recognized to be the combinatorial Laplacian
as defined in equation \ref{eq:unnormalized-laplacian}.  By relaxing $\mathbf{f}$ to 
values in $\mathbb{R}$ instead of discrete ones and setting $g = \mathbf{D}^{1/2}
\mathbf{f}$, the problem becomes

\begin{align}
\min_{g \in \mathbb{R}^{|V|}} \mathbf{g}^\transpose\mathbf{D}^{-1/2}\left( \mathbf{D} - \mathbf{W} \right) \mathbf{D}^{-1/2} g \\
\text{subject to} \hspace{2.5mm} \mathbf{g} \perp D^{1/2} \indicator, \hspace{2.5mm}  \| \mathbf{g} \|^2 = \vol(V) \notag \\
\end{align}

Under this form, the normalized Laplacian $\laplacian$ (equation 
\ref{eq:normalized-laplacian}) finally appears. By the Rayleigh-Ritz theorem, this
quantity is minimized when $\mathbf{g}$ equals the second eigenvector of $\laplacian$
\cite{Chung1997}. From the real vector $g$, the partition membership can be recovered
by thresholding its components. The problem of multi-partitioning or \textit{k-way} partitioning can be solved by applying the spectral bipartitioning approach described above in a top-down recursive manner in each partition.

The normalized cut criterion not only seems to provide better empirical results, but also
provides an intuitive interpretation in terms of random walks. Recall the connection
drawn from $\mathbf{I} - \laplacian$ to the normalized random walk Laplacian $P$ in
equation \ref{eq:rw-similarity} where they were shown to be similar. Therefore, if
$\lambda$ is an eigenvalue of $\laplacian$, then $1 - \lambda$ must be an eigenvector
of $P$. 

It is shown in \cite{Shi2001} how this can be further exploited to express the Ncut
criterion in terms of probability distributions from a random walk process over the
graph. The stationary distribution of a random walk on a connected, non-bipartite graph
can be shown \cite{Levin2008} to be equal to
\begin{equation}
\pi_i = \frac{d_i}{\vol(V)}
\end{equation}

The probability of transitioning from a partition $A$ to $B$ if at the stationary distribution the initial state is in $A$, is written as
\begin{equation}
P(A \to B | A) = \frac{\sum_{x \in A, y \in B} \pi(x) P(x, y)}{\pi(S)}
\label{eq:rw-pr}
\end{equation}

which, for a random walks, boils down to 
\begin{equation}
P(A \to B | A) = \frac{\sum_{x \in A} d_x}{\vol(A)}
\end{equation}

From equation \ref{eq:rw-pr} and \ref{eq:ncut}, it is then easy to see that
\begin{equation}
NCut(S) = P(S \to \bar{S} | S) + P(\bar{S} \to S | \bar{S})
\end{equation}

Interestingly, \cite{Levin2008} calls this measure in \ref{eq:rw-pr} the
\termidx{bottleneck ratio} and defines the \termidx{bottleneck ratio for a Markov chain}
as 
\begin{equation}
\Phi_\star = \min_{S\; : \;\pi(S) \leq \frac{1}{2}} P(S \to \bar{S} | S)
\end{equation}
which corresponds to the Cheeger constant presented in equation 
\ref{eq:cheeger-constant}. Such a quantity happened to have been studied intensively as
a way to bound the \termidx{mixing time} of Markov chains through the notion of
\termidx{conductance} (a term referring to the same Cheeger constant or the bottleneck
ratio presented here). The notion of mixing time can described intuitively as the number
of steps necessary for Markov chain to reach its stationary distribution. Since it is the asymptotic behavior of some Markov chain that is involved, the number of steps involved would not be finite. It is therefore customary to define the notion of mixing time as being \textit{close to} the steady-state distribution \cite{Jerrum1988}.

\begin{figure}
\centering 
\input{fig/bottleneck}
\caption{Some arbitrary graph presenting a bottleneck. The mixing time can be bounded using the bottleneck ratio of a Markov chain.}
\label{fig:bottleneck}
\end{figure}

The smaller the conductance is, the longer the chain is expected to \textit{mix}.
Furthermore, the Markov chain would spend more time within a given partition of the
graph and with lower probability, transition to different adjacent partition (see figure \ref{fig:bottleneck}). It can then be
understood that low conductance would also result in lower probabilities of inter-partition
jumps.

It has been presented above how the second smallest eigenvector of $\laplacian$
minimizes the relaxed $NCut$ criterion and can also bound the Cheeger inequality
\ref{eq:cheeger-constant}. Therefore, it comes as no suprise to see it appearing in many
bounds on the mixing time under the form of the \termidx{spectral gap} $1 - \lambda_1$ The interested reader is referred to \cite{Jerrum1988, Lovasz1996,
Levin2008} for some of these results. 

\section{Relevance for Options Discovery}
\label{sec:ndmc}
The definition of what makes for \textit{interesting} options has remained vaguely
defined throughout the vast body of work on bottleneck-like approaches in
reinforcement learning. The intuition developed in this chapter might provide further
development on this question. After having distilled the essence of the bottleneck
concept through the graph Laplacian, it is also easier to obtain an historical perspective
along the work on \textit{nearly completely decomposable Markov chains}

It has been shown in this chapter how the optimal bipartitioning problem relates to the
cheeger constant or bottleneck ratio which in turn, provide bounds on the mixing time
of Markov chains. It might seem then that bottlenecks allow for the discovery of fast
mixing regions of the state space. In the planning problem, fast mixing might translate
into computational speedup, making policies \textit{simpler} to compute in those
regions. Efficient random sampling techniques of large state spaces are also often
concerned fast mixing time \cite{Boyd2004}. In reinforcement learning, the same
problem would be translated under the question of designing smart \textit{exploration}
strategies. Optimal partitioning of the MDP-induced graph Laplacian could thus help
identifying regions of fast mixing time and create options tailored for exploration.

It also seems that good options are those which are intrinsically \textit{simple}, acting as
elementary building building blocks of increasingly complex hierarchies. Traces of this
ideal of simplicity can be observed under different forms. For example, its is
known from\cite{Shi2001}, that the stochastic matrix $P$ has piecewise constant
eigenvectors with respect to the $NCut$ partitioning. This suggests that optimal
partitions can be described more succinctly (simply). The authors also note the
potential connection with the notion of \termidx{lumpability} introduced in
\cite{Kemeny1976}. Lumpability essentially consists in aggregating states of similar
dynamics so as to simplifying Markov chains when confronted with the curse of
dimensionality. 

The idea of matrix decomposition was initiated in \cite{AndoFisher1963} from
economics for the study of linear dynamical systems with \textit{nearly decomposable}
structures. The class of Markov chains with strong intra-cluster interactions and weak
inter-cluster ones has then been studied under the name of \textit{nearly completely
decomposable Markov chains} (NDMC) \cite{Gaitsgori1975} and covered by
\cite{Courtois1977} in a book on queuing theory. The control problem for Markov chains
with such structure was considered in \cite{Teneketzis1980, Delebecque1981,
Phillips1981, Coderch1983} building upon the Courtois' pertubational aggregation
decomposition. This approach treats the \textit{slower} inter-cluster transitions as
matrix perturbations. Similar ideas are also brought closer to the framework of \mdps
with \cite{Haviv1985, Aldhaheri1991, Zhang1997}. 

From an information theoretic point of view, lumpability can be seen as a lossy
compression scheme \cite{Lindqvist1978}, \cite{Watanabe1960}. On the more general
topic of graph clustering, the  \textsc{infomap} algorithm of \cite{Rosvall2008} also
exploits this compression intuition. By taking random walks in the graph, their
algorithms seeks a partitioning of \textit{minimum description length}. More recently,
\cite{Deng2011} gave a characterization of the optimal aggregation of NDMC systems in
terms of the Kullback-Leibler divergence. They show that optimal partitioning yields
aggregates of maximum \textit{predictability} as measured by the mutual information
between $X(t)$ and $X(t+1)$. It should be stressed that the applicability of these results
were constrained to the uncontrolled case. Similar derivations could be attempted in the
controlled case only by considering the rate distortion for a fixed value function such as
in \cite{Still2012}.
