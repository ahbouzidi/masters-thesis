\section{Markov Chains}

A discrete-time stochastic process is a family of random variables (\rvs)  $\{ X(t), t \in T\}$
indexed by a time parameter $t \in \mathbb{N} = \{0, 1, 2,...\}$. The
value of $X_n$ is referred to as the \termidx{state} of the process. When the family of
random variables is defined over a discrete sample space
$\Omega$, the stochastic process is said to be \textit{discrete-valued}.
Stochastic processes are useful in modelling probabilistic phenomena evolving in
time such the price of a stock, or the
number of connections to a web service over the day for example. Due to the
complexity of many systems in real life, it is desirable to assume certain properties to make the modelling exercise more tractable. One could, for instance, decide to treat the \rvs as being
\textit{independent and identically distributed} (iid), but only at the cost of loosing the ability to
capture correlation among them. The so-called \textit{Markov}
assumption is often used when the iid property is deemed too restrictive.

\begin{defn}
A \termidx{Markov Chain} is a discrete-time and discrete-valued stochastic process
which possesses the Markov property. The Markov property implies that for all times $n \geq
0$ and all states $i_0, \dots, i, j \in \Omega$:
\begin{equation}
P(X_{n+1} = j | X_0 = i_0, X_1 = i_1, ..., X_n = i) = P( X_{n+1} = j | X_n = i)
\end{equation}
\end{defn}

The Markov property (or \termidx{memoryless property}) could be stated simply by
saying that the \textit{future is independent of the past, given the present state}.
Therefore, knowing the current value of $X_n = i$ is enough to characterize the
future evolution of the stochastic process $\{X_{n+1}, X_{n+2}, \dots \}$ and the history
$\{X_0, X_1,\dots, X_{n-1} \}$ can be discarded.

A Markov chain is completely specified by the one-step transition probabilities
$p_{ij} = P(X_{n+1} = i | X_n = j)$ contained in its Markov or
\termidx{stochastic matrix}. For a finite state-space $\mathcal{X}$, we have:
\begin{equation}
\mathbf{P} = \begin{bmatrix}
p_{11} & p_{12} & \cdots & p_{1m} \\
p_{21} & p_{22} & \cdots & p_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
p_{m1} & p_{m2} & \cdots & p_{mm}
\end{bmatrix}
\end{equation}

Furthermore, $\mathbf{P} $ must satisfy the following properties:
\begin{equation}
p_{ij} \geq 0 \mbox{ and } \sum_{j=1}^M p_{ij} =1 
\end{equation}

Let $\mathbf{P}^\optimal$ denote the adjoint of $\mathbf{P}$. If $\mathbf{P}^\optimal = \mathbf{P}$ holds, then the stochastic matrix is said to be \termidx{time-reversible}. 

Those elements of $\mathbf{P}$ for which $p_{ii} = 1$ are said to be
\termidx{absorbing} and once entered, the Markov chain can never escape.

If the elements of $\mathbf{P} $ are independent of the time index, the
Markov chain is said to be \termidx{time-homogeneous} and has \termidx{stationary transition
probabilities}. If this property is satisfied and one knows the transition probabilities of
a Markov Chain, the problem of computing probability distributions is greatly
simplified and can be succinctly expressed using matrix multiplication.  

The \termidx{Chapman-Kolmogorov} equation lets us express the probability of
going from state $i$ to $j$ in $n$ steps by $[\mathbf{P}^n]_{ij} = [
\underbrace{\mathbf{P} \times \mathbf{P} \times \dots \times \mathbf{P}}_{\text{n
times}} ]_{ij}$. The probability of transitioning from state $i$ to any other state under
$n$ steps then becomes:
\begin{equation}
P( \cdot, n | i) = e_i \mathbf{P}^n
\end{equation}
Here $e_i$ stands for the vector with zero components everywhere except of for its
$i$th component set to 1. 

Using the Chapman-Kolmogorov equation, a state $j$ is classified as being
\termidx{accessible} from $i$ if $[\mathbf{P}^n]_{i,j} > 0$ for some $n \geq 0$. When
the relation holds in both direction, $i$ and $j$ are said to \termidx{communicate}. If
every possible pairs of states can communicate, the Markov chain is classified as being
\termidx{irreducible}. Given that a state $j$ has been initially encountered once, it is said to be \termidx{recurrent} if the probability of visiting it again is nonzero. That
is, denote $T_j$ the time at which the chain returns to $j$, the recurrence property
expresses the fact that $P(T_j < \infty | X_0 = j) > 0$. When $E(T_j | X_0) < \infty$,
state $j$ is said to be \termidx{positive recurrent}.

These last definitions are essential for defining the \termidx{ergodicity}
property: a condition often assumed in the analysis of Markov Decision Processes.
\begin{defn}
A Markov chain is \termidx{ergodic} if it is irreducible and positive recurrent.
\end{defn}

It often occurs that one is interested in knowing how the probability distribution
for $X_n$ would evolve in time.  Denote the
distribution of $X_n$ by the row vector $\mu_n$ and the \termidx{initial distribution}
by $\mu_0$. The follow relation can be shown to hold
\begin{equation}
 \mathbf{\mu}_n= \mathbf{\mu}_0 \mathbf{P}^n \label{eq:initial-dist}
\end{equation}
by noting that the Markov property implies that
\begin{align}
P(X_0 &= i_0, X_1 = i_1, X_2 = i_2, \dots, X_n = i_n) = \notag \\
&P(X_0 = i_0) P(X_1 = i_1 | X_0 = i_0) P(X_2 = i_2 | X_1 = i_1) \dots P(X_n = i_n |
X_{n-1} = i_{n-1})
\end{align}
and that our Markov chain is time-homogeneous ($\mathbf{P}$ is fixed).

If it happens that the distribution of $\mu_n$ does not change with $n$, then 
 the Markov chain is said to be \termidx{stationary}. 

\begin{defn}
A Markov chain said to be stationary if it satisfies $P(X_0 = i_0, X_1 = i_1, \dots, X_n
= i_n) = P(X_m = i_0, X_{m+1}, \dots, X_{m+n})$ for any $m \in \mathbb{Z_+}$.
\end{defn}

Using \ref{eq:initial-dist}, the stationarity problem for a Markov Chain amounts to
finding a vector $\pi$ such that
\begin{equation}
\mathbf{\pi}^\transpose \mathbf{P} = \mathbf{\pi}^\transpose
\end{equation}

The stationary distribution $\mathbf{\pi}^\intercal$ can understood as being the left
eigenvector of $\mathbf{P}$ associated with the eigenvalue 1. Furthermore, it must be
that $\sum_k \mathbf{\pi}_k = 1$ in order for $\pi$ to be a well-defined probability
distribution. When the Markov chain reaches the stationary distribution, it is said to be
in its \termidx{steady-state} mode.

An important characterization of the properties of stochastic matrices comes from the
theory of nonnegative matrices in the form of the Perron-Frobenius theorem. First, define the spectral radius of a square matrix as

\begin{equation}
\rho(A) \overset{\underset{\mathrm{def}}{}}{=} \max_i(|\lambda_i|)
\end{equation}

The theorem in its original form by \cite{Horn1986} is about irreducible matrices and amounts to the following. 

\begin{thm}[Perron-Frobenius Theorem]

Let $\mathbf{A}$ be an irreducible and nonnegative matrix, then the following claims hold
\begin{enumerate}
\item The spectral radius $\rho(\mathbf{A})  > 0$
\item $\rho(\mathbf{A})$ is an eigenvalue of $\mathbf{A}$
\item There exists a positive vector $\mathbf{x}$ such that $\mathbf{A}\mathbf{x} = \rho(\mathbf{A})\mathbf{x}$ and $\rho(\mathbf{A})$ is a simple eigenvalue of $\mathbf{A}$.
\item The unique eigenvector whose components sum to 1 is called \termidx{Perron
 vector}, that is
\begin{equation}
\mathbf{A} \mathbf{p} = \rho(\mathbf{A}) \mathbf{p}, \hspace{2.5mm} \mathbf{p} > 0 \hspace{2.5mm} \mbox{and} \hspace{2.5mm} \|\mathbf{p}\|_1 = 1
\end{equation}
\end{enumerate}
\end{thm}

The theorem can also be conveniently adapted to Markov chains under the following
lemma proven in \cite{Montenegro2006}.
\begin{lem}
Let $\mathbf{P}$ be the stochastic matrix associated with a irreducible and reversible 
Markov chain over a state space $\mathcal{X}$ of size $n$. $\mathbf{P}$ must then
have a complete spectrum of real eigenvalues of magnitude at most 1 and of the form
\begin{equation}
1 = \lambda_0 \geq \lambda_1 \lambda \geq \dots \geq \lambda_{n-1} \geq -1
\end{equation}
\end{lem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Markov Decision Processes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Markov Decision Processes}
\label{sec:mdp}
The theoretical treatment given about Markov chains has not considered so far the influence of an external \textit{input} or \textit{control}. The case of controlled Markov chains under the framework of \mdps is studied in this section. The theory of \mdps was instrumental in the development of the field of Reinforcement Learning studied later in this chapter. The following presentation adopts most of the notation from \cite{Csaba2010}. 

\begin{defn}
\label{defn:mdp}
A finite-action discounted \termidx{Markov Decision Process} (MDP) is a tuple
$\mathcal{M} = \langle \mathcal{X}, \mathcal{A}, r, \mathcal{P},\gamma \rangle$ where
$\mathcal{X}$ is a non-empty countable set of states, $\mathcal{A}$ is a finite state
of actions, $\mathcal{P}$ is a transition probability kernel giving rise to a distribution
$P(\cdot |x, a)$ over $\mathcal{X}$ given any $x \in \mathcal{X}$ and $a \in
\mathcal{A}$,  $r$ is the reward function $r: \mathcal{X} \times \mathcal{A} \mapsto
\mathbb{N}$ and $\gamma \in [0, 1]$ is a discount factor.
\end{defn}

A \termidx{deterministic policy} $\pi$ is a mapping $\pi : \mathcal{X} \mapsto
\mathcal{A}$. In the theory of MDPs, stochastic policies of the form $\pi : \mathcal{X}
\mapsto \Omega(\mathcal{A})$ are also considered, in which case actions are
drawn from a conditional probability distribution  over states according to $A_t \sim
\pi(\cdot | X_t)$ (here $A_t$ and $X_t$ are \rvs at time $t$). 

Fixing a policy for an MDP  induces a \termidx{Markov Reward Process}
(MRP) $\langle \mathcal{X}, \mathcal{R}^\pi, \mathcal{P}^\pi, \gamma \rangle$ with
the reward function $\mathcal{R}^\pi(x) = r(x, \pi(x))$ and state transition probability
kernel $P(\cdot | x) = P(\cdot | x, \pi(x))$. An MRP can be thought of as a Markov chain
augmented with a reward at every state. The concepts of induced Markov
chains and Markov Reward Processes are particularly useful in the analysis of 
MDPs. At the level of the induced Markov chain of an MDP, the
existence of a stationary distribution is not necessarily guaranteed.

\subsection{Value Function}
The notion of a value function goes on par with the principle of optimality
that underlies the decision theoretic framework of MDPs. It is assumed in this context
that a decision maker, or \textit{agent}, is acting in such a way as to optimize some
intrinsic notion of \textit{appropriateness} in its behaviour. In the definition \ref{defn:mdp}
of an MDP, the reward function $r$ captures this idea. Under this setting, an agent  tries to optimize a quantity known as the \termidx{return}

\begin{defn}
The return of an \mdp is the total discounted sum of the rewards originating from
the induced \mrp.
\begin{equation}
\mathcal{R} = \sum_{t=0}^\infty \gamma^t R_{t+1} \label{eq:return}
\end{equation}
\end{defn}

Fixing the value of $\gamma < 1$ leads to a kind of return where the immediate
reward is worth exponentially more than the far future. In this case, the resulting MDP 
is called a \termidx{discounted \mdp}. On the other hand, setting $\gamma =
1$ makes the immediate reward at each step equally important: one 
then talks of an \termidx{undiscounted \mdp}

Knowing the desirability, or value, of a state at any moment makes the problem of finding an optimal way of behaving much easier.
\begin{defn}
The \termidx{value function} $V^\pi: \mathcal{X} \mapsto \mathbb{R}$ of a
stationary policy $\pi$ is the conditional expectation of the return (discounted or
undiscounted) given a state. Under the discounted model, 
\begin{equation} 
V^\pi(x) = \expectation \left[ \sum_{t=0}^\infty \gamma^t R_{t+1}  \given  X_0 =
x\right] , x \in \mathcal{X} \label{eq:value-function}
\end{equation}
\end{defn}

A behaviour is said to be optimal when the value of its policy is also optimal. An
optimal value function $V^*: \mathcal{X} \mapsto \mathbb{R}$ is one which obtains
the maximum possible expected return for every state.

% Action value function 
The \termidx{action-value function} of a policy $\pi$ is closely related to the notion of
value function presented above. Instead of being defined only over states, it specifies
the expected return of initially being in state $x$, choosing a first action $a$ and subsequently committing to $\pi$. 
\begin{defn}
The action-value function $Q^\pi: \mathcal{X} \times \mathcal{A} \mapsto
\mathbb{R}$ of a stationary policy $\pi$ is the conditional expectation of the return
given an initial state-action pair. 
\begin{equation}
Q^\pi(x, a) = \expectation \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \vert X_0 = x,
A_0 = a \right], \; x \in \mathcal{X}, a \in \mathcal{A}
\end{equation}
\end{defn}

If the action-value function yields the maximum expected return for every state-action pair, it is said to be optimal and is denoted by $Q^*$. One can go from the
action-value function to the value function by noting that for a finite-action MDP
\begin{equation}
V^*(x) = \max_{a \in \mathcal{A}} Q^*(x, a) \label{eq:qopt-to-vopt}
\end{equation}

In the more general case of a countable non-empty sets of actions, the $\max$
operator should be replaced with the supremum ($\sup$) of $\mathcal{A}$. The
optimal action-value function can also be recovered from the value function as
follow:
\begin{equation}
Q^*(x,a) = r(x,a) + \gamma \sum_{y \in \mathcal{X}} P(y, a, x) V^*(y), \; x \in
\mathcal{X}, a \in \mathcal{A} \label{eq:vopt-to-qopt}
\end{equation}

\subsection{Bellman Equations}
Similar to equation \ref{eq:vopt-to-qopt}, given an MDP, the  so-called 
\termidx{Bellman equations} are expressed under

\begin{equation}
V^\pi(x) = r(x, \pi(x)) + \gamma \sum_{y \in \mathcal{X}} P(y, \pi(x), x) V^\pi(y)
\label{eq:bellman-equations}
\end{equation}

The recursive formulation of the Bellman equations relates the value of states
to that of its possible successor states, weighted by their probability of occurrence. For a finite d-dimensional state space $\mathcal{X}$, $V^\pi$ and
$r^\pi$ can be thought to be vectors in $\mathbb{R}^d$ with $\mathcal{P}$ being a
transition matrix $\mathbf{P}: \mathbb{R}^{d \times d}$. It can be seen that the
Bellman equations define a linear system of equations in
$d$ unknowns whose unique solution is $V^\pi$

\begin{equation}
\mathbf{V}^\pi = \mathbf{r}^\pi + \gamma \mathbf{P}^\pi \mathbf{V}^\pi
\end{equation}

Solving for the left hand side, 

\begin{equation}
\mathbf{V}^\pi = (\mathbf{I} - \gamma \mathbf{P}^\pi)^{-1} \mathbf{r}^\pi
\label{eq:direct-value}
\end{equation}

Under the framework of reinforcement learning adopted in this work,
$\mathbf{P}$ and $\mathbf{r}^\pi$ are not available \textit{a priori}, making the
direct solution of \ref{eq:direct-value} impossible to compute. Furthermore, since
matrix inversion is generally of order $\mathcal{O}(n^3)$, the computational
cost would quickly become impractical for large state spaces. Reinforcement learning algorithms generally solve this problem in an iterative fashion at a very low cost per iteration.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bellman Operator
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bellman Operator}
The notion of \termidx{Bellman operator} subtends the theoretical justification of the online methods of RL for computing the value function of policies. 

\begin{defn}
The Bellman operator $T^\pi: \mathbb{R}^\mathcal{X} \mapsto \mathbb{R}^\mathcal{X}$ underlying a policy $\pi$ for an MDP is defined as

\begin{equation}
(T^\pi V) = r(x, \pi(x)) + \gamma \sum_{y \in \mathcal{X}} P(y, \pi(x), x) V(y), \; x
\mathcal{X} \label{eq:bellman-operator}
\end{equation}
\end{defn}

Similarly, one can define the Bellman operator $T^\pi : \mathbb{R}^{\mathcal{X}
\times \mathcal{A}} \mapsto \mathbb{R}^{\mathcal{X} \times \mathcal{A}}$ for the
action-value function as

\begin{equation}
T^\pi Q(x, a) = r(x, a) + \gamma \sum_{y \in \mathcal{X}} P(y, a, x) V(y), \; x \in
\mathcal{X} \label{eq:bellman-operator-action-value}
\end{equation}

At first blush, it can be easy to miss the difference between the overall form of the
Bellman equations \ref{eq:bellman-equations} and that of equation \ref{eq:bellman-operator}. One must in fact observe that, even though $T^\pi$ is defined for a given
policy, the $V$ term on the other hand might not correspond to $V^\pi$. 
The Bellman operator happens to be a special type of function called a
\termidx{contraction mapping}. This characterization allows the \termidx{Banach
fixed-point theorem} to be applied for proving convergence.  Starting with an
estimate of the value function for a policy, the iterative application of the Bellman
operator will converge in the limit to a unique fixed point corresponding to $V^\pi$. The same principle underlies the value and policy iteration algorithms for computing the optimal value function. 

Before delving into the details of these
algorithms, the \termidx{Bellman optimality operator} must be introduced. Just as
$T^\pi$, $T^\optimal: \mathbb{R}^\mathcal{X} \times \mathbb{R}^\mathcal{X}$ is a maximum-norm contraction mapping  but is defined this time as
\begin{equation}
(T^\optimal V) = \max_{a \in \mathcal{A}} \left\lbrace r(x, a) + \gamma \sum_{y
\mathcal{X}} P(y, \pi(x), x) V^\optimal (y) \right\rbrace, \; x \in \mathcal{X} 
\label{eq:bellman-optimality-operator}
\end{equation}

The optimal value function $V^\optimal$ satisfies the fixed-point equation
$T^\optimal V^\optimal = V^\optimal$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Solving MDPs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Solving MDPs}

The Banach fixed-point theorem mentioned in the previous section lays the foundations of
two important approaches for solving MDPs: the value and policy iteration algorithms. By
\textit{solving and MDP}, one generally refers to finding the optimal value function
underlying an MDP. If the value function found in that way is indeed optimal, an
optimal policy can be derived by greedily picking the best action from it.

The two main problems of RL which consist in finding the value of a policy or finding an optimal policy are usually called the \termidx{prediction} (figure \ref{fig:prediction-problem}) and \termidx{control} (figure \ref{fig:control-problem}) problems. In the control
problem, one tries to derive an optimal policy by taking the greedy policy with
respect to the optimal value function (see theorem 2.2 of \cite{Ross1983} for a proof). The optimal stationary greedy policy maximizes the right side of

\begin{equation}
\pi^\optimal(x) = \arg \max_{a \in \mathcal{A}} \left[ r(x,a) + \gamma \sum_{y \in
\mathcal{X}} P(y, a, x) V^*(y)\right], \; x \in
\mathcal{X}, a \in \mathcal{A} \label{eq:greedy-optimal-policy}
\end{equation}

\begin{figure}
\centering
\input{fig/spaces}
\caption{The policy evaluation problem consists in finding the value function
corresponding to a given policy $\pi$. The focus here is on the space of 
stationary policies $\Pi_{stat}$}
\label{fig:prediction-problem}
\end{figure}

\begin{figure}
\centering
\input{fig/control}
\caption{The control problem aims at finding an optimal policy i.e. one for which the
corresponding value function is optimal. There might be multiple optimal policies, but
the optimal value function must be unique \parencite{Ross1983}.}
\label{fig:control-problem}
\end{figure}

Finding $V^*$ thus allows the control problem to be solved using
equation \ref{eq:greedy-optimal-policy}. The \termidx{value-iteration} algorithm is
one way in which $V^\optimal$ can be obtained. Let $V_0$ be some arbitrary initial
bounded function, this method consists in applying the Bellman optimality operator
$T^\optimal$ successively in the following manner
\begin{equation}
V_{k+1} = T^\optimal V_k \label{eq:value-iteration}
\end{equation}

$V_k$ can be shown (proposition 3.1 of \cite{Ross1983}) to uniformly converge to $V^*$ as $k \to \infty$

With \termidx{policy iteration}, two steps are interleaved: policy evaluation and
policy improvement. The general idea goes as follow: from an initial policy $\pi_0$
compute its corresponding value function (policy evaluation) and derive the greedy
policy $\pi_{k+1}$ from it (policy improvement), then repeat these two steps as
necessary. It can be shown that the policy computed by policy
iteration after $k$ steps is not worse than the greedy policy computed
by value iteration. Because of the policy evaluation step, policy-iteration is
computationally more expensive. It plays however an important role in the Actor
Critic architectures \parencite{Sutton1984}.

The policy and value iterations algorithms belong to a class known as
\termidx{dynamic programming} (DP) methods. Assuming a perfect knowledge of the
transitions dynamics and reward function, the computational 
burden is greatly reduced compared to a naive direct policy search approach which could be order
$\left\vert \mathcal{X} \right\vert^{\left\vert \mathcal{A} \right\vert}$. Under the framework of reinforcement learning, it will not be possible to maintain these assumptions. DP remains of great importance for the understanding of RL algorithms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Reinforcement Learning
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Reinforcement Learning}

Reinforcement Learning (RL) considers the problem setting of a situated
\termidx{agent} learning to optimize a loss (return) from the direct experience
provided by the \termidx{environment} (figure \ref{fig:rl}). Historically, the field has
been influenced by the work on trial-and-error learning from
psychology \parencite{SuttonBarto1998}. RL hinges heavily upon the theory of \mdps introduced in the previous section.

\begin{figure}
  \centering
  \input{fig/rl}
  \caption{Reinforcement Learning. An agent executes an action $a_t$ in the
  environment at time $t$ producing a state transition and instantaneous reward
  dictated by the environment dynamics.}
  \label{fig:rl}
\end{figure}

The DP assumption of an \termidx{environment model} being known is
lifted under the RL framework and \termidx{model-free} learning becomes the focus. The presence
of a model can still be accommodated and subtend a set of RL
methods for \termidx{planning}, i.e. determining the best course of action for
accomplishing a goal by simulating the consequences of actions in the model.
Additionally, RL is concerned with the problem of acquiring relevant experience
from the environment in an online fashion: a problem of joint
\termidx{exploration} and control. The way in which the acquired experience relates
to the target policy depends on the learning algorithm which is either
classified as \termidx{on-policy} or \termidx{off-policy}.

 In the following sections, the Monte-Carlo (MC) method will be presented as a way to learn about the value of states in the absence of a model. Then the Temporal Difference (TD) learning algorithm will be introduced and shown to encompass the MC methods. Finally, the off-policy Q-Learning algorithm will be presented. 

\subsection{Monte-Carlo}
The Monte-Carlo approach for solving the control problem estimates the value
of a state by taking online samples of the return directly from the
environment or with simulated experience using a model. Throughout multiple episodes, independent samples of return are averaged to
obtain an estimate of the true expectation. According to the law of large number, as
the number of samples goes to infinity, the average becomes an unbiased
estimator. 

The procedure described above is however not sufficient for solving the control
problem as it only answers the prediction one. In the absence of a model, it is
necessary to obtain an action-value function for control. Since certain state-action pairs might be difficult to sample frequently enough in large state spaces, it
is often assumed that the agent can be reset in some arbitrary state-action
configuration: an \termidx{exploring start}. In practice this assumption is often impossible to meet.  Fortunately, it can be overcome using \termidx{soft-policies}.

The Monte-Carlo method for control is based on the general framework of policy
iteration where the current policy is improved greedily with respect to some
estimate of its value function. It must be noted that policy iteration only requires to update the policy
towards the greedy policy.  Soft-policies of the form $\pi(x, a) > 0$ consider a slightly perturbed instance
of their greedy policy such that the previous condition is not completely violated. It then becomes possible to explore non-greedy actions and the need for exploring starts is eliminated \parencite{SuttonBarto1998}. A class of soft policies commonly used is the \termidx{$\epsilon$-greedy} one, where a random action is chosen with probability $\epsilon$, and the greedy action for $1 - \epsilon$. 

The  general scheme described so far could be described as an on-policy learning approach,
where the value of a policy is simultaneously being evaluated and used for control.
If the policy used to obtain samples, the \termidx{behavior policy}, is different from
the one being evaluated and improved upon, the \termidx{estimation policy},
importance sampling techniques must be applied to compensate for the
discrepancy in the action selection distributions. Algorithms capable of learning the value of a target policy while following a different behavior policy are said to be off-policy methods.

\subsection{Temporal Difference Learning}
The main drawback of using Monte-Carlo methods in an online setting is
the need to wait for the complete execution of an episode before updating the value
function estimate. The Temporal Difference (TD) learning algorithm of \cite{Sutton1984} showed how to overcome this problem and was instrumental in making the RL approach practical. In TD learning, immediate predictions are used as targets, a technique known as
\termidx{bootstrapping}, and alleviates the need to wait for a full backup. The TD($\lambda$) is a extension of the original algorithm that unifies DP and Monte-Carlo methods.

Recalling the definition of the value function, for all $x \in \mathcal{X}$
\begin{align}
V^\pi(x) &= \expectation \left[ \sum_{t=0}^\infty \gamma^t R_{t+1}  \given  X_0 =
x\right] \notag \\
&= \expectation \left[ R_{t+1} + \gamma \sum_{k=0}^\infty \gamma^k
R_{t+k+2}\given X_t = x\right]  \notag \\
&= \expectation \left[ R_{t+1} + \gamma V(X_{t+1}) \given X_t = x\right]
\label{eq:value-function-recursive}
\end{align}

The TD(0) algorithm incrementally updates the value function estimate using samples
of the form $R_{t+1} + \gamma V(X_{t+1})$ exposed in equation \ref{eq:value-function-recursive}. The value function estimate is updated by
\begin{equation}
\hat{V}_{t+1}(x) = \hat{V}_{t}(x) + \alpha_t \left[ R_{t+1} + \gamma \hat{V}_t (X_{t+1})
- \hat{V}_t(X_t)\right] \label{eq:tdupdate}
\end{equation}

TD is a Stochastic Approximation (SA) method and can be shown to converge \parencite{Csaba2010} to the true $V^\pi$ by treating the sequence $\hat{V}_t$ as a linear ordinary differential equation (ODE). The sequence of step sizes $\alpha$ must also be subject to the Robbins-Monro (RM) conditions according to which
\begin{equation}
\sum_{t=0}^\infty \alpha_t = \infty, \hspace{5mm} 
\sum_{t=0}^\infty \alpha_t^2 < + \infty
\end{equation}

Algorithm \ref{alg:td0} shows how the TD update is performed procedurally.

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized vector $V$, a policy $\pi$ to be evaluated}
\KwResult{The value of $\pi$}
$x \leftarrow$ initial state\;
\While{x is not terminal} {
$a \leftarrow \pi(x)$\;
$r, x^\prime \leftarrow \FuncSty{Step}(a)$\;
$V(x) \leftarrow V(x) + \alpha\left[ r + \gamma V(x^\prime) - V(x) \right]$ \;
$x \leftarrow x^\prime$
}
\caption{Tabular TD(0) algorithm for policy evaluation. The $Step$ function performs
the state transition in the environment and returns the immediate reward and new state.}
\label{alg:td0}
\end{algorithm}

\subsection{Eligibility Traces}

An important extension to the original TD algorithm is the TD($\lambda$) family
\parencite{Sutton1984}, unifying TD(0) at one end and Monte-Carlo prediction at the other.
\termidx{Eligibility traces} act as kind of memory modulating the propagation of backups at a given state. Instead of updating the value function based on 
a single n-steps estimate, TD($\lambda$) computes an average, known as the
$\lambda$-return, over a range of multi-step predictions of the return. The multi-step discounted return is  defined as
\begin{equation}
\mathcal{R}_{t:k} = \sum_{s=t}^{t+k} \gamma^{s-t}R_{s+1} +
\gamma^{k+1}\hat{V}_t(X_{t+k+1})
\end{equation}

The $\lambda$-return is a mixture of multi-step return with weight $(1 -
\lambda)\lambda^k$ on each term
\begin{equation}
\mathcal{R}_t^\lambda = (1 - \lambda)\sum_{k \geq 0} \mathcal{R}_{t:k}
\label{eq:lambda-return}
\end{equation}

As $\lambda$ goes to 0, equation \ref{eq:lambda-return}  simplifies to $R_{t+1} +
\gamma \hat{V}_t (X_{t+1})$ and amounts to a one-step TD backup of the TD(0)
algorithm. On the other hand, as $\lambda$ goes to 1 samples of the return from time $t$ until the end of the episode are obtained as in the MC method.

TD($\lambda$) is implemented using an additional vector of size $\left\vert
\mathcal{X} \right\vert$ where each component gets incremented by 1 each time the
corresponding state is visited. A decay of $\lambda \gamma$ is also applied upon
each component for every time step.

\subsection{Sarsa}
The TD algorithm only solves the policy evaluation problem but not control one. Following the general policy iteration paradigm and 
the MC algorithm for control, \textsc{Sarsa} is an on-policy control algorithm making use of
TD for policy evaluation under a soft-policy exploration strategy. Similar to equation
\ref{eq:tdupdate}, the action-value function is updated as follow
\begin{equation}
\hat{Q}_{t+1}(x, a) = \hat{Q}_{t}(x, a) + \alpha \left[ R_{t+1} + \gamma \hat{Q}_t
(X_{t+1}, A_{t+1}) - \hat{Q}_t(X_t, A_t)\right] \mathbb{I}_{X_t = x, A_t = a}
\label{eq:sarsaupdate}
\end{equation}

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized matrix $Q$}
\KwResult{An optimal action-value function for control}
\ForEach{episode}{
$x \leftarrow$ initial state\;
$a \leftarrow \FuncSty{Greedy}(x)$\;
\While{x is not terminal} {
$r, x^\prime \leftarrow \FuncSty{Step}(a)$\;
$a^\prime \leftarrow \FuncSty{Greedy}(x^\prime)$ \;
$Q(x, a) \leftarrow Q(x, a) + \alpha\left[ r + \gamma Q(x^\prime, a^\prime) - Q(x, a)
\right]$ \;
$x \leftarrow x^\prime$ \;
$a \leftarrow a^\prime$
}
}
\caption{The on-policy Sarsa algorithm based on a TD(0) policy evaluation scheme.
The $Greedy$ function is the soft greedy policy derived from the current estimate of
the action-value function. An $\epsilon$-greedy exploration strategy would be
commonly used.}
\label{alg:sarsa}
\end{algorithm}

As the number of samples for each state-action pair goes to infinity, $\lim_{t \to
\infty} \epsilon = 0$, and under the RM conditions, \textsc{Sarsa} is guaranteed to converge to
an optimal policy \parencite{SuttonBarto1998}. 

\subsection{Q-Learning}

Since TD learning is an on-policy learning method, the choice of exploration strategy
directly impacts the convergence to the estimation policy. \textsc{Q-Learning}
\parencite{Watkins1989} decouples the exploration and evaluation problems and allows for
any behavior policy to be followed while still converging to $Q^\optimal$. In this case,
the updates to the action-value function estimate consist in
\begin{equation}
Q_{t+1} (x,a) = Q_t(x,a) + \alpha_t \left[ R_{t+1} + \gamma \max_{a^\prime \in
\mathcal{A}} Q_t(X_{t+1}, a^\prime) - Q_t(X_t, A_t) \right] \mathbb{I}_{X_t = x, A_t = a} 
\label{eq:qlearning-update}
\end{equation}

The corresponding procedural form is given in algorithm \ref{alg:qlearning}.

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized matrix $Q$}
\KwResult{An optimal action-value function for control}
\ForEach{episode}{
$x \leftarrow$ initial state\;
\While{x is not terminal} {
$a \leftarrow \FuncSty{Greedy}(x)$\;
$r, x^\prime \leftarrow \FuncSty{Step}(a)$\;
$Q(x, a) \leftarrow Q(x, a) + \alpha \left[ r + \gamma \max_{a^\prime \mathcal{A}}
Q(x^\prime, a^\prime) - Q(x, a) \right]$ \;
$x \leftarrow x^\prime$
}
}
\caption{The Q-Learning algorithm under some arbitrary exploration scheme.
$\epsilon$-greedy could once again be used for this task.}
\label{alg:qlearning}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% State Abstraction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{State-Abstraction}
The algorithms presented so far assumed \textit{tabular} update rules where value or
action-value functions were expressed as vectors and matrices respectively. For many
problems, the state space might have infinite cardinality or be simply too large to fit in
memory. Furthermore, as the number of dimensions increases, the computational cost
also increases exponentially. It matters then to seek for a representation of the
value function capable of generalizing across possibly unseen states or state-action
pairs.

By expressing the value function as a parametrized function, the incremental updates
are performed upon the entries of some parameter vector $\theta$. For
large state spaces, the number of components in $\theta$ would be much smaller than
the number of possible states. Value function
approximation can be seen as an instance of a supervised learning problem
with a training set consisting of states as inputs and samples of the return (one-step TD,
or full Monte-Carlo) as targets.

Any existing method from supervised learning could potentially be used for this task:
neural networks, or k-nearest neighbors for regression for example \parencite{Csaba2010}. \textsc{TD-Gammon} of \cite{Tesauro1995} is considered to be one of the great success story or RL and used neural networks for value function approximation. In this thesis, the attention
will be mainly drawn upon the so-called \textit{linear methods}. They are simple, but yet
expressive, function approximators of the form

\begin{equation}
V_{\theta}(x) = \theta^\transpose \varphi(x)
\label{eq:linear-methods}
\end{equation}

Linear methods represent the value function as a linear
combination of \termidx{features}. In equation \ref{eq:linear-methods}, $\theta \in \mathbb{R}^d$ and $\varphi : \mathcal{X} \to \mathbb{R}^d$ with its components
being defined by a set of \termidx{basis functions} $\varphi_i : \mathcal{X} \to
\mathbb{R}$. The way in which $\varphi$ is obtained specifies a feature extraction
method which can be non-linear.

Value function approximation most commonly relies on \termidx{gradient-descent}
methods to derive update rules for $\theta$. Because of the simple form of equation \ref{eq:linear-methods}, taking the
gradient of the value function with respect to $\theta$ yields

\begin{equation}
\nabla_{\theta} V_\theta(x) = \varphi(x)
\label{eq:linear-gradient}
\end{equation}

Gradient-descent methods update the components of $\theta$ by small increments in the direction of
the gradient, pointing in the direction of the steepest error. A typical error function minimized in supervised
learning techniques is the mean-squared error (MSE). Casting the function
approximation problem under a setting where the true (yet unknown) underlying value
function $V^\pi$ is a target, and $V_\theta$ is some approximation using the
parametrized form of equation \ref{eq:linear-methods}, the MSE is 

\begin{equation}
MSE(\theta) = \sum_{ x \in \mathcal{X}} P(x) \left[ V^\pi(x) - V_\theta(x) \right]
\end{equation}

The $P(x)$ term here accounts for probability of observing a given state $x \in \mathcal{X}$ as an input.

TD($\lambda$) with linear function approximation (algorithm \ref{alg:linear-tdlambda}) can be shown to converge under the usual RM conditions on the $\alpha$ parameter \parencite{Tsitsiklis1997}.

\begin{algorithm}
\DontPrintSemicolon
\KwData{An arbitrary initialized vector $\theta$, a policy $\pi$ to be evaluated}
\KwResult{The value of $\pi$}
$x \leftarrow$ initial state\;
\While{x is not terminal} {
$a \leftarrow \pi(x)$\;
$r, x^\prime \leftarrow \FuncSty{Step}(a)$\;
$\delta \leftarrow r + \gamma \theta^\transpose \varphi[x^\prime] -
\theta^\transpose \varphi[x]$\;
$\mathbf{z} \leftarrow \varphi[x] + \gamma \lambda \mathbf{z}$ \;
$\theta \leftarrow \theta + \alpha \delta z$\;
$x \leftarrow x^\prime$
}
\caption{TD($\lambda$) with linear function approximation}
\label{alg:linear-tdlambda}
\end{algorithm}

\subsection{Basis functions}

The choice of proper function space and feature extraction techniques is a challenging problem which can have a great impact on the quality of the approximation. If one has prior knowledge about certain \textit{regularities} of the optimal value function for the problem at hand, relevant features could be
specified explicitly. However, when facing the problem of solving a broad class of
problems for which little is known about its structure, general set of basis functions
must be used. Two feature extraction schemes are presented below, chosen for their wide
applicability and ease of use. 

\subsection{Radial Basis Functions}

Given a state $x \in \mathcal{X}$, each component of $\varphi$ is specified by a radial basis function with the property that $\varphi_i(x_i) = \varphi_i(\|x - c_i\|)$, where $c_i$ specifies the \textit{center} of the basis. Gaussian functions are most commonly used as radial basis functions (RBF) and are defined as

\begin{equation}
\varphi_i(x) = \exp\left( \frac{\| x - c_i\|}{2\sigma_i^2}\right)
\label{eq:rbf}
\end{equation}

The $\sigma_i$ parameter specifies the \textit{width} of the Gaussian and must be
tuned by hand or using some model selection technique together with the
layout of the basis functions and their number. Decreasing the width and increasing the
number of basis functions would result in a finer approximation but also incur a higher
computational cost. 

The choice of norm in equation \ref{eq:rbf} is not restricted to the Euclidean distance
and other metric could be used. In \cite{Sugiyama2008} for example, the Geodesic
distance taken over the graph induced by some MDP attempts to better
capture intrinsic geometrical features.
 
\subsection{Fourier Basis Functions}

The use of Fourier basis functions for value function approximation was introduced in
\cite{Konidaris2011b} but relies on the well established theory of Fourier analysis.

The nth order Fourier expansion of some univariate periodic function $f$ with period
$T$ is given by
\begin{equation}
\hat{f}(x) = \frac{a_0}{2} \sum_{k=1}^n \left[ a_k \cos\left(k\frac{2\pi}{T} x\right) +
b_k \sin \left(k \frac{2\pi}{T}x \right) \right]
\label{eq:fourier}
\end{equation}

with Fourier coefficients $a_k$ and $b_k$ defined as
\begin{equation}
a_k = \frac{2}{T} \int_0^T f(x) \cos \frac{2\pi kx}{T}dx, \; \mbox{and} \; b_k = \frac{2}
{T} \int_0^T f(x) \sin \frac{2\pi kx}{T}dx
\label{eq:fouriercoeffs}
\end{equation}

Since $V^\optimal$ is unknown, the Fourier coefficients cannot be obtained directly from \ref{eq:fouriercoeffs}. They must rather be treated as \textit{weights} and approximated under the linear approximation framework. The Fourier expansion of $f$ results in $2n+1$ terms but
\cite{Konidaris2011b} showed how it can be simplified to only $n+1$ terms if some
assumptions are made on the periodicity of the value function. A function $f$ is even if
$f(x) = f(-x)$, in which case the $\sin$ terms of equation \ref{eq:fourier} can be
droped. A similar observation can be made if $f$ is odd $-f(x) = f(-x)$ and the
$\cos$ terms can be omitted. 


Setting the period to $T=2$, the nth order univariate Fourier basis is defined as:
\begin{equation}
\phi_i(s) = \cos(i\pi x) \hspace{2mm} \forall i = 0, ..., n
\end{equation}

The multivariate Fourier expansion of some function $F$ with period T up to order $n$
bears a similar form
\begin{equation}
\hat{F}(\mathbf{x}) = \sum_{\mathbf{c}} \left[ a_c \cos \left( \frac{2\pi}{T} \mathbf{c}
\cdot \mathbf{x} \right) +  b_c \sin \left( \frac{2\pi}{T} \mathbf{c} \cdot \mathbf{x}
\right)\right]
\label{eq:multi-fourier}
\end{equation}

The $\mathbf{c}$ term in equation \ref{eq:multi-fourier} is the Cartesian power 
$\mathcal{X}^d$ of the d-dimensional state space such that $\mathbf{c} = [c_1,
\dots, c_i, \dots, c_d]$ where $c_i \in [0, \dots, n]$. The number of basis functions
required for the nth order expansion under this scheme is $2(n+1)^d$.
Fortunately using the same argument as for the univariate case, only half of the terms
must be kept. The ith basis function is then defined as
\begin{equation}
\varphi(x)_i = \cos \left(  \pi \mathbf{c}^i \cdot \mathbf{x} \right)
\end{equation}

While Fourier basis severely suffer from the curse of dimensionality, the approach has
the merit of being simple and effective empirically.  Experiments conducted during this
thesis support this claim. Furthermore, the only choice practitioners have to make is the
order of the expansion. Reasonable results can usually obtained by using only the
few first lower frequencies of the expansion.
