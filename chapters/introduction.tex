With the breakthroughs in hierarchical reinforcement learning (HRL) at the beginning of the last
decade, temporal abstraction has been expected to play an central role in scaling up currently
known techniques to new levels. Its development however still seems to be hindered by an
easily explainable, yet challenging problem: how can temporal abstractions be discovered
automatically ? This is the "elephant in the room" for HRL. The existence of this problem has
been acknowledged by many authors who also called for the need to develop efficient
techniques to adress it. The author of the MAXQ framework even went to qualify it as being the
"biggest open problem" \cite{Dietterich2000} in HRL. Without automatic methods for
decomposing the problem structure into simpler sub-problems, hierarchical approaches are of
limited interest as the effort for manually specifying them can quickly become insurmountable. 
In one attempt to jointly solve state and temporal abstraction, the author of the HEXQ algorithm
goes on to refer to manual task decomposition as a quasi ``art-form'' \cite{Hengst2002}.

The bottleneck concept arose early in the field of HRL as an intuitive response to this issue. Bottlenecks have been defined as those states which appear frequently on successful trajectories to a goal but not on unsuccessful ones~\cite{Mcgovern2001, Stolle2002} or as nodes which allow for densely connected regions of the interaction graph to reach other such regions~\cite{Menache2002, Simsek2004, Kazemitabar2009}. \cite{Simsek2004} qualifies such states as \textit{access states}, allowing difficult regions of the state space to be reached. The canonical example of bottleneck states often refers to \textit{doorways} in a navigation problem across rooms of a grid-world domain.

\section{Graph Clustering Perspective}
The ideas presented in this thesis found their roots early in the history or HRL. It
will be argued in chapter \ref{chap:dynamics}  that bottlenecks are intrinsically related to graph
clustering. \cite{Hauskrecht1998} had already envisioned such a state-space partitioning
approach leading to an abstract MDP being overlaid to the \textit{periphery} of the partitions.
\textit{Macro-actions} would then be learnt to transition between such abstract peripheral
states. While it is assumed that such a decomposition is provided, it is shown that such a model
can lead to a reduction in the size of the state or action spaces. A similar state-space
partitioning intuition had also been explored in \cite{Dean1995} by one of the co-author of the
previous paper. While the method of \cite{Dean1995} was not cast yet under the framework of
reinforcement learning, it showed how a global planning problem could be decomposed into
smaller locally-solvable problems which could be then be recombined into an optimal policy.
The machinery or HRL being in its infancy, Dantzig-Wolfe method \cite{Dantzig1960} of
decomposition (a linear programming algorithm) was used to solve the abstract MDP. The
problem of obtaining a proper partitioning is also sidestepped in this work.

Along the same line of work, \cite{Menache2002} also proposed a decomposition algorithm called \textsc{Q-Cut} based on graph partitioning view of the state transition graph. The Max-Flow/Min-Cut algorithm is used for finding the bottleneck states and flat policies are constructed to reach them. The problem of learning the policy over the problem decomposition is then cast under the options framework \cite{Sutton1999} where the \textsc{SMDP Q-Learning} algorithm is applied. The approach is revisited in \cite{Mannor2004} where an agglomerative graph clustering is applied. Under this slightly modified perspective, temporal abstraction takes place between pairs of clusters rather than from arbitrary states to bottleneck states. Two type of clusterings are also differentiated: \textit{topological} and \textit{value-based}. The former attempts to group states into clusters based on structural regularities of the environment while the latter considers the reward information collected during learning.
This agglomerative approach has recently been reformulated under an online setting in the \textit{Online Graph-based Agglomerative Hiearchical Clustering} algorithm (\textsc{OGAHC}) of \cite{Metzen2012}.

The \textsc{L-Cut} algorithm of \cite{Simsek2005} can be seen as an extension of \textsc{Q-Cut} subject to the constraint of a local knowledge of the environment. Rather than setting up the graph bisection problem under the Max-Flow/Min-Cut formulation, the \textsc{NCut} criterion of \cite{ShiMalik2000} is used to measure the quality of the partitioning and is solved as as generalized eigenvalue problem. A local graph representation is obtained by collecting samples of experience through a random walk process. After a certain number of iterations, the spectral bipartionning algorithm is applied and a policy, encoded as an \textit{option}, is learnt to reach the identified subgoals. A statistical test is applied on the set of boundary states of each partition to discriminate \textit{useful} subgoals from noise.

The spectral clustering approach was revisited more recently in \cite{Mathew2012} under the Robust Perron Cluster Analysis of \cite{Weber2004} and its \textsc{PCCA+} algorithm. While \textsc{NCut} and \textsc{PCCA+} are both spectral techniques sharing much in common,  Perron Cluster Analysis lends itself more easily to an interpretation in terms of metastability and invariant subsets of a Markov chain. Intuitively, metastable regions corresponds to subsets of states in which the stochastic process spends more time, switching to other such subsets on rare occasions. The \textit{cascade decomposition} technique of \cite{Chiu2010} attempts to find bottlenecks by computing the second eigenvector of the normalized graph Laplacian. It is in essence very similar to \textsc{L-Cut} which uses \textsc{NCut} \cite{ShiMalik200}, also derived from the normalized graph Laplacian. 

Trying to address both the discovery problem and the related one of transfer learning, \cite{Bouvrie2012} builds upon the diffusion maps framework of \cite{Coifman2006} and sharing ideas from \cite{Mahadevan2007}. A spectral partitioning approach is also taken to identify \textit{geometric bottlenecks} of the environment without taking the reward structure into consideration. The diffusion framework is defined in terms of the random walk process on a graph and the spectrum of the corresponding normalized graph Laplacian \cite{Chung1997}. The uniqueness of this other instance of the graph partitioning approach to temporal abstraction lies its ability to \textit{blend} policies and reward functions found at different levels of abstraction. 

Rather than searching for a general partitioning of the state space, \cite{Kazemitabar2009} attempts to find strongly connected components (SCC). While similar in spirit, the idea of finding such structures raise some questions regarding ergodic properties. It seems indeed that this scheme could be highly sensible to the sampling strategy. It could be expected that in most situations, only a single component would be found. In short, the SCC constraint seems to be too strong. Surprisingly, the \textsc{HEXQ} framework of \cite{Hengst2002} is not mentioned by  the authors. \textsc{HEXQ} also attempts to automatically discover hierarchical structure based on the analysis of the SCC decomposition of each state variable. The author gives an interpretation of the resulting decomposition in terms of maximum \textit{predictability} within those regions. Such a view also admits an information theoretic interpretation which is hinted to the reader later in this thesis.

\section{States Classification Approaches}
Rather than explicitly partitioning the state space using graph theoretic tools, another body of work focused on directly classifying out bottleneck states based on a visitation frequency principle. This view of bottleneck detection is in a sense more aligned with the philosophical principle of reinforcement learning. Indeed, it tends to lend itself more easily to an online formulation and is cheap to compute. The proposed solutions are however deeply heuristic-based and it quickly becomes difficult to determine which of them might be superior. While little theory is also known about the class of graph partitioning approaches, it seems easier to back it up with meaningful theory as argued in chapter \ref{chap:dynamics}.

The bottleneck discovery problem is formulated in \cite{Mcgovern2001} as a multiple-instance learning problem over bags of feature vectors collected by interacting with the environment. Two sets of bags are obtained in this way: from observations collected along successful and unsuccessful trajectories respectively. The notion of diverse density is then applied  either by exhaustive search or gradient descent to find regions of the feature space with the most positive instances and the least negative ones. Because negative bags must only contain unsuccessful trajectories, while positive bags have to contain at least one successful feature vector, this method is sensitive to noise. The classification constraint imposed  by \cite{Mcgovern2001} between \textit{good} and \textit{bad} trajectories is lifted in \cite{Stolle2002}. It rather directly extracts those states which have been visited frequently and define them as subgoals. The initiation sets of the options is found by interpolating between over the states which appear frequently enough (above average) on the paths going through those maxima.

In~\cite{Simsek2004}, the authors identify subgoals by looking for \textit{access states} which lead to the regions of the state space that have not been visited recently and trying to capture the notion of \textit{relative novelty}. The time frame within which to search for such novelty events is a parameter of this algorithm and influences the results obtained.

The use of a standard network centrality measure called \textit{betweenness centrality} is investigated in \cite{Simsek2008}. Betweenness centrality measures the fraction of shortest paths passing through a given vertex of the graph. Nodes with high betweenness are deemed more important as they would appear more frequently on trajectories to the goal. The reward function is somehow taken into account into this measure by weighting the paths depending on weather they successfully reached the goal or not. A new graph centrality measure is proposed in \cite{Rad2010} called \textit{connection graph stability} and is argued empirically to be a better fit than betweenness centrality for discovering bottlenecks. The same group of authors then exploited slight variations of this definitions under different names such as \textit{connection bridge centrality} or \textit{co-betweenness centrality} \cite{Moradi2010}.  

compute betweenness centrality to identify important subgoals. They
also propose a potential incremental formulation of their algorithm
that finds local maxima of betweenness within the subgraph obtained
by collecting short trajectories. However, it has also been argued~\cite{Rad2010} that
the centrality measure called \textit{connection graph stability} leads
to better subgoal identification than betweenness or closeness centrality.

\subsection{Outline}

Basic theory of stochastic processes and Markov chains is first presented in chapter \ref{chap:decisionmaking}. The inclusion of this material is motivated by the probabilistic interpretation of spectral graph theory studied in chapter \ref{chap:dynamics}. The theory of Markov Decision Processes is presented in section \ref{sec:mdp} as a necessary prerequisite to the appreciation of the techniques developed in reinforcement learning. Chapter \ref{chap:temporalabstraction} focuses on the presentation of the options framework \cite{Sutton1999} in reinforcement learning as a way to express temporal abstraction and learn optimal policies over such hierarchical representation. 

The connection from graph \textit{structure} to system \textit{dynamics} is developed throughout chapter \ref{chap:dynamics} and is instrumental is understanding the strengths and pitfalls of the graph partitioning approach to options discovery. It also allows a better understanding of the relevance of the work on Nearly-Completely Decomposable Markov Chains (NCD) towards better theoretical developments on the bottleneck concept. Furthermore, the perturbational approach traditionally taken in this field hints to an information theoretic comprehension of temporal abstraction. 

A new algorithm for options discovery is developed in chapter \ref{chap:buildingoptions} based on the community detection algorithm \textsc{Walktrap} of \cite{Pons2005}. Although \textsc{Walktrap} finds its roots into spectral graph theory, its running time is order $\mathcal{O}(mn^2)$ rather than $\mathcal{O}(n^3)$ by avoiding to compute the eigenvectors explicitly. The problem of options discovery and construction is also set under the assumption of a continuous space. Techniques for constructing proximity graphs are also developed in section \ref{sec:proximitygraphs}. It is also shown how approximate nearest neighbors algorithms can be used to properly define the initiation and termination components of the options when subject to continuous observations.

An illustration of the proposed algorithm is provided in chapter \ref{chap:illustration} under the Pinball domain of \cite{Konidaris2009}. Practical difficulties having to do oscillations and off-policy learning are analysed. The proper empirical choices for the number of nearest neighbors, type of proximity graph and time scale for the \textsc{Walktrap} algorithm are discussed.