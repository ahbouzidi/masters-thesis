The breakthroughs in hierarchical reinforcement learning (HRL) at the beginning of the last decade have promised to propel the field to new levels. Its development has however been hindered by an easily explainable, but yet challenging problem: how can temporal abstractions be discovered automatically ? It is the ``elephant in the room'' for HRL which, despite much effort, remains unsolved. The existence of this problem has been acknowledged by many authors. In \cite{Dietterich2000}, the author of the \textsc{MAXQ} framework  goes as far as qualifying it as being the ``biggest open problem'' in HRL. Lacking automatic methods for decomposing the problem structure into simpler sub-problems, hierarchical approaches are of limited interest as the effort for manually specifying them can quickly become insurmountable. \cite{Hengst2002} even refers to manual problem decomposition as being an ``art-form'' .

The bottleneck concept arose early in the field of HRL as an intuitive response to this issue. Bottlenecks have been defined as those states which appear frequently on successful trajectories to a goal but not on unsuccessful ones \parencite{Mcgovern2001, Stolle2002} or as nodes which allow for densely connected regions of the interaction graph to reach other such regions \parencite{Menache2002, Simsek2004, Kazemitabar2009}. \cite{Simsek2004} qualifies such states as \textit{access states}, allowing difficult regions of the state space to be reached. The canonical example for the bottleneck concept often explains it with the \textit{doorways} of some navigation problem in a grid-world domain.

The ideas presented in this thesis found their roots early in the history of HRL. It seems that two lines of approach have since been studied by a number of authors and could be recognized as either belonging to some \textit{graph clustering}, or  \textit{classification} perspective. While aiming towards the same goal, the classification perspective might have been more influenced by the \textit{online} and \textit{model-free} requirements  traditionally sought for in RL. It will be argued in chapter \ref{chap:dynamics}  that bottlenecks are intrinsically related to graph clustering and can be tackled more easily under this angle. 

\section{Graph Clustering Perspective}
\cite{Hauskrecht1998} described a state space partitioning
approach leading to an abstract MDP being overlaid to the periphery of the partitions.
Macro-actions would then be learnt as transitions between such peripheral
states. While a decomposition is assumed to be available \textit{a priori}, a contribution of this paper was to prove that such a model can lead to a reduction in the size of the state or action spaces. A similar state space partitioning intuition had been previously explored by a collaborator in \cite{Dean1995}. Even though the method of \cite{Dean1995} was not cast yet under the framework of
reinforcement learning, it showed how a global planning problem could be decomposed into
smaller locally-solvable problems which could be then be recombined into an optimal policy.
The machinery or HRL being in its infancy, the Dantzig-Wolfe method of decomposition \parencite{Dantzig1960} -- a linear programming algorithm -- was used to solve the abstract MDP. The problem of obtaining a proper partitioning was also sidestepped in this work.

Along the same line of work, \cite{Menache2002} also proposed a decomposition algorithm called \textsc{Q-Cut} based on graph partitioning view of the state-transition graph. The Max-Flow/Min-Cut algorithm was used for finding the bottleneck states for which flat policies were constructed to reach them. The problem of representation and learning for temporal abstraction was cast under the options framework \parencite{Sutton1999} in which the \textsc{SMDP Q-Learning} algorithm could be applied. The approach was revisited in \cite{Mannor2004} where an agglomerative graph clustering algorithm was applied. Under this other perspective, options where defined between pairs of clusters rather than from arbitrary states to bottleneck states. Two types of graph clusterings were also defined: \textit{topological} and \textit{value-based}. The former attempted to group states into clusters based on structural regularities of the environment while the latter considered the reward information collected during learning.
This agglomerative approach has recently been reformulated under an online setting in the \textit{Online Graph-based Agglomerative Hiearchical Clustering} algorithm (\textsc{OGAHC}) of \cite{Metzen2012}.

The \textsc{L-Cut} algorithm of \cite{Simsek2005} can be seen as an extension of \textsc{Q-Cut} under a local knowledge of the environment only. Rather than setting up the graph bisection problem under the Max-Flow/Min-Cut formulation, the \textsc{NCut} criterion of \cite{ShiMalik2000} was used to measure the quality of the partitioning and solved as as generalized eigenvalue problem. A local graph representation was obtained by collecting samples of experience through a random walk process. After a certain number of iterations, the spectral bipartionning algorithm was applied and a policy, encoded as an \textit{option}, was learnt to reach the identified subgoals. A statistical test was applied on the set of boundary states of each partition to discriminate \textit{useful} subgoals from noise.

The spectral clustering approach was revisited more recently in \cite{Mathew2012} under the Robust Perron Cluster Analysis of \cite{Weber2004} and their  \textsc{PCCA+} algorithm. While \textsc{NCut} and \textsc{PCCA+} are both spectral techniques sharing much in common,  Perron Cluster Analysis lends itself more easily to an interpretation in terms of metastability and invariant subsets of Markov chains. Intuitively, metastable regions correspond to subsets of states in which the stochastic process spends more time, switching to other such subsets on rare occasions. The \textit{cascade decomposition} technique of \cite{Chiu2010} is another recent attempt to find bottlenecks by computing the second eigenvector of the normalized graph Laplacian. It is in essence very similar to \textsc{L-Cut} which uses \textsc{NCut} \cite{ShiMalik2000}, also derived from the normalized graph Laplacian. 

Trying to address both the discovery problem and the related one of transfer learning, \cite{Bouvrie2012} builds upon the diffusion maps framework of \cite{Coifman2006} and sharing ideas from \cite{Mahadevan2007}. A spectral partitioning approach is also adopted to identify \textit{geometric bottlenecks} of the environment, avoiding to take the reward structure into consideration. Policies and reward functions associated with these partitions are obtained by an intricate \textit{blending} procedure across scales. The diffusion framework can be explained in terms of the random walk process on a graph and the spectrum of the corresponding normalized graph Laplacian. To that account, the algorithm presented in this thesis shares much in common. 

Instead of the general graph partitioning paradigm considered so far, \cite{Kazemitabar2009} attempts to find strongly connected components (SCC). While similar in spirit, this approach raises some questions regarding ergodicity. It seems that this scheme could be highly sensible to the sampling strategy, resulting in many cases with only a single component being found. The author's viewpoint is that the SCC constraint might be too strong. Surprisingly, the \textsc{HEXQ} framework of \cite{Hengst2002} is not mentioned by \cite{Kazemitabar2009}. \textsc{HEXQ} was an early attempt to automatically discover hierarchical structure based on a SCC decomposition of individual state variable. The author gave an interpretation of the resulting decomposition in terms of maximum \textit{predictability} within those regions. Such a viewpoint could also admit an information theoretic interpretation which is hinted to the reader later in this thesis.

\section{States Classification Approaches}
Rather than explicitly partitioning the state space using graph theoretic tools, another body of work focused on directly classifying out bottleneck states based on a visitation frequency principle. This view of bottleneck detection is in a sense closer to the philosophical principles of reinforcement learning. The proposed solutions tended to be cheaply computable \textit{online} and without a complete model of the environment. It seems however that they rely abundantly on heuristics, making it even hard to compare them . On the other hand, the graph partitioning approach appears to be justifiable more easily in theory as argued in chapter \ref{chap:dynamics}.

The bottleneck discovery problem was formulated in \cite{Mcgovern2001} as a multiple-instance learning problem over bags of feature vectors collected by interacting with the environment. Two sets of bags were obtained from observations collected along successful and unsuccessful trajectories. The notion of diverse density then served  during either exhaustive search or gradient descent to find regions of the feature space with the most positive instances and the least negative ones. The authors acknowledged  that this method can be particularly sensitive to noise. The classification constraint imposed  by \cite{Mcgovern2001} between \textit{good} and \textit{bad} trajectories was lifted in \cite{Stolle2002}. It rather tried to directly extract those states which have been visited frequently and define them as subgoals. The initiation sets of the options was found by interpolation over the states which appear frequently enough (above average) on the paths to the subgoals.

\cite{Simsek2004} identified subgoals by looking for \textit{access states}, leading to the regions of the state space that have not been visited recently: a notion which they called \textit{relative novelty}. The time frame within which novel events can happen is parameter of the algorithm and has a direct influence on which subgoals are detected.

The use of a standard network centrality measure called \textit{betweenness} is investigated in \cite{Simsek2008}. Betweenness centrality measures the fraction of shortest paths passing through a given vertex of the graph. Nodes with high betweenness are deemed more important as they would appear more frequently on short, therefore more likely to be optimal, trajectories to the goal. The reward function is somehow taken into account into this measure by weighting the paths depending on weather they successfully reached the goal or not. A new graph centrality measure is proposed in \cite{Rad2010} called \textit{connection graph stability} and is argued to be a better fit than betweenness centrality for discovering bottlenecks. Some of the contributors then exploited slight variations of this definitions under different names such as \textit{connection bridge centrality} or \textit{co-betweenness centrality} \parencite{Moradi2010}.  

\subsection{Outline}

Basic theory of stochastic processes and Markov chains is first presented in chapter \ref{chap:decisionmaking}. The inclusion of this material is motivated by the probabilistic interpretation of spectral graph theory studied in chapter \ref{chap:dynamics}. The theory of Markov Decision Processes is presented in section \ref{sec:mdp} as a necessary prerequisite for the appreciation of the techniques developed in reinforcement learning. Chapter \ref{chap:temporalabstraction} focuses on the presentation of the options framework \parencite{Sutton1999} in reinforcement learning as a way to represent temporal abstraction and learn optimal control over it.

The connection from graph \textit{structure} to system \textit{dynamics} is developed throughout chapter \ref{chap:dynamics} and is instrumental is understanding the strengths and pitfalls of the graph partitioning approach for options discovery. It also allows a better understanding of the relevant work on Nearly-Completely Decomposable Markov Chains (NCD) for future theoretical research on the bottleneck concept. The NCD theory seems to call for an information theoretic comprehension of temporal abstraction which is briefly developed at the end of this section.

A new algorithm for options discovery is proposed in chapter \ref{chap:buildingoptions} based on the  \textsc{Walktrap} community detection algorithm of \cite{Pons2005}. Although \textsc{Walktrap} finds its roots into spectral graph theory, its running time is only order $\mathcal{O}(mn^2)$ rather than $\mathcal{O}(n^3)$ by avoiding to compute the eigenvectors explicitly. The problem of options discovery and construction is also set under the assumption of a continuous state space. Techniques for constructing proximity graphs in Euclidean space are developed in section \ref{sec:proximitygraphs}. Section \ref{sec:knnoptions} shows how approximate nearest neighbors algorithms can be used to properly define the initiation and termination components of options under continuous observations.

An illustration of the proposed algorithm is provided in chapter \ref{chap:illustration} with the Pinball domain of \cite{Konidaris2009}. Practical difficulties having to do oscillations and off-policy learning are analysed. The proper empirical choices for the number of nearest neighbors, type of proximity graph and time scale for the \textsc{Walktrap} algorithm are discussed.